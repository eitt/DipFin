
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>22. Introducción a la Regresión Lineal &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'DipFin Sesion 3.1';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="37. 1. Introducción a las Redes Neuronales" href="DipFin%20Sesion%203.2.html" />
    <link rel="prev" title="18. Introducción a la Limpieza de Datos" href="DipFin%20Sesion%201.5.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introducción
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Fundamentos de Python y Estadística Básica</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DipFin%20Sesion%201.1.html">1. Introducción a Python para Finanzas</a></li>




<li class="toctree-l1"><a class="reference internal" href="DipFin%20Sesion%201.2.html">6. Funciones en Python</a></li>


<li class="toctree-l1"><a class="reference internal" href="DipFin%20Sesion%201.3.html">9. Introducción a la Estadística en Python</a></li>




<li class="toctree-l1"><a class="reference internal" href="DipFin%20Sesion%201.4.html">14. Introducción a la Adquisición de Datos</a></li>



<li class="toctree-l1"><a class="reference internal" href="DipFin%20Sesion%201.5.html">18. Introducción a la Limpieza de Datos</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">22. Introducción a la Regresión Lineal</a></li>














<li class="toctree-l1"><a class="reference internal" href="DipFin%20Sesion%203.2.html">37. 1. Introducción a las Redes Neuronales</a></li>










<li class="toctree-l1"><a class="reference internal" href="DipFin%20Sesion%203.3.html">48. Introducción a Modelos de Series Temporales</a></li>










<li class="toctree-l1"><a class="reference internal" href="DipFin%20Sesion%203.4.html">59. Introducción al Análisis de Clustering</a></li>



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FDipFin Sesion 3.1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/DipFin Sesion 3.1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introducción a la Regresión Lineal</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">22. Introducción a la Regresión Lineal</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentos-teoricos">22.1. 1. <strong>Fundamentos Teóricos</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#que-es-la-regresion-lineal">22.1.1. ¿Qué es la Regresión Lineal?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicaciones-en-finanzas">22.1.2. Aplicaciones en Finanzas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relacion-entre-variables">22.1.3. Relación entre Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-los-coeficientes-de-la-ecuacion">22.1.4. Cálculo de los coeficientes de la ecuación</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-lineal-con-predictores-discretos">23. Regresión Lineal con Predictores Discretos</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concepto-de-efecto-y-calculo-de-pendientes">23.1. Concepto de Efecto y Cálculo de Pendientes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ecuacion-del-modelo">23.1.1. <strong>Ecuación del Modelo</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretacion-de-coeficientes">23.1.2. <strong>Interpretación de Coeficientes</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">23.2. Aplicaciones en Finanzas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparacion-entre-regresion-lineal-con-predictores-continuos-y-discretos">23.3. Comparación entre Regresión Lineal con Predictores Continuos y Discretos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizacion-comparativa-de-modelos">23.4. Visualización Comparativa de Modelos</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#caso-de-estudio-analisis-de-precios-de-mercedes-benz-en-ee-uu">24. Caso de Estudio: Análisis de Precios de Mercedes-Benz en EE.UU.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#descripcion-del-dataset">24.1. Descripción del Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparacion-y-limpieza-de-datos">24.2. Preparación y Limpieza de Datos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#codigo-para-la-limpieza-de-datos">24.2.1. Código para la Limpieza de Datos</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-lineal-para-predecir-precios">24.3. Regresión Lineal para Predecir Precios</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#codigo-para-la-regresion-lineal">24.3.1. Código para la Regresión Lineal</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-a-la-regresion-logistica">25. Introducción a la Regresión Logística</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">25.1. Fundamentos Teóricos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definicion-de-regresion-logistica">25.1.1. Definición de Regresión Logística</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relacion-con-la-regresion-lineal">25.2. Relación con la Regresión Lineal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diferencias-entre-regresion-lineal-y-logistica">25.3. Diferencias entre Regresión Lineal y Logística</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importancia-de-la-funcion-logit">25.3.1. Importancia de la Función Logit</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-parametros">26. Cálculo de Parámetros</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocesamiento-de-datos-para-regresion-logistica">27. Preprocesamiento de Datos para Regresión Logística</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variables-dummy">27.1. 1. Variables Dummy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizacion">27.2. 2. Normalización</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformaciones-logaritmicas">27.3. 3. Transformaciones Logarítmicas</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-logit">28. Modelo Logit</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ecuacion-del-modelo-logit">28.1. Ecuación del Modelo Logit</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-probit">29. Modelo Probit</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ecuacion-del-modelo-probit">29.1. Ecuación del Modelo Probit</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#diferencias-clave">30. Diferencias Clave</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-de-clasificacion-binaria-usando-regresion-caso-de-pronostico-de-riesgo">31. Ejemplo de clasificación binaria usando regresión: caso de pronóstico de riesgo</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#descripcion-de-las-columnas-del-dataset">31.1. Descripción de las columnas del dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion-del-modelo-de-regresion-logistica">31.2. Implementación del modelo de regresión logística</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-del-rendimiento-del-modelo-de-regresion-logistica">32. Análisis del rendimiento del modelo de regresión logística</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matriz-de-confusion">32.1. Matriz de confusión</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curva-roc-y-auc">32.2. Curva ROC y AUC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretacion-de-la-curva-roc">32.3. Interpretación de la curva ROC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coeficientes-en-la-regresion-logistica">32.4. Coeficientes en la Regresión Logística</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretacion-de-los-coeficientes">32.5. Interpretación de los Coeficientes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estandares-y-regularizacion">32.6. Estándares y Regularización</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocesamiento">33. Preprocesamiento</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-a-los-arboles-de-decision">34. Introducción a los Árboles de Decisión</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algoritmo-id3-iterative-dichotomiser-3">34.1. Algoritmo ID3 (Iterative Dichotomiser 3)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitaciones-del-id3">34.1.1. Limitaciones del ID3:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algoritmo-c4-5">34.2. Algoritmo C4.5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">34.3. 1. <strong>Fundamentos Teóricos</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptos-basicos">34.3.1. Conceptos Básicos</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#criterios-de-division">34.3.2. Criterios de División</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">35. Criterios de División</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ventajas-y-desventajas-frente-a-modelos-lineales">35.1. Ventajas y Desventajas Frente a Modelos Lineales</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-de-clasificacion-binaria-usando-arboles-de-decision-caso-de-pronostico-de-riesgo">36. Ejemplo de clasificación binaria usando árboles de decisión: caso de pronóstico de riesgo</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">36.1. Descripción de las columnas del dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion-del-modelo-de-arboles-de-decision">36.2. Implementación del modelo de árboles de decisión</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explicacion-matematica-de-feature-importances-en-arboles-de-decision">36.3. Explicación Matemática de <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> en Árboles de Decisión</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-la-importancia-de-las-caracteristicas">36.3.1. Cálculo de la Importancia de las Características</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#formula-general">36.3.1.1. Fórmula General</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gini-impurity">36.3.1.2. Gini Impurity</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-ilustrativo">36.3.2. Ejemplo Ilustrativo</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="introduccion-a-la-regresion-lineal">
<h1><span class="section-number">22. </span>Introducción a la Regresión Lineal<a class="headerlink" href="#introduccion-a-la-regresion-lineal" title="Link to this heading">#</a></h1>
<p>La regresión lineal es un método estadístico que intenta modelar la relación entre una variable dependiente y una o más variables independientes mediante el ajuste de una ecuación lineal (regresión lineal univariada o multivariada según la cantidad de predictores). Esta técnica es ampliamente utilizada debido a su simplicidad y versatilidad y hace parte de la familia de <a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_linear_model#:~:text=The%20GLM%20generalizes%20linear%20regression,function%20of%20its%20predicted%20value.">Modelos Lineales Generalizados</a>. Su capacidad para modelar relaciones lineales entre variables la hace indispensable para la predicción y el análisis financiero.</p>
<section id="fundamentos-teoricos">
<h2><span class="section-number">22.1. </span>1. <strong>Fundamentos Teóricos</strong><a class="headerlink" href="#fundamentos-teoricos" title="Link to this heading">#</a></h2>
<section id="que-es-la-regresion-lineal">
<h3><span class="section-number">22.1.1. </span>¿Qué es la Regresión Lineal?<a class="headerlink" href="#que-es-la-regresion-lineal" title="Link to this heading">#</a></h3>
<p>La forma más básica de la regresión lineal es la regresión lineal simple, donde solo se utiliza una variable independiente. La ecuación de una regresión lineal simple se puede expresar como:</p>
<div class="math notranslate nohighlight">
\[ y = \beta_0 + \beta_1x + \epsilon \]</div>
<p>donde:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y \)</span>es la variable dependiente.</p></li>
<li><p><span class="math notranslate nohighlight">\( x \)</span> es la variable independiente.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span> es el término de intercepción.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_1 \)</span> es el coeficiente de la variable independiente, que indica el cambio en <span class="math notranslate nohighlight">\( y \)</span> por una unidad de cambio en <span class="math notranslate nohighlight">\( x \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \epsilon \)</span> es el término del error, que representa lo que no puede ser explicado por el modelo lineal. Cabe resaltar que para este modelo se parte del supuesto que el error se distribuye como una variable normal aleatoria <span class="math notranslate nohighlight">\(N(0,σ)\)</span>y, en tal sentido, y se distribuye como <span class="math notranslate nohighlight">\(N(\beta_0 + \beta_1x,σ)\)</span></p></li>
</ul>
</section>
<section id="aplicaciones-en-finanzas">
<h3><span class="section-number">22.1.2. </span>Aplicaciones en Finanzas<a class="headerlink" href="#aplicaciones-en-finanzas" title="Link to this heading">#</a></h3>
<p>En el contexto financiero, la regresión lineal puede ser utilizada para:</p>
<ul class="simple">
<li><p>Predecir el precio de las acciones basándose en factores económicos.</p></li>
<li><p>Evaluar la relación entre el rendimiento de un activo y el mercado más amplio (modelo CAPM - *capital Assest Pricing Model * - ).</p></li>
<li><p>Estimar el crecimiento de los ingresos de una empresa a partir de indicadores económicos.</p></li>
</ul>
</section>
<section id="relacion-entre-variables">
<h3><span class="section-number">22.1.3. </span>Relación entre Variables<a class="headerlink" href="#relacion-entre-variables" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Variables Independientes (Predictores)</strong>: En un modelo financiero, estas podrían incluir indicadores como tasas de interés, índices bursátiles, o balances financieros.</p></li>
<li><p><strong>Variable Dependiente (Respuesta)</strong>: El resultado que se quiere predecir, como el precio de una acción.</p></li>
<li><p><strong>Coeficientes de Regresión</strong>: Indican la influencia de cada variable independiente en la variable dependiente. Un coeficiente positivo sugiere una relación directa; uno negativo, una inversa.</p></li>
</ul>
</section>
<section id="calculo-de-los-coeficientes-de-la-ecuacion">
<h3><span class="section-number">22.1.4. </span>Cálculo de los coeficientes de la ecuación<a class="headerlink" href="#calculo-de-los-coeficientes-de-la-ecuacion" title="Link to this heading">#</a></h3>
<p>Para encontrar los parámetros <span class="math notranslate nohighlight">\(\beta_0\)</span> y <span class="math notranslate nohighlight">\( \beta_1 \)</span> de la regresión lineal mediante el método de mínimos cuadrados, se utiliza un proceso de optimización donde se minimiza la suma de los cuadrados de los errores (residuos) entre los valores observados y los valores predichos por el modelo lineal.</p>
<ul class="simple">
<li><p><strong>Paso 1: Definir la función de costo</strong></p></li>
</ul>
<p>La función de costo, también conocida como función de pérdida, en el contexto de la regresión lineal simple es la suma de los cuadrados de los residuos. Para un conjunto de datos <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> donde <span class="math notranslate nohighlight">\(i = 1, 2, ..., n\)</span>, la función de costo se define como:</p>
<div class="math notranslate nohighlight">
\[ L(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2 \]</div>
<ul class="simple">
<li><p><strong>Paso 2: Calcular las derivadas parciales</strong></p></li>
</ul>
<p>Para encontrar los valores de <span class="math notranslate nohighlight">\( \beta_0 \)</span> y <span class="math notranslate nohighlight">\( \beta_1 \)</span> que minimizan la función de costo, calculamos las derivadas parciales de <span class="math notranslate nohighlight">\( L \)</span> con respecto a <span class="math notranslate nohighlight">\( \beta_0 \)</span> y <span class="math notranslate nohighlight">\( \beta_1 \)</span> y las igualamos a cero. Estas derivadas nos indican la tasa de cambio de la función de costo con respecto a cada parámetro.</p>
<p>Derivada respecto a <span class="math notranslate nohighlight">\( \beta_0 \)</span>:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial L}{\partial \beta_0} = -2 \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i)) \]</div>
<p>Derivada respecto a <span class="math notranslate nohighlight">\( \beta_1 \)</span>:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial L}{\partial \beta_1} = -2 \sum_{i=1}^n x_i (y_i - (\beta_0 + \beta_1 x_i)) \]</div>
<ul class="simple">
<li><p><strong>Paso 3: Igualar las derivadas a cero para encontrar puntos críticos</strong></p></li>
</ul>
<p>Igualamos cada derivada a cero para encontrar los valores de <span class="math notranslate nohighlight">\( \beta_0 \)</span> y <span class="math notranslate nohighlight">\( \beta_1 \)</span> que minimizan la función de costo:</p>
<div class="math notranslate nohighlight">
\[ -2 \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i)) = 0 \]</div>
<div class="math notranslate nohighlight">
\[ -2 \sum_{i=1}^n x_i (y_i - (\beta_0 + \beta_1 x_i)) = 0 \]</div>
<p>Simplificando, obtenemos las ecuaciones normales del sistema:</p>
<div class="math notranslate nohighlight">
\[ \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i)) = 0 \]</div>
<div class="math notranslate nohighlight">
\[ \sum_{i=1}^n x_i (y_i - (\beta_0 + \beta_1 x_i)) = 0 \]</div>
<ul class="simple">
<li><p><strong>Paso 4: Resolver el sistema de ecuaciones</strong></p></li>
</ul>
<p>Resolviendo este sistema de ecuaciones lineales, obtenemos:</p>
<ol class="arabic simple">
<li><p>De la primera ecuación:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ n \beta_0 + \beta_1 \sum_{i=1}^n x_i = \sum_{i=1}^n y_i \]</div>
<ol class="arabic simple" start="2">
<li><p>De la segunda ecuación:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ \beta_0 \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2 = \sum_{i=1}^n x_i y_i \]</div>
<p>Este es un sistema lineal de dos ecuaciones con dos incógnitas (<span class="math notranslate nohighlight">\( \beta_0 \)</span> y <span class="math notranslate nohighlight">\( \beta_1 \)</span>), que puede resolverse mediante métodos algebraicos como la eliminación o mediante matrices.</p>
<ul class="simple">
<li><p><strong>Paso 5: Solución explícita de los betas</strong></p></li>
</ul>
<p>La solución de este sistema nos da los estimadores de mínimos cuadrados:</p>
<div class="math notranslate nohighlight">
\[ \beta_1 = \frac{n \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \sum_{i=1}^n y_i}{n \sum_{i=1}^n x_i^2 - (\sum_{i=1}^n x_i)^2} \]</div>
<div class="math notranslate nohighlight">
\[ \beta_0 = \frac{\sum_{i=1}^n y_i - \beta_1 \sum_{i=1}^n x_i}{n} \]</div>
<p>Estos valores de <span class="math notranslate nohighlight">\( \beta_0 \)</span> y <span class="math notranslate nohighlight">\( \beta_1 \)</span> son los que mejor ajustan la línea recta a los datos en el sentido de minimizar la suma de los cuadrados de los residuos.</p>
</section>
</section>
</section>
<section id="regresion-lineal-con-predictores-discretos">
<h1><span class="section-number">23. </span>Regresión Lineal con Predictores Discretos<a class="headerlink" href="#regresion-lineal-con-predictores-discretos" title="Link to this heading">#</a></h1>
<p>La regresión lineal es típicamente conocida por su capacidad de modelar relaciones lineales entre una variable dependiente continua y una o más variables independientes que pueden ser tanto continuas como discretas. Cuando se trata de predictores discretos, el modelo ofrece una visión interesante del efecto de cada categoría sobre la variable de respuesta.</p>
<section id="concepto-de-efecto-y-calculo-de-pendientes">
<h2><span class="section-number">23.1. </span>Concepto de Efecto y Cálculo de Pendientes<a class="headerlink" href="#concepto-de-efecto-y-calculo-de-pendientes" title="Link to this heading">#</a></h2>
<p>En el contexto de la regresión lineal, un predictor discreto es generalmente tratado como una variable categórica. El tratamiento de estas variables en un modelo lineal implica convertirlas en una serie de variables indicadoras o <em>dummy</em>.</p>
<section id="ecuacion-del-modelo">
<h3><span class="section-number">23.1.1. </span><strong>Ecuación del Modelo</strong><a class="headerlink" href="#ecuacion-del-modelo" title="Link to this heading">#</a></h3>
<p>Para un predictor discreto con <span class="math notranslate nohighlight">\( k \)</span> categorías, la relación puede modelarse como:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 d_2 + \beta_3 d_3 + \ldots + \beta_k d_k + \epsilon
\]</div>
<p>donde:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y \)</span> es la variable dependiente.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span> es el intercepto.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_1 x_1 \)</span> es un predictor continuo (si existe).</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_2, \beta_3, \ldots, \beta_k \)</span> son los coeficientes para cada una de las <span class="math notranslate nohighlight">\( k-1 \)</span> variables dummy creadas por el predictor discreto.</p></li>
<li><p><span class="math notranslate nohighlight">\( d_2, d_3, \ldots, d_k \)</span> son las variables dummy (0 o 1).</p></li>
<li><p><span class="math notranslate nohighlight">\( \epsilon \)</span> es el término del error.</p></li>
</ul>
</section>
<section id="interpretacion-de-coeficientes">
<h3><span class="section-number">23.1.2. </span><strong>Interpretación de Coeficientes</strong><a class="headerlink" href="#interpretacion-de-coeficientes" title="Link to this heading">#</a></h3>
<p>Cada coeficiente <span class="math notranslate nohighlight">\( \beta_i \)</span> asociado con una variable dummy representa el efecto adicional en <span class="math notranslate nohighlight">\( y \)</span> al cambiar de la categoría de referencia (usualmente la categoría para la cual todas las dummies son 0) a la categoría <span class="math notranslate nohighlight">\( i \)</span>.</p>
</section>
</section>
<section id="id1">
<h2><span class="section-number">23.2. </span>Aplicaciones en Finanzas<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Análisis de Crédito</strong>: Modelar la probabilidad de incumplimiento basada en categorías de clientes como riesgo bajo, medio o alto.</p></li>
<li><p><strong>Rendimiento de Inversión</strong>: Evaluar el impacto de diferentes clases de activos (como bonos, acciones, o derivados) en el rendimiento de la cartera.</p></li>
</ul>
</section>
<section id="comparacion-entre-regresion-lineal-con-predictores-continuos-y-discretos">
<h2><span class="section-number">23.3. </span>Comparación entre Regresión Lineal con Predictores Continuos y Discretos<a class="headerlink" href="#comparacion-entre-regresion-lineal-con-predictores-continuos-y-discretos" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Característica</p></th>
<th class="head"><p>Predictores Continuos</p></th>
<th class="head"><p>Predictores Discretos</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Naturaleza de Variables</p></td>
<td><p>Valores numéricos en un rango continuo.</p></td>
<td><p>Categorías fijas (p.ej., género, clase de activo).</p></td>
</tr>
<tr class="row-odd"><td><p>Interpretación de Coeficientes</p></td>
<td><p>Cambio en <span class="math notranslate nohighlight">\( y \)</span> por cada unidad de cambio.</p></td>
<td><p>Cambio en <span class="math notranslate nohighlight">\( y \)</span> al cambiar de categoría.</p></td>
</tr>
<tr class="row-even"><td><p>Visualización</p></td>
<td><p>Línea continua en un gráfico.</p></td>
<td><p>Saltos entre categorías en el gráfico.</p></td>
</tr>
<tr class="row-odd"><td><p>Tipo de Análisis</p></td>
<td><p>Correlaciones y tendencias.</p></td>
<td><p>Comparación entre grupos distintos.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="visualizacion-comparativa-de-modelos">
<h2><span class="section-number">23.4. </span>Visualización Comparativa de Modelos<a class="headerlink" href="#visualizacion-comparativa-de-modelos" title="Link to this heading">#</a></h2>
<p>A continuación, se presenta un código en Python utilizando <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> para comparar visualmente los modelos de regresión lineal con predictores continuos y discretos.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Datos simulados</span>
<span class="n">x_cont</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_cont</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x_cont</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">x_disc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y_disc</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x_disc</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Gráfico para predictores continuos</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_cont</span><span class="p">,</span> <span class="n">y_cont</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_cont</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x_cont</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Regresión Lineal con Predictor Continuo&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predictor Continuo&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Variable Dependiente&#39;</span><span class="p">)</span>

<span class="c1"># Gráfico para predictores discretos</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_disc</span><span class="p">,</span> <span class="n">y_disc</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">y_disc</span><span class="p">[</span><span class="n">x_disc</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">),</span> <span class="n">subset</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Regresión Lineal con Predictor Discreto&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predictor Discreto&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Variable Dependiente&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Datos simulados</span>
<span class="n">x_cont</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_cont</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x_cont</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">x_disc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y_disc</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x_disc</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Gráfico para predictores continuos</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_cont</span><span class="p">,</span> <span class="n">y_cont</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_cont</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x_cont</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Regresión Lineal con Predictor Continuo&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predictor Continuo&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Variable Dependiente&#39;</span><span class="p">)</span>

<span class="c1"># Gráfico para predictores discretos</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_disc</span><span class="p">,</span> <span class="n">y_disc</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">y_disc</span><span class="p">[</span><span class="n">x_disc</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">),</span> <span class="n">subset</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="o">*</span>

<span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Regresión Lineal con Predictor Discreto&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predictor Discreto&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Variable Dependiente&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d3849c67617aec5b42e8f7ee30edb95232dd763a9624af67652b949bf9966910.png" src="_images/d3849c67617aec5b42e8f7ee30edb95232dd763a9624af67652b949bf9966910.png" />
</div>
</div>
</section>
</section>
<section id="caso-de-estudio-analisis-de-precios-de-mercedes-benz-en-ee-uu">
<h1><span class="section-number">24. </span>Caso de Estudio: Análisis de Precios de Mercedes-Benz en EE.UU.<a class="headerlink" href="#caso-de-estudio-analisis-de-precios-de-mercedes-benz-en-ee-uu" title="Link to this heading">#</a></h1>
<p>Este caso de estudio se centra en un conjunto de datos que lista diversas características y precios de vehículos Mercedes-Benz en Estados Unidos. Los datos provienen de anuncios de coches y proporcionan una visión valiosa sobre cómo varían los precios en función de factores como el modelo del coche, el kilometraje, la valoración del vendedor y el número de revisiones. El conjunto de datos está disponible para descarga en <a class="reference external" href="https://www.kaggle.com/datasets/danishammar/usa-mercedes-benz-prices-dataset">Kaggle</a>.</p>
<section id="descripcion-del-dataset">
<h2><span class="section-number">24.1. </span>Descripción del Dataset<a class="headerlink" href="#descripcion-del-dataset" title="Link to this heading">#</a></h2>
<p>El dataset incluye las siguientes columnas:</p>
<ul class="simple">
<li><p><strong>Name</strong>: Nombre y modelo del coche.</p></li>
<li><p><strong>Mileage</strong>: Kilometraje del coche en millas.</p></li>
<li><p><strong>Rating</strong>: Valoración media del vendedor de coches.</p></li>
<li><p><strong>Review Count</strong>: Número de revisiones del vendedor.</p></li>
<li><p><strong>Price</strong>: Precio del coche en dólares estadounidenses.</p></li>
</ul>
</section>
<section id="preparacion-y-limpieza-de-datos">
<h2><span class="section-number">24.2. </span>Preparación y Limpieza de Datos<a class="headerlink" href="#preparacion-y-limpieza-de-datos" title="Link to this heading">#</a></h2>
<p>Antes de proceder con el análisis, es crucial limpiar y preparar los datos adecuadamente. Este proceso incluye la extracción del año del coche del campo “Name”, la conversión del kilometraje a un formato numérico y la gestión de posibles valores faltantes.</p>
<section id="codigo-para-la-limpieza-de-datos">
<h3><span class="section-number">24.2.1. </span>Código para la Limpieza de Datos<a class="headerlink" href="#codigo-para-la-limpieza-de-datos" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Cargar datos</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;path_to_data/usa_mercedes_benz_prices_dataset.csv&#39;</span><span class="p">)</span>

<span class="c1"># Extraer el año del coche y limpiar la columna &#39;Name&#39;</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Name&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Convertir &#39;Mileage&#39; a numérico</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Mileage&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Mileage&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; mi.&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Asegurar que &#39;Rating&#39; y &#39;Review Count&#39; sean numéricos</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Rating&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Rating&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Review Count&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Review Count&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>

<span class="c1"># Tratar valores faltantes</span>
<span class="n">data</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Mostrar las primeras filas del dataset limpio</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</section>
</section>
<section id="regresion-lineal-para-predecir-precios">
<h2><span class="section-number">24.3. </span>Regresión Lineal para Predecir Precios<a class="headerlink" href="#regresion-lineal-para-predecir-precios" title="Link to this heading">#</a></h2>
<p>Ahora, implementaremos un modelo de regresión lineal para predecir el precio de los coches basado en su año, kilometraje, valoración del vendedor y número de revisiones.</p>
<section id="codigo-para-la-regresion-lineal">
<h3><span class="section-number">24.3.1. </span>Código para la Regresión Lineal<a class="headerlink" href="#codigo-para-la-regresion-lineal" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Preparar los datos para el modelo</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Year&#39;</span><span class="p">,</span> <span class="s1">&#39;Mileage&#39;</span><span class="p">,</span> <span class="s1">&#39;Rating&#39;</span><span class="p">,</span> <span class="s1">&#39;Review Count&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">]</span>

<span class="c1"># Dividir los datos en conjuntos de entrenamiento y prueba</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Crear y entrenar el modelo de regresión lineal</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predecir y evaluar el modelo</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;El error cuadrático medio (MSE) del modelo es: </span><span class="si">{</span><span class="n">mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Coeficientes del modelo</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Load data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/content/usa_mercedes_benz_prices.csv&#39;</span><span class="p">)</span>

<span class="c1"># Extract the year from the car and clean the &#39;Name&#39; column</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Name&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Convert &#39;Mileage&#39; to numeric</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Mileage&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Mileage&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; mi.&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="c1"># Remove the dollar sign and commas, and replace &#39;Not Priced&#39; with None</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Not Priced&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="c1"># Ensure &#39;Rating&#39; and &#39;Review Count&#39; are numeric</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Rating&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Rating&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Review Count&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Review Count&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>

<span class="c1"># Handle missing values</span>
<span class="n">data</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Display the first rows of the cleaned dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">5</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="c1"># Load data</span>
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/content/usa_mercedes_benz_prices.csv&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="c1"># Extract the year from the car and clean the &#39;Name&#39; column</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span>

<span class="nn">File ~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026,</span> in <span class="ni">read_csv</span><span class="nt">(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)</span>
<span class="g g-Whitespace">   </span><span class="mi">1013</span> <span class="n">kwds_defaults</span> <span class="o">=</span> <span class="n">_refine_defaults_read</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1014</span>     <span class="n">dialect</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1015</span>     <span class="n">delimiter</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1022</span>     <span class="n">dtype_backend</span><span class="o">=</span><span class="n">dtype_backend</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1023</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1024</span> <span class="n">kwds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwds_defaults</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1026</span> <span class="k">return</span> <span class="n">_read</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="n">kwds</span><span class="p">)</span>

<span class="nn">File ~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620,</span> in <span class="ni">_read</span><span class="nt">(filepath_or_buffer, kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">617</span> <span class="n">_validate_names</span><span class="p">(</span><span class="n">kwds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;names&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">619</span> <span class="c1"># Create the parser.</span>
<span class="ne">--&gt; </span><span class="mi">620</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">TextFileReader</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">622</span> <span class="k">if</span> <span class="n">chunksize</span> <span class="ow">or</span> <span class="n">iterator</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">623</span>     <span class="k">return</span> <span class="n">parser</span>

<span class="nn">File ~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620,</span> in <span class="ni">TextFileReader.__init__</span><span class="nt">(self, f, engine, **kwds)</span>
<span class="g g-Whitespace">   </span><span class="mi">1617</span>     <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">[</span><span class="s2">&quot;has_index_names&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwds</span><span class="p">[</span><span class="s2">&quot;has_index_names&quot;</span><span class="p">]</span>
<span class="g g-Whitespace">   </span><span class="mi">1619</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="p">:</span> <span class="n">IOHandles</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="ne">-&gt; </span><span class="mi">1620</span> <span class="bp">self</span><span class="o">.</span><span class="n">_engine</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_engine</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">)</span>

<span class="nn">File ~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880,</span> in <span class="ni">TextFileReader._make_engine</span><span class="nt">(self, f, engine)</span>
<span class="g g-Whitespace">   </span><span class="mi">1878</span>     <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mode</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1879</span>         <span class="n">mode</span> <span class="o">+=</span> <span class="s2">&quot;b&quot;</span>
<span class="ne">-&gt; </span><span class="mi">1880</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="n">get_handle</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1881</span>     <span class="n">f</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1882</span>     <span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1883</span>     <span class="n">encoding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoding&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1884</span>     <span class="n">compression</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;compression&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1885</span>     <span class="n">memory_map</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;memory_map&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1886</span>     <span class="n">is_text</span><span class="o">=</span><span class="n">is_text</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1887</span>     <span class="n">errors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoding_errors&quot;</span><span class="p">,</span> <span class="s2">&quot;strict&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1888</span>     <span class="n">storage_options</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;storage_options&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1889</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1890</span> <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">   </span><span class="mi">1891</span> <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="o">.</span><span class="n">handle</span>

<span class="nn">File ~/.local/lib/python3.10/site-packages/pandas/io/common.py:873,</span> in <span class="ni">get_handle</span><span class="nt">(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">868</span> <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">869</span>     <span class="c1"># Check whether the filename is to be opened in binary mode.</span>
<span class="g g-Whitespace">    </span><span class="mi">870</span>     <span class="c1"># Binary mode does not support &#39;encoding&#39; and &#39;newline&#39;.</span>
<span class="g g-Whitespace">    </span><span class="mi">871</span>     <span class="k">if</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">encoding</span> <span class="ow">and</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">872</span>         <span class="c1"># Encoding</span>
<span class="ne">--&gt; </span><span class="mi">873</span>         <span class="n">handle</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">874</span>             <span class="n">handle</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">875</span>             <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">876</span>             <span class="n">encoding</span><span class="o">=</span><span class="n">ioargs</span><span class="o">.</span><span class="n">encoding</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">877</span>             <span class="n">errors</span><span class="o">=</span><span class="n">errors</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">878</span>             <span class="n">newline</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">879</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">880</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">881</span>         <span class="c1"># Binary mode</span>
<span class="g g-Whitespace">    </span><span class="mi">882</span>         <span class="n">handle</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;/content/usa_mercedes_benz_prices.csv&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="c1"># Assuming your cleaned data is already loaded into the dataframe `data`</span>
<span class="c1"># If not, you&#39;ll need to insert your data loading and cleaning code here</span>

<span class="c1"># List of predictors, excluding the target variable &#39;Price&#39;</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s1">&#39;Price&#39;</span><span class="p">]</span>

<span class="c1"># Generate scatter plots for each predictor</span>
<span class="k">for</span> <span class="n">predictor</span> <span class="ow">in</span> <span class="n">predictors</span><span class="p">:</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">predictor</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Price&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Scatter plot of </span><span class="si">{</span><span class="n">predictor</span><span class="si">}</span><span class="s1"> vs Price&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="76ab9b36-3832-449a-8eed-5784507be9b4" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("76ab9b36-3832-449a-8eed-5784507be9b4")) {                    Plotly.newPlot(                        "76ab9b36-3832-449a-8eed-5784507be9b4",                        [{"hovertemplate":"Mileage=%{x}\u003cbr\u003ePrice=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[29636,5540,4890,29746,32631,3573,1254,23071,15755,2910,17499,3159,61419,15009,14021,3083,4425,12705,20431,61614,29909,12708,44343,10714,7709,28446,4559,38471,6110,55090,8115,31574,2236,11489,7795,39881,18398,15029,9795,37405,32905,16044,58175,46859,69676,21826,53331,15142,3518,14929,37995,22646,12708,36526,34707,17198,10625,4309,13665,68216,25277,30865,46460,45089,145,12718,25449,3937,40531,30790,38052,37955,20328,25850,2018,17046,7577,49565,40020,4188,10208,12829,48598,39165,6488,26270,5742,10148,7145,31248,15151,13576,8198,6282,12007,37992,1053,1549,10320,6889,44003,32608,3758,5538,25673,45089,10877,2336,16980,8213,7135,53509,6061,18458,69897,5192,8120,48608,14967,35788,19821,18901,10605,11770,16147,6149,4354,53699,6419,47533,15492,48928,45105,32001,1847,4389,3526,13740,4484,2505,2027,1264,7563,2435,19454,8257,3676,2900,1533,2195,29414,29065,16340,5848,49726,31830,15393,39115,10827,72646,7633,16385,23056,2489,14344,6451,18228,5462,4497,5146,15581,4140,35705,2355,29348,2333,7780,28718,6483,6989,15556,4885,7200,26280,5696,89199,892,35775,14494,4108,14908,5674,30882,45396,3600,11525,12603,15069,39256,1661,8544,3990,49180,8712,30349,9905,37037,30978,3699,21275,16300,11705,2767,29629,42188,25426,13804,21287,23695,21944,26835,23508,5936,6190,47210,21717,3036,6695,14150,346,3643,4354,2454,8859,39499,4143,7830,55945,12750,1233,10083,1999,7301,17886,8227,23795,28875,1643,21258,39723,23311,39468,16300,10636,46120,37252,9153,8400,56278,32012,27727,35534,9797,41105,4508,13074,46601,37229,2287,7957,12817,18139,2133,4138,12205,6348,23016,5147,3208,18737,19441,32076,16610,37968,47672,4164,5314,9491,21501,19356,49751,28862,72271,1638,1099,6488,7077,9911,24993,29118,55262,2810,31036,12000,37628,2878,4000,13297,16655,27265,2912,2518,3704,29694,66208,39881,10489,18039,26307,43673,30286,59907,2199,10799,25161,6058,17898,47054,49793,15180,9700,588,25804,6307,46381,37000,9649,10998,5439,25037,515,2183,2953,36812,44675,1581,3493,3284,77307,6182,1899,42066,7551,24144,13816,5492,4385,9649,10218,375,6540,25035,45827,10200,27066,39325,2897,134,43747,27751,52646,982,39865,45751,17741,25738,37048,1234,16385,11069,4031,29966,50401,4901,31448,2427,13459,4720,23023,6566,36001,80969,11100,5147,8695,16300,28040,16047,3160,5577,2708,56733,61358,33065,4913,8798,39807,70038,13660,37688,1357,32889,24247,65645,34042,9638,5218,5690,15375,33972,7485,25399,36238,35924,22406,9838,34000,9985,25932,15559,8146,2618,10401,16557,11441,1282,10143,2663,17088,5033,16887,8712,29344,35773,19679,1785,15245,19025,10525,9411,30474,18456,15078,9887,19165,9790,6929,13159,13467,34768,57471,4402,5003,15371,3171,2016,39606,11382,52587,5682,939,3879,28632,2705,40783,24923,7977,2960,7986,38475,71408,3812,4433,6451,2978,11692,10795,3688,71558,7793,33560,9654,23918,64064,21062,3538,57557,3051,41972,77546,11619,16543,30619,28255,4989,38370,43143,43722,8193,4009,2094,23096,39305,6665,13878,3160,7715,10806,7831,11291,8440,18327,30597,22365,9123,6307,194043,6569,7003,5534,51599,20822,23396,36278,7181,13744,9060,4740,4227,3855,9634,5018,11862,10378,16647,16345,6307,6999,26682,26732,46916,31000,14225,71405,5052,7964,5155,4840,33414,7455,29560,9957,46285,61628,23904,2030,24071,5052,4904,29920,38577,30622,4048,44646,5289,760,9450,13980,2986,7320,17867,5082,4989,2501,56282,27171,16796,5845,3171,6940,7402,29361,2677,7159,43020,52979,4358,9649,3378,10262,44582,36415,37441,2974,21568,2731,22991,11164,4466,4740,27480,9535,2220,5340,1905,4188,15146,17261,10342,17770,1713,30982,25842,8823,3132,37971,29636,26172,6379,2100,3755,9176,3275,15103,31372,1225,5355,5837,32031,46006,2418,3484,7556,5156,4855,10495,4885,21574,16411,4501,1774,33425,13087,22738,11937,17569,19950,80087,3604,15006,4021,1844,4964,9080,63281,7747,7881,5034,8362,73750,6173,4727,9743,8942,5889,29110,6409,12481,33072,1099,58162,9999,9885,3500,2157,50518,4131,9693,27317,4497,15009,8025,2621,42980,58175,35276,28960,28137,3095,31593,34539,2900,4928,4850,48750,18164,40746,71722,2662,36725,3750,20719,25952,9306,25068,4162,2283,88269,6675,2405,44755,8032,25880,7464,34539,4108,7325,23533,49497,6715,2971,26166,2296,15246,3008,7483,45637,6118,33160,1317,9707,6844,4238,37076,94804,12439,35902,56407,4873,17600,70749,3426,3809,22339,5577,37437,15578,4861,2152,9152,114,10120,31943,16061,44092,5368,14859,1449,39345,27775,41721,25609,31217,31191,8284,5755,10632,5819,20947,45089,21020,7860,11347,3410,6795,29598,1866,12143,37282,4705,24686,4354,2885,41695,7337,4950,24221,59951,8574,19684,14690,9055,3399,9551,9966,7479,6847,74871,50080,14102,6106,6802,32761,2873,14655,10489,5879,4205,14045,4713,6135,3272,89782,48699,3540,6255,5720,39275,52846,2070,72710,103,74318,13834,9092,2477,25819,5044,8901,5003,32322,8086,37100,29129,30883,9642,22386,9105,21902,18577,2625,60634,2977,1587,5157,56236,27411,5702,3505,35781,45067,11600,53616,9983,4818,2990,25583,4594,71732,20805,10778,7485,8256,5727,22540,25632,39241,7719,11270,27613,49400,38351,17120,2475,34708,9665,88180,6819,5447,39501,25914,9116,9366,23339,20420,32238,8126,4245,8523,2384,19607,23058,4138,58416,4167,11562,24368,19101,817,15150,39199,9298,3326,9030,3668,33042,50358,2354,35312,28891,11269,41887,6183,10096,16201,8898,18650,21632,3661,40516,1607,28628,3885,4663,6228,11776,42552,8659,7629,19605,39144,6362,7223,19926,3497,10525,3999,30835,3400,5801,38982,25518,7234,53031,3330,34549,15110,9955,2006,30765,2773,52740,36200,10590,5869,30450,33047,21359,18553,8150,36600,2413,4693,63952,8548,79619,45979,16645,9347,34144,9322,18885,20345,38094,36607,5181,20554,9300,32978,11080,4462,2629,11573,53524,10791,62241,15887,46090,6059,7617,55728,43243,43276,6494,3587,50048,2405,25540,24190,39652,2315,6906,33649,6427,33505,37197,10806,4107,29329,40257,3894,6083,8891,20939,38803,32190,3830,568,16860,19283,2771,12229,53030,45795,35060,32256,28137,7698,10336,35657,38758,2479,18153,19720,9474,49384,4691,9997,1401,38712,7333,28264,3002,49057,37925,1601,1307,2613,8571,2906,38720,27110,30104,44806,36313,34714,4218,42503,38523,6095,8791,6028,5144,9183,30868,29961,48954,11002,2663,23167,13329,6479,15074,15056,14462,36126,4189,1991,2722,22427,1321,42140,60506,7987,10554,4923,2715,34728,37815,11598,4431,21057,8186,45113,41950,6465,5725,6729,39050,895,2467,1518,28553,28519,4677,239,3259,9638,12317,15545,10198,10020,1893,21274,2517,33042,2644,28196,13704,19739,27908,6878,19623,11026,2503,22547,6087,1545,32035,38571,24549,31525,84617,7061,10694,3227,2470,10459,5157,4302,7149,8956,6516,6913,12889,12996,2462,1586,15190,3669,11770,4605,4461,39767,18810,50968,24434,2804,25321,12574,5740,31432,8539,2643,21010,22912,28730,56083,33334,11369,8029,49666,1301,19581,3513,7434,1234,1506,3535,61406,5600,4902,28841,3057,2977,2650,4497,61041,1066,42823,9780,2839,817,13893,25835,1501,8108,10765,1234,4216,33782,37050,36301,15233,54537,50301,13768,1250,7390,2822,18502,14407,5606,4471,21455,4163,24907,2507,2513,69818,21951,37971,10162,21466,5312,5522,3240,28684,44116,2139,18416,44798,36536,18160,34787,8781,26096,41258,39545,49249,5362,7972,27209,2951,27041,11428,1170,56189,3083,15328,27057,100,19195,7562,5613,23389,45586,19845,6634,35551,8221,7097,63195,24626,32393,1601,12784,5306,22881,27281,5958,8462,3931,35798,13705,38159,3737,3704,6418,5191,4383,29489,38205,11666,12002,4727,8603,14721,38609,3962,2910,1887,4979,4892,60383,11991,1729,13770,6202,3226,3563,43089,20339,1975,11472,1771,20371,3660,39332,64230,11613,9202,18813,22300,12092,37583,1750,10900,46840,36926,21887,12320,5321,36592,24234,22720,43853,21668,21300,8981,45054,6465,44144,34391,4800,8401,2750,3425,29021,5640,23019,1440,32982,6783,4016,69941,3641,38107,21332,37125,32441,2440,6171,1629,65654,21065,21600,34986,20975,67516,9363,23042,36882,12827,8400,31745,18940,20379,2370,45368,20270,36083,35155,29149,3949,53587,15027,13942,31481,12074,9169,3686,47183,49865,4801,14204,14064,389,13600,16960,25839,8765,9111,4383,8003,61693,33249,33352,37151,18664,32083,990,19987,73222,13162,3516,10152,54670,40497,34702,5448,25118,1502,22116,3211,22443,8408,6251,302,24609,43683,8720,42949,20214,26909,9553,1460,36005,11213,5466,4669,30789,4211,9080,7339,35442,12434,45330,37143,32428,33183,4753,5136,1077,39253,57746,2204,17593,25430,39903,5146,64725,45647,32975,27256,15203,50170,4111,9196,12721,64114,19980,16418,22481,24790,11082,38959,41533,30001,49095,22930,54832,6894,39450,10702,9723,851,38493,3299,3984,62102,3145,48823,2673,7332,28841,5797,17216,27929,30169,37309,14621,4193,19341,2552,7646,17101,26589,10773,2014,35735,15216,6629,33129,4764,61898,25954,21374,40403,14955,12083,5019,33169,20033,24028,13269,14589,11491,9841,3298,428,42812,23644,6663,42075,4162,98205,28998,23235,24320,18406,12769,22243,21296,14975,7459,2292,5572,11932,51007,60585,32455,29469,8330,8440,25120,15630,21283,7211,39805,5877,27657,2881,34596,34629,5993,45959,12353,5814,9583,52285,6813,40404,10842,23903,45050,30532,24897,50302,37104,26365,10632,3251,29616,16535,14427,461,4698,4561,13704,3000,15535,3413,18209,66169,7368,33636,22324,12532,12135,31961,8361,2272,46226,24377,24774,36126,5890,2077,5906,7638,13863,31953,4823,3165,42280,49307,2974,15426,24688,9146,8189,14688,15007,3292,3452,3987,49832,7129,1730,53204,40517,9768,26653,29938,9138,895,18820,37445,9600,11064,69244,14268,11473,22841,35558,9309,22233,10818,24901,3726,3606,6399,28406,7790,2509,745,40717,9200,4646,4701,5488,16424,9767,7281,7601,33875,21160,73117,28146,11886,42091,14059,12625,42143,33278,6227,7378,37820,35684,33653,30324,30278,38675,4826,12880,12401,27134,23199,6430,41370,49855,42748,6452,29260,12689,18567,16289,32458,44156,1156,17495,12328,42985,455,11596,13318,2914,36257,34125,27768,31829,1476,5120,7397,10626,44116,12114,8741,62833,19001,4194,11358,3521,54536,6534,27501,4820,21431,19065,4715,23423,418,63437,1694,53470,21517,1485,41373,5181,40741,15092,41019,20192,37049,36873,36550,49040,6662,5510,2870,59154,52349,7420,5350,8273,6566,19543,41403,7134,9963,12343,10532,2601,12867,10446,32041,2541,8849,63814,24318,7587,19320,5977,2570,44705,24468,20721,30066,43565,102,47424,61051,35973,85186,3390,10641,8485,3838,11440,29522,45251,11077,9707,21957,46057,7561,74431,6880,38471,39724,3011,29078,28870,21289,49974,525,38680,49047,44792,2688,42383,6460,14363,1287,3718,74827,7571,6781,37521,11238,26507,37322,300,27488,31445,43659,9424,40323,56517,17298,14983,8910,16756,8250,3042,5900,12142,2560,37933,25969,4438,1687,4721,21652,10637,4160,29986,11039,8045,26626,9924,2904,33140,4105,18128,2336,2961,4754,26579,38262,19819,34604,5424,19831,38314,17680,8028,36480,12336,33467,31753,10771,54169,13640,2105,75925,7388,9387,33108,10261,11309,29902,21758,5335,52678,5183,2112,9216,9233,16635,17308,7899,802,2609,7429,22967,40301,11072,14706,47720,25780,45385,5665,31270,4316,7122,5811,34511,10351,7354,10206,20027,36415,34140,20914,3045,34624,3876,12659,20954,10992,7338,4728,8311,28940,30249,13139,49275,13609,34546,47116,7991,8914,76127,4148,57378,13108,39070,17483,39564,1509,29006,24308,1046,4319,1563,47021,28538,26590,4526,8760,13002,9058,3309,32977,4489,1708,2881,5458,4566,100880,3640,111752,43517,41902,11653,26314,61467,31732,17089,9389,9446,5015,16806,20447,4625,10517,30056,36802,9696,4295,11879,32290,38784,4000,24813,42161,26797,4630,6484,24735,8251,6919,17159,25733,17179,5940,39938,9083,4904,3435,20049,24444,6372,17649,36542,4868,24011,32568,31884,4950,11124,14424,11054,38988,8908,1083,56635,51626,27931,7910,33478,41584,11779,739,10163,3854,7477,15554,35724,14999,11478,7232,49701,56113,36588,18322,24355,42203,69836,16413,9026,17216,9985,21632,25700,36887,3230,23458,32418,13446,65199,30021,33375,7865,24251,40655,21227,4677,32976,12431,1752,4797,4887,11466,22509,3033,10881,17241,29293,29601,16292,24404,27258,28209,34211,2801,68208,37536,35601,29854,34006,45852,33984,9992,31493,50728,23446,11204,21577,7433,3071,15697,6923,10679,33872,30512,3441,2020,22377,2820,1500,4812,2332,4202,1096,36022,19119,2830,3868,17809,14822,42485,27167,5976,1607,5574,44239,9204,1117,18838,11015,6944,57038,33460,40100,32910,38888,9161,20703,6405,7987,5279,33594,24682,16685,894,45012,3969,13739,34783,10800,7175,25533,54151,22673,27390,14821,3158,17450,13891,4605,27500,9761,5305,33569,14721,33787,15173,50979,20340,27979,10043,30932,48300,32055,3873,1403,13682,27110,8675,5275,12884,6218,9566,10097,23788,37143,9836,5970,7906,21743,34989,14564,34018,29337,16745,43171,28001,29054,55949,58797,8680,16498,28566,36728,59913,23325,1624,29156,3755,36609,10752,80457,12092,7042,22106,21454,5329,39682,29259,45813,34880,15847,2608,30387,17319,10498,25215,38195,26083,10629,3722,11026,4589,12068,7815,4879,19306,8555,5141,4959,33322,8472,13825,49968,20082,31307,26434,19756,9709,4266,8685,25702,19291,8412,7460,21953,21953,38991,60272,45570,26375,35901,27507,15008,19652,19302,9958,12283,7332,15637,31835,13173,18778,26928,9796,4917,28678,29868,12655,4156,13145,4353,12061,22114,21135,11102,405,3795,56206,956,41447,20405,5534,3223,30367,28802,23898,8298,46372,2701,26769,6430,12004,10051,10659,14362,2989,21237,23073,861,5879,5187,32655,2862,3623,43066,37344,51779,32122,18665,6124,2103,5673,28301,6087,11289,36687,73169,9806,7543,31736,5956,26007,9574,4500,3109,16926,31207,2239,18209,2955,5318,23432,14013,14817,7639,6166,9147,27403,23430,3694,19749,1890,15747,38222,22812,22812,35428,61136,16566,35659,30993,66555,25776,49666,16246,7572,7557,4504,23775,18730,1124,7722,11163,24894,38554,29630,3045,2524,28991,23652,21890,17942,29130,7675,1592,10126,30966,55431,31777,5750,10195,6938,25469,45980,9264,2475,33125,17591,35172,44087,39957,1777,2407,12170,2898,14984,24230,5070,1340,9950,58279,6308,1369,2595,12980,44596,34893,2549,1211,47528,48230,4153,2484,6916,5765,37650,47890,3071,18709,36814,54422,11395,30392,3413,6336],"xaxis":"x","y":[30900.0,139999.0,132999.0,58587.0,95990.0,74999.0,130999.0,55995.0,71999.0,62975.0,82895.0,50226.0,22495.0,63998.0,65024.0,69771.0,51995.0,57877.0,53977.0,22498.0,28784.0,67800.0,42595.0,74788.0,58770.0,53690.0,46900.0,38996.0,69306.0,39442.0,112749.0,57900.0,49995.0,153722.0,43855.0,50563.0,74887.0,59413.0,68983.0,46300.0,62421.0,198000.0,36900.0,62292.0,45991.0,50299.0,98984.0,65992.0,77700.0,65514.0,54660.0,143554.0,67800.0,54000.0,57463.0,84997.0,74377.0,57900.0,49913.0,21082.0,161775.0,79711.0,48500.0,50900.0,39950.0,54018.0,38963.0,107552.0,50595.0,29999.0,47562.0,50888.0,95981.0,98555.0,57445.0,94995.0,43105.0,55998.0,41000.0,39237.0,58798.0,30880.0,26499.0,33497.0,177595.0,69998.0,120997.0,142000.0,48000.0,73998.0,65444.0,69997.0,157500.0,45705.0,114750.0,41948.0,99805.0,39950.0,67910.0,60312.0,58997.0,35824.0,55995.0,70991.0,86690.0,50900.0,49400.0,60994.0,53922.0,67585.0,56940.0,62704.0,209995.0,55000.0,41690.0,44744.0,65999.0,49983.0,87390.0,111326.0,68500.0,67515.0,33999.0,31293.0,69997.0,99999.0,78900.0,36880.0,42499.0,20999.0,74988.0,28000.0,26540.0,94984.0,86975.0,77000.0,53900.0,112997.0,46888.0,97397.0,49777.0,41200.0,43777.0,68398.0,73990.0,49500.0,48846.0,45900.0,93888.0,47222.0,33951.0,59900.0,37650.0,59700.0,27412.0,57453.0,44196.0,45551.0,39864.0,41929.0,44777.0,30400.0,31336.0,107999.0,89999.0,98444.0,99795.0,105999.0,43900.0,47222.0,78899.0,44222.0,112498.0,46888.0,67983.0,55089.0,68274.0,76777.0,46777.0,199865.0,178899.0,43888.0,92499.0,48998.0,61991.0,33797.0,125997.0,149995.0,30880.0,46900.0,93882.0,53445.0,50754.0,108337.0,38740.0,38997.0,47721.0,34600.0,29397.0,173880.0,48995.0,50888.0,164882.0,157771.0,58702.0,35595.0,60146.0,79987.0,46888.0,60977.0,36295.0,85987.0,158880.0,65987.0,30502.0,35887.0,49995.0,35000.0,67500.0,45955.0,79894.0,64911.0,54750.0,108946.0,58998.0,139994.0,52888.0,41069.0,79998.0,101998.0,119899.0,78900.0,87902.0,39955.0,64992.0,77900.0,135990.0,20657.0,37870.0,69795.0,84690.0,61992.0,73499.0,47399.0,130994.0,46519.0,29598.0,119510.0,35878.0,46999.0,50477.0,142106.0,36295.0,52988.0,24862.0,54495.0,86881.0,46999.0,33316.0,32390.0,53436.0,43497.0,98995.0,28975.0,45999.0,79876.0,81921.0,85175.0,71998.0,48998.0,80991.0,39843.0,77802.0,57900.0,74995.0,52000.0,45712.0,106777.0,57642.0,49999.0,44777.0,49881.0,126995.0,30955.0,28990.0,104900.0,64443.0,34988.0,79997.0,58997.0,41994.0,127500.0,27620.0,44990.0,164900.0,177595.0,223414.0,72555.0,32998.0,73890.0,36991.0,77033.0,85500.0,89888.0,28988.0,51897.0,55900.0,89995.0,61805.0,98999.0,164994.0,49995.0,82980.0,81994.0,125690.0,50563.0,50905.0,98509.0,75850.0,49445.0,63987.0,45375.0,54777.0,61997.0,27477.0,47995.0,110950.0,30323.0,45950.0,41850.0,39999.0,144870.0,80594.0,109554.0,66413.0,99997.0,65595.0,99998.0,103044.0,50994.0,75998.0,123881.0,50194.0,45142.0,62000.0,63777.0,57499.0,103500.0,18578.0,96713.0,49988.0,35999.0,73800.0,89597.0,72490.0,42640.0,48932.0,65595.0,159990.0,56998.0,60495.0,34551.0,48350.0,41999.0,43399.0,86925.0,52744.0,89900.0,57007.0,28484.0,24999.0,49988.0,128887.0,61998.0,46995.0,43495.0,47647.0,46777.0,30400.0,85292.0,69287.0,50997.0,78999.0,72293.0,142825.0,53988.0,86249.0,56004.0,45982.0,29598.0,37995.0,28954.0,44999.0,80994.0,72865.0,36295.0,35897.0,44210.0,40000.0,221994.0,53895.0,86995.0,57988.0,41280.0,49995.0,40999.0,38200.0,23949.0,98140.0,27995.0,56997.0,47875.0,99777.0,39497.0,37490.0,76589.0,51500.0,67466.0,64876.0,33397.0,47893.0,87987.0,44898.0,36131.0,36991.0,64815.0,51490.0,47999.0,32300.0,85000.0,54987.0,58375.0,46099.0,78000.0,39999.0,150000.0,62988.0,63550.0,30904.0,46912.0,54998.0,157771.0,44997.0,38482.0,164900.0,74994.0,54981.0,169661.0,62988.0,73495.0,39100.0,38689.0,76286.0,64955.0,50988.0,203999.0,38288.0,179120.0,56994.0,64995.0,24688.0,65507.0,47882.0,80995.0,58883.0,45998.0,36125.0,42997.0,49495.0,49995.0,219995.0,44150.0,129995.0,46288.0,69998.0,33498.0,89990.0,58991.0,124988.0,39982.0,22913.0,109997.0,53983.0,98444.0,52495.0,55699.0,80654.0,55990.0,38492.0,44777.0,74999.0,40493.0,129618.0,53791.0,32339.0,63208.0,21900.0,59410.0,24991.0,20437.0,46980.0,41584.0,40970.0,30791.0,45994.0,26840.0,50991.0,27500.0,38497.0,43900.0,47777.0,49988.0,37539.0,44995.0,49850.0,43888.0,43860.0,61000.0,40016.0,52188.0,55997.0,141215.0,32000.0,34495.0,69992.0,109554.0,8999.0,44299.0,46417.0,44222.0,28995.0,28999.0,54981.0,26667.0,76987.0,40699.0,49884.0,45553.0,81000.0,43888.0,30966.0,48220.0,74442.0,70500.0,37037.0,69612.0,109554.0,42900.0,32298.0,29211.0,30500.0,29991.0,26885.0,69990.0,58003.0,77306.0,48982.0,44933.0,41497.0,40000.0,48950.0,41574.0,29000.0,21844.0,44185.0,134495.0,30985.0,58003.0,68990.0,27495.0,69892.0,32984.0,62777.0,42991.0,56111.0,61285.0,39987.0,41988.0,53777.0,42981.0,157991.0,74519.0,45994.0,72777.0,38995.0,58990.0,41990.0,85777.0,58883.0,118988.0,55994.378500823725,53900.0,43299.0,44421.0,51990.0,26995.0,45485.0,65595.0,40888.0,39995.0,37872.0,29495.0,41990.0,53275.0,104357.0,48888.0,80239.0,32795.0,38866.0,45553.0,48487.0,44699.0,69459.0,39577.0,49777.0,52627.0,61880.0,52500.0,36998.0,57981.0,50515.0,31695.0,85991.0,38000.0,77586.0,56900.0,30900.0,30877.0,66888.0,48000.0,57995.0,42989.0,55123.0,37893.0,69500.0,45500.0,44500.0,55989.0,57495.0,31777.0,40888.0,45900.0,36900.0,49997.0,44998.0,90000.0,49988.0,81494.0,41988.0,56001.0,45900.0,77898.0,37983.0,65466.0,97888.0,47950.0,38746.0,19498.0,72992.0,55650.0,52990.0,99495.0,39495.0,38900.0,41874.0,64777.0,39975.0,68910.0,50777.0,22992.0,58441.0,84614.0,44810.0,45988.0,43900.0,37978.0,43991.0,62706.0,29933.0,164900.0,32000.0,55984.0,77400.0,55777.0,159565.0,52751.0,42000.0,37994.0,33967.0,43900.0,46789.0,55184.0,53899.0,49990.0,36900.0,30752.0,95879.0,33695.0,49777.0,45968.0,53489.0,45900.0,55651.0,54495.0,55803.0,32989.0,29816.0,41998.0,58199.0,40995.0,90988.0,96838.0,41488.0,62981.0,30999.0,90000.0,42777.0,44162.0,46401.0,55117.0,59000.0,42300.0,28481.0,101888.0,43895.0,46900.0,46411.0,36633.0,76900.0,56777.0,219805.0,32337.0,42989.0,66888.0,55101.0,110084.0,30653.0,47495.0,30775.0,46999.0,109990.0,44788.0,66205.0,28395.0,16998.0,40670.0,27904.0,44963.0,55904.0,50988.0,54900.0,67800.0,47888.0,55936.0,221994.0,30423.0,43981.0,42495.0,69999.0,44997.0,229654.0,42787.0,46987.0,39490.0,33600.0,47605.0,28725.0,44990.0,28995.0,33395.0,40952.0,48988.0,43991.0,25999.0,84800.0,52888.0,41997.0,56156.0,46685.0,50900.0,46206.0,69989.0,154200.0,38954.0,67399.0,34984.0,50257.0,47346.0,56900.0,55444.0,73870.0,78900.0,42888.0,57980.0,58883.0,38350.0,42900.0,35918.0,65712.0,81891.0,43888.0,73777.0,49995.0,78933.0,45305.0,41500.0,48474.0,31991.0,24400.0,54900.0,78499.0,129999.0,37494.0,53658.0,55800.0,34537.0,57441.0,45988.0,43498.0,112999.0,44000.0,52000.0,25894.0,26995.0,47203.0,83982.0,42681.0,34726.0,49491.0,46444.0,45677.0,49000.0,26497.0,38599.0,42574.0,58999.0,27500.0,47305.0,40999.0,47882.0,30998.0,47308.0,48306.0,69800.0,32140.0,57899.0,35922.0,47509.0,30991.0,71888.0,45998.0,34377.0,64502.0,125880.0,68500.0,54811.0,44499.0,52660.0,50977.0,28795.0,39154.0,139250.0,39491.0,36988.0,83824.0,42991.0,48994.0,42487.0,33877.0,28475.0,41940.0,47893.0,49700.0,55835.0,38000.0,29625.0,33937.0,38444.0,62756.0,29250.0,32997.0,67890.0,51000.0,53484.0,35598.0,53148.0,68998.0,94777.0,72390.0,30583.0,45994.0,73345.0,38552.0,90000.0,154900.0,44990.0,54317.0,45888.0,39521.0,76992.0,90988.0,92896.0,57900.0,37823.0,77465.0,83777.0,39894.0,39500.0,67989.0,43992.0,36885.0,42300.0,37899.0,45645.0,77482.0,46991.0,38098.0,39659.0,44851.0,32589.0,52075.0,28995.0,46328.0,39494.0,45935.0,52713.0,34445.0,60833.0,44900.0,40007.0,68113.0,46688.0,63910.0,139977.0,39995.0,81593.0,50881.0,58796.0,36000.0,35595.0,41733.0,159979.0,45281.0,35650.0,45900.0,43988.0,63000.0,37995.0,70490.0,39900.0,42735.0,59546.0,44000.0,39592.0,54999.0,30934.0,38995.0,50284.0,40775.0,71989.0,47502.0,53510.0,37974.0,54480.0,40998.0,37500.0,47791.0,33295.0,50639.0,112998.0,55991.0,49888.0,53910.0,43881.0,39988.0,28000.0,29052.0,75890.0,63491.0,27995.0,40900.0,27250.0,40495.0,31416.0,35475.0,59750.0,38995.0,41999.0,32230.0,40990.0,42995.0,45000.0,96510.0,23991.0,38999.0,33738.0,38594.0,50555.0,41981.0,51788.0,26199.0,28888.0,31593.0,88977.0,60900.0,43498.0,129991.0,31652.0,31756.0,46998.0,169995.0,47100.0,27990.0,52983.0,33750.0,32998.0,87851.0,42995.0,33991.0,30998.0,39634.0,52892.0,32485.0,35988.0,72666.0,58998.0,54999.0,54994.0,38314.0,34650.0,224891.0,41999.0,37500.0,29996.0,43345.0,29500.0,31933.0,36988.0,38989.0,36390.0,70164.0,52777.0,36852.0,71994.0,43888.0,36900.0,55900.0,49444.0,55777.0,61999.0,58880.0,51469.0,54329.0,23199.0,34500.0,89500.0,144999.0,46690.0,43992.0,71892.0,40684.0,41888.0,40102.0,46315.0,27825.0,43988.0,42700.0,30979.0,34238.0,58988.0,39400.0,47644.0,42344.0,40600.0,41594.0,44888.0,51933.0,40997.0,53686.0,30840.0,50997.0,43000.0,209894.0,60566.0,55998.0,34991.0,50792.0,57774.0,60470.0,47990.0,51555.0,29990.0,37890.0,216999.0,50991.0,53000.0,39997.0,42892.0,31152.0,44990.0,45925.0,32685.0,61997.0,31562.0,41200.0,48400.0,46977.0,50788.0,56739.0,71892.0,77945.0,53965.0,28013.0,26433.0,47044.0,218937.0,62900.0,67897.0,40000.0,41964.0,45693.0,42963.0,60000.0,39485.0,49779.0,34900.0,85999.0,34522.0,47952.0,189888.0,43905.0,48006.0,46995.0,40989.0,48999.0,44941.0,51971.0,76765.0,29526.0,28999.0,35988.0,44932.0,55900.0,39480.0,41999.0,44989.0,57505.0,75999.0,46999.0,47804.0,95997.0,54395.0,40894.0,51995.0,61998.0,53997.0,53717.0,72007.0,39964.0,41495.0,49991.0,47900.0,37496.0,34833.0,137900.0,29275.0,36839.0,219894.0,34977.0,39400.0,69867.0,29960.0,39988.0,49998.0,41498.0,31998.0,34746.0,42994.0,35888.0,32928.0,72550.0,24990.0,50999.0,30735.0,70226.0,51507.0,100804.0,53698.0,45244.0,27695.0,86989.0,46939.0,65991.0,46500.0,64986.0,50481.0,42495.0,30960.0,129351.0,59991.0,41900.0,47983.0,40983.0,46220.0,30995.0,50999.0,55110.0,42200.0,47888.0,43977.0,53974.0,165994.0,34690.0,43998.0,38000.0,21958.0,85777.0,60125.0,84994.0,38795.0,31495.0,77991.0,43481.0,50384.0,36888.0,41600.0,33896.0,103995.0,51999.0,26565.0,30987.0,55990.0,41990.0,32395.0,46446.0,52835.0,52835.0,30972.0,38899.0,67035.0,45844.0,58884.0,79988.0,34998.0,29892.0,49475.0,165997.0,26900.0,41000.0,31988.0,41300.0,48777.0,30995.0,57425.0,48000.0,128388.0,54776.0,48991.0,45994.0,109995.0,30756.0,49777.0,39989.0,52496.0,47300.0,33233.0,36985.0,56991.0,48000.0,44900.0,43777.0,79991.0,35986.0,41900.0,60200.0,66876.0,69777.0,41000.0,38000.0,35900.0,45777.0,55999.0,92857.0,67997.0,77995.0,30888.0,40563.0,61994.0,97892.0,54493.0,42000.0,47988.0,41990.0,46000.0,56997.0,53555.0,46908.0,33795.0,54954.0,73890.0,56444.0,57990.0,51555.0,169999.0,30999.0,44480.0,74999.0,53840.0,61777.0,94001.0,55212.0,39874.0,43440.0,78820.0,119995.0,71988.0,51696.0,52473.0,29527.0,69991.0,44998.0,68204.0,30994.0,38999.0,34410.0,38893.0,43555.0,85883.0,41000.0,29990.0,94983.0,69846.0,45777.0,43987.0,31987.0,77000.0,34989.0,36400.0,32817.0,58000.0,32995.0,52399.0,39988.0,33950.0,55521.0,36742.0,46295.0,51860.0,28496.0,49999.0,42988.0,55414.0,82994.0,41884.0,45998.0,29695.0,41432.0,28999.0,52466.0,47735.0,42986.0,67994.0,41973.0,43988.0,30500.0,47695.0,79799.0,33833.0,31773.0,29883.0,40900.0,63874.0,56481.0,55963.0,41985.0,29997.0,55900.0,38597.0,72765.0,24899.0,53441.0,29999.0,55995.0,38692.0,46398.0,21360.0,79700.0,52894.0,27981.0,33890.0,41990.0,43777.0,25632.0,48000.0,55349.0,41492.0,37616.0,48777.0,31925.0,38494.0,29890.0,43777.0,43983.0,51788.0,41883.0,32999.0,30933.0,45998.0,38995.0,35991.0,36884.0,98482.0,46692.0,24998.0,39800.0,128991.0,39988.0,27598.0,53332.0,51901.0,46098.0,83890.0,68853.0,81004.0,75007.0,34354.0,79991.0,51991.0,198888.0,55888.0,37295.0,43883.0,42588.0,64991.0,33989.0,48988.0,45000.0,43573.0,56894.0,65030.0,49983.0,38991.0,49145.0,46235.0,59990.0,31994.0,32309.0,32000.0,79990.0,42496.0,56775.0,45996.0,43600.0,45997.0,30988.0,45992.0,187987.0,176988.0,29893.0,34683.0,50000.0,31890.0,44800.0,29888.0,36789.0,34995.0,43990.0,42763.0,42775.0,114895.0,25983.0,182975.0,50000.0,44157.0,37700.0,42793.0,26880.0,39562.0,37259.0,36990.0,47998.0,35250.0,34597.0,48888.0,40039.0,41777.0,38761.0,30989.0,53983.0,47840.0,39699.0,48250.0,45699.0,52777.0,55004.0,42916.0,39495.0,32900.0,170882.0,30295.0,45768.0,41990.0,55498.0,98900.0,70990.0,53476.0,30337.0,32204.0,45990.0,72488.0,29894.0,52997.0,50435.0,69963.0,41991.0,28989.0,97999.0,29128.0,48700.0,53637.0,109977.0,43892.0,36866.0,39894.0,31894.0,63486.0,199860.0,77900.0,62988.0,49983.0,54995.0,39997.0,42880.0,40996.0,41100.0,44998.0,21260.0,35500.0,38494.0,36999.0,95984.0,87993.0,47845.0,39981.0,159990.0,60777.0,75445.0,67960.0,73980.0,35678.0,44988.0,28886.0,169999.0,47261.0,44995.0,29999.0,39948.0,36221.0,43995.0,84950.0,45000.0,29946.0,53484.0,69888.0,51989.0,46307.0,47983.0,35233.0,46000.0,42595.0,24586.0,84444.0,165966.0,55997.0,63899.0,47800.0,26999.0,26140.0,25994.0,37900.0,51200.0,41000.0,47983.0,27998.0,41990.0,87770.0,51499.0,40900.0,49983.0,67984.0,40501.0,64484.0,91433.0,29886.0,19990.0,47131.0,27750.0,37417.0,58594.0,61988.0,36851.0,91888.0,48500.0,133448.0,48787.0,42999.0,27583.0,54840.0,41998.0,45994.0,66990.0,75450.0,47851.0,56555.0,99900.0,32221.0,132228.0,48000.0,37393.0,75008.0,38991.0,36991.0,40122.0,66690.0,39900.0,64991.0,64900.0,27749.0,46800.0,49000.0,29989.0,53988.0,43545.0,44562.0,64000.0,63933.0,44000.0,39894.0,47618.0,81285.0,79900.0,15888.0,41403.0,212894.0,31881.0,49885.0,81894.0,30698.0,31986.0,56999.0,73362.0,47983.0,37882.0,36991.0,44977.0,118500.0,92676.0,27497.0,42000.0,50490.0,170388.0,129900.0,53900.0,47675.0,37894.0,37661.0,34984.0,192997.0,43777.0,42900.0,41990.0,39877.0,41939.0,39984.0,31000.0,42974.0,43465.0,38986.0,32998.0,45991.0,38595.0,32900.0,29999.0,71356.0,56633.0,47307.0,61988.0,32995.0,39459.0,38741.0,31997.0,30990.0,71995.0,41874.0,119255.0,43781.0,66164.0,77933.0,49900.0,29862.0,54800.0,31654.0,50500.0,26615.0,93100.0,91358.0,29999.0,53999.0,27990.0,70497.0,51997.0,30998.0,37844.0,47990.0,40441.0,44998.0,44995.0,62500.0,206885.0,25900.0,35998.0,50997.0,46997.0,73777.0,30978.0,56777.0,36595.0,43712.0,47575.0,69111.0,43803.0,55988.0,112999.0,25998.0,40877.0,29984.0,54995.0,117999.0,25991.0,68523.0,31383.0,70994.0,35988.0,55265.0,47988.0,30439.0,26650.0,29853.0,39900.0,39600.0,91999.0,39983.0,29990.0,39891.0,41600.0,51990.0,218554.0,32294.0,30991.0,79900.0,62988.0,62988.0,33495.0,41888.0,43820.0,217994.0,29795.0,54999.0,46809.0,39870.0,46999.0,37995.0,36395.0,41990.0,78500.0,49722.0,35993.0,31789.0,32855.0,29933.0,65555.0,47870.0,108670.0,58900.0,33639.0,71360.0,39890.0,39500.0,57835.0,51999.0,34994.0,29995.0,44611.0,44004.0,40364.0,30304.0,55992.0,24888.0,52900.0,43892.0,48999.0,52995.0,31900.0,32500.0,48794.0,41855.0,92925.0,56499.0,25594.0,33977.0,47755.0,28997.0,54777.0,162900.0,139891.0,45988.0,25998.0,45688.0,62545.0,50393.0,45493.0,45450.0,28892.0,60950.0,44029.0,30998.0,29997.0,43590.0,47750.0,25790.0,41500.0,45991.0,36988.0,51933.0,44900.0,49795.0,63989.0,40984.0,41994.0,27496.0,28999.0,53896.0,73993.0,43995.0,63995.0,66983.0,49990.0,36980.0,42990.0,51588.0,29994.0,43592.0,64500.0,45500.0,44900.0,96991.0,71500.0,46777.0,49983.0,29690.0,41880.0,48000.0,104995.0,46822.0,73922.0,63998.0,26690.0,73887.0,35494.0,81225.0,80893.0,29991.0,41221.0,24999.0,45997.0,52983.0,39699.0,38961.0,62988.0,31500.0,37350.0,64493.0,32190.0,74893.0,59933.0,40495.0,82444.0,44995.0,33865.0,37988.0,51989.0,61693.0,114500.0,43848.0,64498.0,41492.0,49988.0,26999.0,39974.0,45798.0,35900.0,30489.0,23980.0,44555.0,94898.0,39885.0,49500.0,45440.0,39493.0,65999.0,44990.0,34958.0,31563.0,163981.0,32882.0,31000.0,47000.0,24991.0,45777.0,63995.0,33756.0,45624.0,56998.0,41900.0,63798.0,116963.0,33392.0,34111.0,38368.0,43890.0,59900.0,35696.0,43400.0,36882.0,35998.0,48488.0,66991.0,198888.0,43997.0,48885.0,60888.0,53225.0,39999.0,31756.0,45898.0,130000.0,50426.0,33493.0,33989.0,39562.0,65973.0,90972.0,58999.0,52777.0,58903.0,35248.0,57555.0,44876.0,63500.0,41400.0,55490.0,20991.0,53777.0,20647.0,32928.0,32282.0,90971.0,30840.0,38880.0,47894.0,31195.0,39997.0,69986.0,56895.0,46273.0,71995.0,55777.0,37442.0,35987.0,41555.0,60880.0,42900.0,45990.0,27890.0,30993.0,70977.0,26979.0,38562.0,40650.0,38300.0,67991.0,41986.0,44000.0,53771.0,32900.0,37989.0,49998.0,43498.0,31488.0,43800.0,74320.0,47050.0,32892.0,27998.0,42495.0,53505.0,49184.0,67393.0,54596.0,61577.0,36991.0,76990.0,39661.0,40490.0,41451.0,37997.0,44480.0,76994.0,29795.0,31750.0,178000.0,44580.0,32495.0,41780.0,38523.0,52777.0,41507.0,39988.0,37990.0,56954.0,166230.0,50992.0,36900.0,69801.0,45495.0,17998.0,29946.0,31933.0,30995.0,47598.0,20754.0,32596.0,66500.0,99998.0,41444.0,42991.0,72563.0,49990.0,52996.0,30500.0,45651.0,32962.0,25894.0,33989.0,28998.0,39692.0,30986.0,40981.0,43777.0,99884.0,42250.0,54894.0,66777.0,45393.0,117981.0,97973.0,33988.0,64777.0,40594.0,52990.0,28818.0,32818.0,35500.0,31500.0,32000.0,29999.0,37988.0,149495.0,26995.0,72999.0,27999.0,36958.0,72990.0,33988.0,33390.0,36393.0,25736.0,40490.0,30880.0,41000.0,34656.0,61991.0,45990.0,48043.0,51045.0,39672.0,47500.0,31998.0,47840.0,53991.0,34409.0,135640.0,53342.0,46998.0,57315.0,127598.0,78585.0,72895.0,59000.0,89984.0,45950.0,32725.0,39991.0,51505.0,38798.0,49695.0,228323.0,43149.0,54998.0,67750.0,65893.0,49888.0,55998.0,38779.0,147900.0,30383.0,55791.0,31145.0,43995.0,41800.0,31990.0,42999.0,48253.0,43884.0,52887.0,32988.0,60575.0,99999.0,75982.0,40777.0,112999.0,36854.0,115991.0,48125.0,40987.0,26988.0,33534.0,27933.0,39490.0,45888.0,30000.0,82900.0,53555.0,40499.0,68777.0,104992.0,56705.0,33960.0,30595.0,32933.0,61844.0,36784.0,34900.0,44000.0,31707.0,42998.0,31750.0,84000.0,53299.0,40777.0,29933.0,33555.0,52905.0,34888.0,55901.0,44000.0,49480.0,40385.0,27996.0,73478.0,33498.0,38998.0,37918.0,32544.0,41863.0,36990.0,46545.0,39997.0,40982.0,37916.0,61291.0,34444.0,150299.0,52989.0,38000.0,63750.0,45822.0,25200.0,50997.0,49989.0,34991.0,46900.0,29584.0,34895.0,19677.0,163998.0,38794.0,31998.0,88933.0,45977.0,30490.0,52995.0,38780.0,47995.0,65991.0,94000.0,65480.0,31379.0,46500.0,33933.0,34890.0,71998.0,116991.0,43900.0,60000.0,46900.0,89585.0,173998.0,40525.0,29950.0,60805.0,37999.0,39390.0,35780.0,39862.0,54991.0,29894.0,43993.0,72999.0,30834.0,31998.0,209991.0,43893.0,44965.0,59993.0,49741.0,52900.0,48499.0,48890.0,48890.0,29524.0,32998.0,37840.0,40000.0,47588.0,33870.0,37989.0,36992.0,59995.0,51994.0,47777.0,48989.0,49987.0,72661.0,37813.0,79990.0,34988.0,49900.0,72374.0,28860.0,29998.0,44834.0,70065.0,97000.0,75000.0,39965.0,56299.0,75900.0,41996.0,54500.0,49680.0,25499.0,57140.0,45484.0,61930.0,51565.0,73994.0,33999.0,30799.0,33892.0,55000.0,41999.0,56795.0,29995.0,39879.0,172500.0,64107.0,40000.0,38865.0,51900.0,27685.0,35933.0,72991.0,36994.0,57499.0,33990.0,54888.0,67755.0,65936.0,32633.0,48777.0,24997.0,34250.0,77506.0,50777.0,77850.0,34449.0,97892.0,43777.0,61577.0,67989.0,46225.0,48558.0,28500.0,50400.0,44987.0,60597.0,46900.0,49988.0,53900.0,51998.0,48777.0,68999.0,75994.0,54900.0,34887.0,30945.0,39597.0,44984.0,50778.0,45777.0,32544.0,29893.0,68290.0,31950.0,54998.0,85894.0,29889.0,41291.0,41291.0,28550.0,49581.0,91988.0,25976.0,31000.0,27999.0,29599.0,30497.0,36750.0,39991.0,73994.0,45990.0,41558.0,193901.0,89989.0,54498.0,47781.0,31995.0,30740.0,33850.0,88799.0,40558.0,31489.0,20995.0,30799.0,56995.0,39900.0,56999.0,45777.0,32000.0,46900.0,27500.0,36792.0,39998.0,36382.0,48157.0,28351.0,40500.0,46777.0,69880.0,33950.0,152986.0,31195.0,23168.0,40999.0,229918.0,72545.0,97575.0,64215.0,63994.0,87599.0,39200.0,57499.0,63995.0,56695.0,38760.0,94989.0,74989.0,44933.0,29500.0,34995.0,57153.0,51763.0,30900.0,20562.0,57555.0,89998.0,45877.0,117495.0,31992.0,35416.0,85999.0,65886.0,75577.0,53490.0,89599.0,32890.0,53983.0,46035.0],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Mileage"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Price"}},"legend":{"tracegroupgap":0},"title":{"text":"Scatter plot of Mileage vs Price"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('76ab9b36-3832-449a-8eed-5784507be9b4');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div><div class="output text_html"><html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="5e6ee982-c4a7-4332-b18d-d8b8002523c9" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("5e6ee982-c4a7-4332-b18d-d8b8002523c9")) {                    Plotly.newPlot(                        "5e6ee982-c4a7-4332-b18d-d8b8002523c9",                        [{"hovertemplate":"Rating=%{x}\u003cbr\u003ePrice=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[3.9,4.7,4.7,4.8,5.0,4.7,4.7,3.2,4.8,4.5,4.7,4.6,4.0,4.8,4.3,4.9,4.0,4.5,4.7,4.5,4.533723653395785,4.7,3.8,4.0,4.3,4.7,4.9,4.8,4.7,4.6,4.6,4.8,4.7,4.7,4.3,4.533723653395785,4.7,4.7,4.8,4.2,4.5,3.0,3.9,4.5,4.533723653395785,4.8,4.4,4.533723653395785,4.7,4.533723653395785,4.533723653395785,4.5,4.7,4.7,4.9,4.3,4.0,4.7,4.7,4.8,4.5,4.3,4.9,3.9,4.533723653395785,4.6,4.2,4.9,4.6,4.533723653395785,4.3,4.533723653395785,4.8,4.3,4.0,4.9,4.3,4.5,4.533723653395785,4.533723653395785,4.533723653395785,4.533723653395785,4.533723653395785,5.0,4.9,4.5,4.3,4.533723653395785,4.8,4.6,4.533723653395785,4.8,4.9,4.8,4.3,4.8,4.7,4.533723653395785,4.7,4.6,4.3,4.8,4.0,4.533723653395785,4.7,3.9,4.533723653395785,3.4,4.5,4.3,4.6,4.6,4.9,4.3,4.7,4.9,4.6,4.8,4.7,4.6,4.8,4.533723653395785,4.6,4.8,4.8,4.7,3.9,4.6,4.7,4.6,4.8,4.533723653395785,4.7,4.8,4.7,4.8,4.7,4.3,4.6,4.4,4.7,4.533723653395785,4.6,4.533723653395785,5.0,4.533723653395785,4.4,3.9,4.4,4.6,4.2,4.5,4.7,4.533723653395785,4.8,4.7,4.8,5.0,4.6,4.533723653395785,4.6,4.533723653395785,4.533723653395785,4.7,4.7,4.9,4.8,4.3,3.9,4.6,4.7,4.6,4.5,4.6,4.8,4.6,4.533723653395785,4.7,4.6,4.7,4.7,4.6,4.8,4.3,4.2,4.4,4.3,4.2,3.7,3.9,4.8,4.0,4.9,4.6,4.0,5.0,4.8,4.0,4.4,3.7,4.7,4.6,4.5,4.9,4.9,4.1,4.8,4.7,4.6,4.7,4.9,4.8,3.7,3.8,4.8,4.8,4.0,4.8,4.6,4.7,4.8,4.5,4.9,4.8,4.5,4.8,4.6,4.7,4.6,4.5,4.9,3.9,4.7,4.9,3.6,4.7,4.9,4.8,4.8,4.533723653395785,4.7,4.8,4.4,4.1,4.7,4.9,4.4,4.8,4.8,4.7,4.3,4.3,4.9,4.8,4.533723653395785,3.2,4.5,4.533723653395785,4.8,4.7,4.3,4.3,4.1,4.5,4.7,4.6,4.8,4.4,4.8,4.6,4.6,4.9,4.7,3.9,4.0,4.2,4.9,4.0,4.4,4.6,4.7,4.5,4.5,4.3,4.533723653395785,4.9,4.4,4.8,4.3,4.533723653395785,4.533723653395785,4.8,4.7,4.6,3.9,4.9,4.7,4.3,4.7,4.3,4.4,4.3,4.9,4.3,4.8,4.4,4.8,4.1,4.533723653395785,4.7,3.4,3.2,4.0,4.533723653395785,4.7,4.533723653395785,4.8,4.0,4.7,4.533723653395785,3.6,4.5,4.7,5.0,4.3,4.0,4.7,4.6,4.6,4.7,4.533723653395785,4.6,4.8,4.9,4.8,4.9,4.9,4.7,4.0,4.533723653395785,4.7,4.5,4.8,4.9,2.7,4.4,3.0,4.533723653395785,4.7,4.7,4.5,3.1,4.0,4.3,4.2,4.7,4.3,4.9,4.7,4.0,5.0,4.3,4.7,4.533723653395785,4.7,4.0,4.9,4.533723653395785,4.3,4.533723653395785,4.3,4.4,4.3,4.4,4.5,3.4,4.1,4.7,4.533723653395785,4.7,4.8,4.9,4.8,4.6,4.0,4.5,4.3,4.9,4.8,4.8,3.4,4.5,2.5,4.4,4.8,4.9,4.3,4.2,4.7,4.9,4.9,4.0,4.533723653395785,4.2,4.0,4.3,4.2,4.5,4.8,4.533723653395785,4.3,4.5,4.0,5.0,4.5,4.6,4.8,4.9,4.8,4.4,4.7,3.6,4.8,4.7,4.3,4.5,4.2,4.6,4.8,4.2,4.4,4.1,4.7,4.533723653395785,3.1,4.7,4.5,4.6,4.5,4.9,4.4,4.9,4.3,4.8,3.4,4.7,3.2,4.7,4.5,4.5,4.6,4.6,4.7,4.5,4.8,4.7,4.9,4.7,4.533723653395785,4.9,5.0,3.3,4.9,5.0,4.9,4.4,4.9,5.0,4.7,4.0,4.8,4.8,4.7,4.6,4.6,4.9,3.7,4.533723653395785,4.5,4.8,4.8,4.3,4.8,4.9,4.9,4.533723653395785,4.0,5.0,4.7,4.6,4.5,4.533723653395785,4.533723653395785,4.533723653395785,4.4,4.4,4.4,4.1,4.2,4.7,4.6,4.8,4.7,3.1,4.9,4.9,4.533723653395785,4.4,4.3,4.8,4.7,4.5,4.9,4.9,4.7,4.6,4.6,4.533723653395785,4.3,4.4,4.3,2.9,4.6,4.1,4.6,4.9,4.3,4.4,4.9,4.6,4.1,4.6,5.0,4.7,4.8,3.0,4.5,4.9,3.0,4.6,4.6,4.8,4.9,4.0,4.1,4.4,4.9,4.7,4.6,4.533723653395785,4.533723653395785,4.7,4.7,4.7,4.9,4.6,4.7,4.4,3.4,4.533723653395785,4.533723653395785,4.5,4.7,4.533723653395785,4.8,4.6,4.533723653395785,4.9,4.9,4.9,4.8,4.8,4.6,4.3,4.9,4.0,4.3,4.533723653395785,4.7,3.2,4.5,4.7,4.9,4.7,4.2,4.533723653395785,4.533723653395785,4.6,4.9,4.533723653395785,4.5,4.533723653395785,4.533723653395785,4.8,4.533723653395785,4.0,4.7,4.9,4.6,3.2,2.9,3.2,4.533723653395785,4.0,4.6,4.6,4.3,3.2,4.4,4.9,4.3,4.533723653395785,4.8,4.2,4.7,4.8,4.533723653395785,4.5,4.3,4.2,4.6,3.2,4.5,4.533723653395785,4.6,4.9,3.9,4.533723653395785,4.5,4.8,4.5,4.4,4.6,4.8,4.7,4.8,4.533723653395785,4.533723653395785,4.533723653395785,4.4,4.6,3.9,4.2,4.7,4.6,4.533723653395785,4.7,4.533723653395785,4.533723653395785,4.9,4.8,5.0,4.8,4.6,4.4,4.9,4.533723653395785,4.5,5.0,4.7,4.6,4.0,3.2,4.0,4.9,4.0,4.5,4.7,4.6,4.5,4.9,4.3,4.9,4.5,4.7,4.6,3.6,4.9,4.8,3.9,4.533723653395785,4.4,4.0,4.7,4.7,4.7,4.533723653395785,4.533723653395785,4.6,3.9,4.7,4.533723653395785,4.533723653395785,4.4,3.9,4.533723653395785,4.6,4.5,4.7,4.9,4.533723653395785,3.9,4.9,4.9,4.533723653395785,4.6,4.9,4.6,4.4,4.0,4.5,4.6,4.6,4.2,4.7,4.9,4.2,4.8,4.7,4.6,4.533723653395785,4.9,3.2,4.5,4.0,3.9,3.2,4.533723653395785,4.8,4.7,4.7,4.9,4.2,4.5,4.6,4.9,4.6,4.9,4.9,4.533723653395785,4.533723653395785,4.5,4.533723653395785,3.8,4.9,4.6,4.8,4.6,4.7,4.5,4.7,4.2,4.5,4.7,4.9,4.8,4.2,4.9,4.6,4.3,4.7,4.8,4.8,4.3,4.533723653395785,4.7,4.0,4.8,4.7,4.7,4.6,4.8,4.533723653395785,4.533723653395785,4.0,4.5,4.3,4.7,4.5,3.9,4.5,4.2,4.8,4.9,4.5,4.8,4.6,4.533723653395785,4.9,4.9,3.0,3.9,4.4,4.533723653395785,4.9,4.6,3.9,4.8,4.3,4.8,4.8,4.6,4.533723653395785,4.8,4.7,4.8,4.6,4.9,4.8,4.533723653395785,4.5,4.7,4.533723653395785,4.9,4.0,4.533723653395785,4.4,4.4,5.0,4.7,4.8,4.8,4.8,4.9,4.7,4.8,4.4,4.533723653395785,4.5,4.9,3.5,4.533723653395785,4.5,4.5,4.5,4.0,4.6,4.7,4.533723653395785,4.9,4.6,4.7,4.5,2.7,4.3,3.6,4.6,4.7,5.0,4.8,4.6,4.3,4.7,4.533723653395785,4.533723653395785,4.5,4.533723653395785,4.1,4.6,4.2,4.9,4.6,4.5,4.5,4.3,4.533723653395785,4.6,4.4,4.5,4.5,4.6,4.7,4.9,4.4,4.7,4.0,4.7,4.533723653395785,4.3,4.3,4.533723653395785,4.8,4.0,4.9,4.6,4.533723653395785,4.5,4.7,4.7,4.8,4.7,4.3,4.4,4.0,3.4,4.4,4.6,4.6,4.4,4.7,4.4,4.4,3.9,4.7,4.0,4.7,4.8,4.8,4.2,4.8,4.6,4.533723653395785,4.533723653395785,4.8,4.8,4.9,5.0,4.533723653395785,4.9,4.9,4.533723653395785,4.9,4.6,4.9,4.5,4.6,4.533723653395785,4.8,3.9,4.3,4.3,4.9,3.9,4.7,4.9,4.6,4.5,4.6,4.6,4.9,4.8,4.9,4.4,4.7,3.9,4.533723653395785,4.8,4.7,3.9,4.7,4.6,4.6,4.533723653395785,4.5,4.7,4.7,4.9,4.533723653395785,4.7,4.533723653395785,4.7,4.0,4.0,4.533723653395785,5.0,4.6,4.7,4.9,4.8,4.0,4.7,4.533723653395785,4.8,4.4,4.8,4.7,4.8,4.3,4.3,4.9,4.8,4.6,4.5,4.6,4.4,4.7,4.9,4.533723653395785,4.0,4.533723653395785,4.9,4.8,4.5,4.9,4.9,4.533723653395785,4.533723653395785,4.2,4.4,4.5,4.533723653395785,4.9,4.9,4.7,3.9,5.0,4.9,4.9,4.7,4.6,4.5,4.6,4.8,4.7,4.6,4.7,4.9,3.2,4.5,4.7,4.8,4.533723653395785,4.7,4.8,4.6,4.4,4.7,4.5,4.533723653395785,4.7,4.8,4.8,4.533723653395785,4.6,4.8,4.8,4.533723653395785,4.4,4.5,4.5,4.7,4.4,4.533723653395785,4.8,3.4,4.9,4.533723653395785,4.9,4.4,4.9,4.8,4.8,4.3,4.5,4.533723653395785,4.533723653395785,4.7,4.8,5.0,4.9,4.3,4.8,4.7,4.8,4.6,4.5,4.8,4.4,4.7,4.3,4.0,4.9,4.4,4.0,4.8,4.8,4.8,4.3,4.3,4.3,4.2,4.9,4.8,4.533723653395785,5.0,4.533723653395785,4.4,4.9,4.0,4.6,4.5,4.533723653395785,4.9,4.7,4.533723653395785,4.8,5.0,4.9,4.8,4.6,3.9,4.5,4.3,4.7,4.2,4.7,4.7,4.5,4.3,4.8,4.0,4.0,4.5,4.6,4.9,4.8,4.7,4.4,4.0,4.4,4.8,4.7,4.5,4.7,4.6,4.5,4.7,4.5,4.5,4.5,4.8,4.9,4.7,4.8,4.6,4.4,4.5,4.0,4.9,4.8,4.8,4.2,4.7,4.533723653395785,5.0,4.2,4.3,4.8,4.7,4.7,4.3,4.5,4.8,4.0,5.0,5.0,4.3,4.3,4.8,4.9,4.9,3.9,4.6,4.533723653395785,4.7,4.0,4.6,4.8,4.4,4.0,4.9,4.4,4.5,4.9,4.9,4.3,4.8,4.7,4.8,4.3,3.7,4.533723653395785,4.6,4.7,4.6,4.7,4.4,4.3,4.9,4.0,5.0,4.0,4.533723653395785,4.4,4.533723653395785,3.2,4.9,4.7,4.7,4.9,4.533723653395785,4.8,4.8,4.8,4.533723653395785,4.6,4.533723653395785,4.0,4.6,4.6,4.3,4.8,4.7,5.0,2.9,4.7,4.7,4.0,4.533723653395785,4.6,4.5,4.3,4.4,4.3,4.8,4.8,4.6,4.0,4.6,4.3,4.8,4.9,4.6,4.2,4.9,4.5,4.7,4.6,4.533723653395785,4.7,4.4,4.4,4.8,4.9,4.8,4.7,5.0,4.533723653395785,4.3,4.4,4.8,4.6,4.9,4.0,4.3,4.6,4.0,4.533723653395785,4.7,3.9,4.7,4.7,4.4,4.5,4.6,4.533723653395785,4.7,4.9,4.533723653395785,4.9,4.4,4.9,4.533723653395785,4.9,4.533723653395785,4.8,4.6,4.533723653395785,4.9,4.5,4.6,5.0,4.7,4.533723653395785,4.533723653395785,4.6,4.7,4.8,4.8,4.6,4.6,4.8,4.8,4.8,5.0,4.5,4.7,4.6,4.9,4.7,4.3,4.0,4.5,4.533723653395785,4.3,4.533723653395785,4.4,4.3,4.5,4.5,4.3,4.8,4.8,4.5,4.533723653395785,4.533723653395785,4.3,4.3,4.3,4.533723653395785,5.0,4.2,4.9,4.533723653395785,4.0,4.7,4.5,4.8,4.9,4.9,4.8,4.8,4.7,4.8,3.6,4.9,4.533723653395785,4.533723653395785,4.3,4.9,4.533723653395785,4.533723653395785,4.8,4.7,4.3,4.7,4.7,3.9,4.6,4.7,4.8,4.3,4.533723653395785,4.8,4.4,4.6,4.5,5.0,3.8,3.8,4.533723653395785,4.533723653395785,4.5,4.4,4.6,4.0,4.8,4.533723653395785,4.3,4.5,4.8,4.4,4.533723653395785,4.9,4.5,4.3,3.9,4.533723653395785,4.7,4.4,4.4,4.3,4.9,4.7,4.4,4.7,4.9,4.8,3.2,4.7,5.0,4.7,4.9,4.533723653395785,4.7,4.8,4.8,4.7,4.8,4.533723653395785,4.533723653395785,4.4,4.8,4.5,4.8,4.8,4.8,4.2,4.533723653395785,4.533723653395785,4.8,4.8,4.6,4.5,4.533723653395785,4.5,4.5,4.533723653395785,4.3,4.4,2.5,4.3,4.9,4.9,4.3,4.4,4.6,4.5,4.5,4.6,3.8,4.8,4.6,4.5,4.2,4.5,4.9,4.6,4.4,4.5,4.8,3.6,4.0,4.8,5.0,4.8,4.6,4.6,2.1,4.8,4.533723653395785,4.4,4.8,4.3,4.8,4.9,4.7,4.9,4.8,4.8,4.9,4.5,4.8,4.8,4.6,4.8,4.533723653395785,4.4,4.5,4.7,4.8,4.9,4.7,4.4,4.8,4.8,3.7,4.7,4.7,4.533723653395785,4.4,4.9,4.3,4.533723653395785,4.3,4.4,4.533723653395785,4.2,4.8,4.7,4.5,4.7,4.5,4.5,4.9,4.4,4.5,4.533723653395785,4.8,4.5,4.6,4.6,4.4,4.8,4.6,4.8,4.6,4.6,4.6,4.533723653395785,4.8,4.6,4.7,4.7,4.9,4.2,5.0,4.3,4.9,4.6,4.7,4.7,4.6,4.8,4.8,4.9,4.8,4.8,4.5,4.8,4.9,4.533723653395785,4.8,4.8,4.533723653395785,4.4,4.5,4.6,4.533723653395785,4.6,4.8,4.8,4.5,4.2,4.7,4.5,4.533723653395785,4.5,4.6,4.6,4.8,4.8,4.5,3.2,4.5,4.6,4.533723653395785,4.7,4.8,4.7,4.8,4.6,4.3,4.8,4.2,4.533723653395785,4.8,4.533723653395785,2.9,4.9,4.3,4.8,4.9,4.6,4.2,4.9,4.8,4.3,4.7,4.9,4.9,4.533723653395785,4.8,4.5,4.533723653395785,4.9,4.6,3.9,4.8,4.8,4.5,4.4,4.8,4.533723653395785,4.3,4.6,4.6,4.533723653395785,4.7,4.5,4.4,4.8,4.7,4.9,4.8,4.3,4.8,4.6,4.533723653395785,4.8,5.0,4.5,4.6,4.5,4.8,4.4,4.9,4.8,4.8,4.9,4.533723653395785,4.4,4.533723653395785,4.7,3.9,4.5,4.9,4.7,4.6,4.7,4.2,4.4,4.533723653395785,4.7,4.6,4.8,4.8,4.533723653395785,4.4,4.3,4.7,4.3,4.533723653395785,4.8,4.5,4.3,4.8,4.8,4.7,4.8,4.3,4.8,4.8,4.5,4.7,4.5,4.8,4.6,2.9,3.9,4.8,4.8,4.8,4.5,4.4,4.4,4.4,3.1,4.5,3.9,4.533723653395785,4.5,4.533723653395785,4.4,4.8,4.9,4.533723653395785,4.533723653395785,4.5,4.7,4.5,3.9,4.9,4.9,4.4,4.5,4.5,4.9,4.3,4.4,4.6,4.6,4.5,4.4,4.7,4.5,4.3,4.9,4.9,4.533723653395785,4.533723653395785,4.9,4.3,4.9,4.8,4.9,4.9,4.7,4.533723653395785,5.0,4.3,4.9,4.533723653395785,5.0,4.4,5.0,4.5,4.6,4.9,4.533723653395785,5.0,4.7,4.8,4.6,4.7,4.5,4.5,4.4,4.0,4.533723653395785,4.7,4.8,4.7,4.5,4.4,4.4,3.1,4.7,4.5,4.9,4.8,4.8,4.5,4.9,4.8,4.7,4.0,4.6,4.9,4.8,4.7,4.8,4.6,4.4,4.8,4.6,4.9,4.6,4.9,3.4,4.5,4.5,4.533723653395785,4.4,4.533723653395785,4.9,4.5,4.7,4.3,4.6,4.3,4.9,4.5,4.7,3.9,4.9,4.9,4.6,4.7,4.533723653395785,4.3,4.6,4.7,4.533723653395785,4.3,4.9,4.6,4.533723653395785,4.7,5.0,4.9,4.7,4.4,4.533723653395785,4.533723653395785,4.533723653395785,3.6,4.8,4.8,4.6,4.7,4.5,4.7,4.5,4.2,4.9,4.0,4.533723653395785,4.7,4.7,4.7,4.6,4.5,4.533723653395785,4.8,4.4,4.5,4.3,4.9,4.8,4.3,4.5,4.8,4.5,4.8,4.9,4.8,4.5,4.533723653395785,4.7,4.533723653395785,4.5,4.5,4.8,3.9,4.7,4.2,4.8,3.4,4.6,3.1,4.4,4.8,4.6,4.8,4.4,4.8,4.4,4.533723653395785,4.9,4.3,4.8,4.533723653395785,3.7,4.7,3.6,4.533723653395785,4.7,4.8,4.9,4.8,4.7,3.9,4.5,4.5,4.9,4.9,4.8,4.533723653395785,4.0,4.7,4.6,4.533723653395785,4.8,3.1,4.8,4.8,4.4,4.5,4.533723653395785,4.533723653395785,4.8,2.1,4.8,4.5,4.8,4.3,4.533723653395785,4.0,4.5,4.2,4.8,4.7,4.533723653395785,4.5,4.7,4.9,4.7,4.8,4.533723653395785,4.9,4.6,4.5,4.5,4.5,4.533723653395785,4.9,4.8,4.8,4.3,2.1,4.8,4.6,4.3,4.9,4.6,4.8,4.8,4.7,5.0,4.7,4.5,4.7,4.8,4.9,4.7,4.9,4.4,4.2,4.3,4.9,4.5,4.0,4.8,5.0,4.533723653395785,4.5,4.5,4.8,4.7,4.8,4.9,4.9,4.6,4.5,3.8,4.0,4.8,4.6,4.7,4.6,4.9,4.7,4.7,4.7,4.533723653395785,4.5,4.6,4.533723653395785,2.7,4.533723653395785,4.7,4.5,4.3,4.4,4.4,4.6,4.7,4.8,4.8,4.7,4.8,4.533723653395785,4.6,4.533723653395785,4.7,4.7,4.6,4.8,4.3,4.8,3.9,4.6,4.3,4.6,4.7,4.6,4.7,4.7,4.8,3.6,4.533723653395785,2.9,4.6,3.9,4.1,4.6,4.5,4.9,4.533723653395785,4.2,3.9,4.8,4.7,4.9,4.9,4.9,4.9,4.9,4.7,4.6,4.533723653395785,4.533723653395785,4.533723653395785,4.6,4.3,4.533723653395785,3.6,4.7,4.7,3.7,4.533723653395785,4.4,4.7,4.7,4.7,4.533723653395785,4.5,4.6,4.4,4.3,4.6,4.0,4.9,4.8,4.6,4.5,4.8,4.8,4.5,4.3,4.3,4.6,4.8,4.533723653395785,3.4,4.7,4.6,4.6,4.9,4.3,4.533723653395785,4.8,4.2,4.8,4.3,4.533723653395785,4.4,4.7,4.8,4.3,4.8,4.4,4.8,4.4,4.8,4.5,4.6,4.8,4.9,4.3,4.7,4.7,4.8,3.7,4.6,4.4,4.8,4.2,4.8,4.3,4.533723653395785,4.6,4.4,4.5,4.8,4.4,4.4,4.8,4.7,4.5,4.533723653395785,4.7,4.3,4.8,4.533723653395785,4.9,3.4,4.9,4.5,4.7,4.7,4.9,4.5,4.4,4.5,4.3,4.8,4.4,4.8,4.533723653395785,4.6,4.6,4.2,5.0,4.0,4.3,4.8,4.3,4.8,4.8,4.8,4.5,4.4,3.8,4.8,4.3,4.0,3.1,4.6,4.533723653395785,4.6,4.6,4.8,4.3,4.8,5.0,4.7,4.8,4.7,4.533723653395785,4.3,4.0,4.7,4.6,4.9,4.6,4.9,4.533723653395785,3.6,2.9,4.533723653395785,4.5,4.5,4.6,4.4,4.3,4.6,4.4,4.8,4.5,4.4,4.9,4.7,4.4,5.0,4.6,4.8,4.533723653395785,4.6,4.8,4.3,3.9,4.6,4.7,4.3,4.533723653395785,4.7,4.4,4.4,4.9,4.4,4.4,4.533723653395785,4.6,5.0,4.6,4.8,4.8,4.4,4.3,4.533723653395785,4.3,4.533723653395785,4.9,4.6,4.5,4.533723653395785,3.1,4.2,3.6,3.9,4.8,4.5,4.533723653395785,4.9,4.4,4.7,4.8,4.7,4.5,4.7,4.533723653395785,4.9,3.6,4.8,4.8,4.6,4.9,4.8,4.3,4.533723653395785,4.6,3.9,4.9,3.9,4.7,5.0,4.9,4.5,4.8,4.8,4.9,4.533723653395785,4.533723653395785,4.9,4.8,4.8,4.4,4.7,4.9,4.5,4.8,4.533723653395785,4.6,4.6,4.8,4.9,4.6,4.6,4.7,4.4,4.6,4.9,4.4,4.533723653395785,4.2,4.8,4.533723653395785,4.8,4.6,4.2,4.3,4.9,4.7,4.6,4.8,4.9,4.0,4.533723653395785,4.533723653395785,4.533723653395785,2.9,2.7,2.9,4.533723653395785,4.533723653395785,4.533723653395785,4.8,4.3,3.9,4.6,4.0,4.4,4.8,3.9,3.4,4.9,4.3,4.8,4.7,4.5,4.0,5.0,4.4,4.533723653395785,3.8,4.6,4.7,4.9,4.5,4.5,3.4,4.6,4.8,4.5,4.8,3.9,4.4,4.6,4.7,4.3,4.6,4.5,4.4,4.2,4.7,4.6,4.533723653395785,4.7,4.533723653395785,4.5,4.7,4.9,4.7,4.3,4.3,4.9,4.5,4.5,4.4,4.7,4.7,4.8,4.9,4.4,4.6,4.9,4.8,4.6,4.5,4.7,4.8,4.9,3.8,4.9,4.8,4.2,4.8,4.8,4.7,4.6,4.5,4.3,4.9,4.6,4.3,4.9,4.7,3.8,4.7,3.9,4.9,4.5,4.2,4.9,4.5,4.533723653395785,4.7,4.533723653395785,4.5,4.9,4.6,4.7,4.5,4.0,4.9,4.6,4.7,4.6,4.9,4.0,4.8,4.9,4.8,4.5,4.6,4.533723653395785,4.5,4.8,4.6,4.533723653395785,4.0,3.5,4.7,4.4,4.7,5.0,4.1,4.8,5.0,4.8,4.3,4.1,3.6,4.7,4.2,4.1,4.8,4.7,4.1,4.3,4.9,4.7,4.7,4.5,4.8,4.7,4.4,4.8,4.6,4.7,4.4,4.7,4.533723653395785,4.5,4.9,4.8,4.5],"xaxis":"x","y":[30900.0,139999.0,132999.0,58587.0,95990.0,74999.0,130999.0,55995.0,71999.0,62975.0,82895.0,50226.0,22495.0,63998.0,65024.0,69771.0,51995.0,57877.0,53977.0,22498.0,28784.0,67800.0,42595.0,74788.0,58770.0,53690.0,46900.0,38996.0,69306.0,39442.0,112749.0,57900.0,49995.0,153722.0,43855.0,50563.0,74887.0,59413.0,68983.0,46300.0,62421.0,198000.0,36900.0,62292.0,45991.0,50299.0,98984.0,65992.0,77700.0,65514.0,54660.0,143554.0,67800.0,54000.0,57463.0,84997.0,74377.0,57900.0,49913.0,21082.0,161775.0,79711.0,48500.0,50900.0,39950.0,54018.0,38963.0,107552.0,50595.0,29999.0,47562.0,50888.0,95981.0,98555.0,57445.0,94995.0,43105.0,55998.0,41000.0,39237.0,58798.0,30880.0,26499.0,33497.0,177595.0,69998.0,120997.0,142000.0,48000.0,73998.0,65444.0,69997.0,157500.0,45705.0,114750.0,41948.0,99805.0,39950.0,67910.0,60312.0,58997.0,35824.0,55995.0,70991.0,86690.0,50900.0,49400.0,60994.0,53922.0,67585.0,56940.0,62704.0,209995.0,55000.0,41690.0,44744.0,65999.0,49983.0,87390.0,111326.0,68500.0,67515.0,33999.0,31293.0,69997.0,99999.0,78900.0,36880.0,42499.0,20999.0,74988.0,28000.0,26540.0,94984.0,86975.0,77000.0,53900.0,112997.0,46888.0,97397.0,49777.0,41200.0,43777.0,68398.0,73990.0,49500.0,48846.0,45900.0,93888.0,47222.0,33951.0,59900.0,37650.0,59700.0,27412.0,57453.0,44196.0,45551.0,39864.0,41929.0,44777.0,30400.0,31336.0,107999.0,89999.0,98444.0,99795.0,105999.0,43900.0,47222.0,78899.0,44222.0,112498.0,46888.0,67983.0,55089.0,68274.0,76777.0,46777.0,199865.0,178899.0,43888.0,92499.0,48998.0,61991.0,33797.0,125997.0,149995.0,30880.0,46900.0,93882.0,53445.0,50754.0,108337.0,38740.0,38997.0,47721.0,34600.0,29397.0,173880.0,48995.0,50888.0,164882.0,157771.0,58702.0,35595.0,60146.0,79987.0,46888.0,60977.0,36295.0,85987.0,158880.0,65987.0,30502.0,35887.0,49995.0,35000.0,67500.0,45955.0,79894.0,64911.0,54750.0,108946.0,58998.0,139994.0,52888.0,41069.0,79998.0,101998.0,119899.0,78900.0,87902.0,39955.0,64992.0,77900.0,135990.0,20657.0,37870.0,69795.0,84690.0,61992.0,73499.0,47399.0,130994.0,46519.0,29598.0,119510.0,35878.0,46999.0,50477.0,142106.0,36295.0,52988.0,24862.0,54495.0,86881.0,46999.0,33316.0,32390.0,53436.0,43497.0,98995.0,28975.0,45999.0,79876.0,81921.0,85175.0,71998.0,48998.0,80991.0,39843.0,77802.0,57900.0,74995.0,52000.0,45712.0,106777.0,57642.0,49999.0,44777.0,49881.0,126995.0,30955.0,28990.0,104900.0,64443.0,34988.0,79997.0,58997.0,41994.0,127500.0,27620.0,44990.0,164900.0,177595.0,223414.0,72555.0,32998.0,73890.0,36991.0,77033.0,85500.0,89888.0,28988.0,51897.0,55900.0,89995.0,61805.0,98999.0,164994.0,49995.0,82980.0,81994.0,125690.0,50563.0,50905.0,98509.0,75850.0,49445.0,63987.0,45375.0,54777.0,61997.0,27477.0,47995.0,110950.0,30323.0,45950.0,41850.0,39999.0,144870.0,80594.0,109554.0,66413.0,99997.0,65595.0,99998.0,103044.0,50994.0,75998.0,123881.0,50194.0,45142.0,62000.0,63777.0,57499.0,103500.0,18578.0,96713.0,49988.0,35999.0,73800.0,89597.0,72490.0,42640.0,48932.0,65595.0,159990.0,56998.0,60495.0,34551.0,48350.0,41999.0,43399.0,86925.0,52744.0,89900.0,57007.0,28484.0,24999.0,49988.0,128887.0,61998.0,46995.0,43495.0,47647.0,46777.0,30400.0,85292.0,69287.0,50997.0,78999.0,72293.0,142825.0,53988.0,86249.0,56004.0,45982.0,29598.0,37995.0,28954.0,44999.0,80994.0,72865.0,36295.0,35897.0,44210.0,40000.0,221994.0,53895.0,86995.0,57988.0,41280.0,49995.0,40999.0,38200.0,23949.0,98140.0,27995.0,56997.0,47875.0,99777.0,39497.0,37490.0,76589.0,51500.0,67466.0,64876.0,33397.0,47893.0,87987.0,44898.0,36131.0,36991.0,64815.0,51490.0,47999.0,32300.0,85000.0,54987.0,58375.0,46099.0,78000.0,39999.0,150000.0,62988.0,63550.0,30904.0,46912.0,54998.0,157771.0,44997.0,38482.0,164900.0,74994.0,54981.0,169661.0,62988.0,73495.0,39100.0,38689.0,76286.0,64955.0,50988.0,203999.0,38288.0,179120.0,56994.0,64995.0,24688.0,65507.0,47882.0,80995.0,58883.0,45998.0,36125.0,42997.0,49495.0,49995.0,219995.0,44150.0,129995.0,46288.0,69998.0,33498.0,89990.0,58991.0,124988.0,39982.0,22913.0,109997.0,53983.0,98444.0,52495.0,55699.0,80654.0,55990.0,38492.0,44777.0,74999.0,40493.0,129618.0,53791.0,32339.0,63208.0,21900.0,59410.0,24991.0,20437.0,46980.0,41584.0,40970.0,30791.0,45994.0,26840.0,50991.0,27500.0,38497.0,43900.0,47777.0,49988.0,37539.0,44995.0,49850.0,43888.0,43860.0,61000.0,40016.0,52188.0,55997.0,141215.0,32000.0,34495.0,69992.0,109554.0,8999.0,44299.0,46417.0,44222.0,28995.0,28999.0,54981.0,26667.0,76987.0,40699.0,49884.0,45553.0,81000.0,43888.0,30966.0,48220.0,74442.0,70500.0,37037.0,69612.0,109554.0,42900.0,32298.0,29211.0,30500.0,29991.0,26885.0,69990.0,58003.0,77306.0,48982.0,44933.0,41497.0,40000.0,48950.0,41574.0,29000.0,21844.0,44185.0,134495.0,30985.0,58003.0,68990.0,27495.0,69892.0,32984.0,62777.0,42991.0,56111.0,61285.0,39987.0,41988.0,53777.0,42981.0,157991.0,74519.0,45994.0,72777.0,38995.0,58990.0,41990.0,85777.0,58883.0,118988.0,55994.378500823725,53900.0,43299.0,44421.0,51990.0,26995.0,45485.0,65595.0,40888.0,39995.0,37872.0,29495.0,41990.0,53275.0,104357.0,48888.0,80239.0,32795.0,38866.0,45553.0,48487.0,44699.0,69459.0,39577.0,49777.0,52627.0,61880.0,52500.0,36998.0,57981.0,50515.0,31695.0,85991.0,38000.0,77586.0,56900.0,30900.0,30877.0,66888.0,48000.0,57995.0,42989.0,55123.0,37893.0,69500.0,45500.0,44500.0,55989.0,57495.0,31777.0,40888.0,45900.0,36900.0,49997.0,44998.0,90000.0,49988.0,81494.0,41988.0,56001.0,45900.0,77898.0,37983.0,65466.0,97888.0,47950.0,38746.0,19498.0,72992.0,55650.0,52990.0,99495.0,39495.0,38900.0,41874.0,64777.0,39975.0,68910.0,50777.0,22992.0,58441.0,84614.0,44810.0,45988.0,43900.0,37978.0,43991.0,62706.0,29933.0,164900.0,32000.0,55984.0,77400.0,55777.0,159565.0,52751.0,42000.0,37994.0,33967.0,43900.0,46789.0,55184.0,53899.0,49990.0,36900.0,30752.0,95879.0,33695.0,49777.0,45968.0,53489.0,45900.0,55651.0,54495.0,55803.0,32989.0,29816.0,41998.0,58199.0,40995.0,90988.0,96838.0,41488.0,62981.0,30999.0,90000.0,42777.0,44162.0,46401.0,55117.0,59000.0,42300.0,28481.0,101888.0,43895.0,46900.0,46411.0,36633.0,76900.0,56777.0,219805.0,32337.0,42989.0,66888.0,55101.0,110084.0,30653.0,47495.0,30775.0,46999.0,109990.0,44788.0,66205.0,28395.0,16998.0,40670.0,27904.0,44963.0,55904.0,50988.0,54900.0,67800.0,47888.0,55936.0,221994.0,30423.0,43981.0,42495.0,69999.0,44997.0,229654.0,42787.0,46987.0,39490.0,33600.0,47605.0,28725.0,44990.0,28995.0,33395.0,40952.0,48988.0,43991.0,25999.0,84800.0,52888.0,41997.0,56156.0,46685.0,50900.0,46206.0,69989.0,154200.0,38954.0,67399.0,34984.0,50257.0,47346.0,56900.0,55444.0,73870.0,78900.0,42888.0,57980.0,58883.0,38350.0,42900.0,35918.0,65712.0,81891.0,43888.0,73777.0,49995.0,78933.0,45305.0,41500.0,48474.0,31991.0,24400.0,54900.0,78499.0,129999.0,37494.0,53658.0,55800.0,34537.0,57441.0,45988.0,43498.0,112999.0,44000.0,52000.0,25894.0,26995.0,47203.0,83982.0,42681.0,34726.0,49491.0,46444.0,45677.0,49000.0,26497.0,38599.0,42574.0,58999.0,27500.0,47305.0,40999.0,47882.0,30998.0,47308.0,48306.0,69800.0,32140.0,57899.0,35922.0,47509.0,30991.0,71888.0,45998.0,34377.0,64502.0,125880.0,68500.0,54811.0,44499.0,52660.0,50977.0,28795.0,39154.0,139250.0,39491.0,36988.0,83824.0,42991.0,48994.0,42487.0,33877.0,28475.0,41940.0,47893.0,49700.0,55835.0,38000.0,29625.0,33937.0,38444.0,62756.0,29250.0,32997.0,67890.0,51000.0,53484.0,35598.0,53148.0,68998.0,94777.0,72390.0,30583.0,45994.0,73345.0,38552.0,90000.0,154900.0,44990.0,54317.0,45888.0,39521.0,76992.0,90988.0,92896.0,57900.0,37823.0,77465.0,83777.0,39894.0,39500.0,67989.0,43992.0,36885.0,42300.0,37899.0,45645.0,77482.0,46991.0,38098.0,39659.0,44851.0,32589.0,52075.0,28995.0,46328.0,39494.0,45935.0,52713.0,34445.0,60833.0,44900.0,40007.0,68113.0,46688.0,63910.0,139977.0,39995.0,81593.0,50881.0,58796.0,36000.0,35595.0,41733.0,159979.0,45281.0,35650.0,45900.0,43988.0,63000.0,37995.0,70490.0,39900.0,42735.0,59546.0,44000.0,39592.0,54999.0,30934.0,38995.0,50284.0,40775.0,71989.0,47502.0,53510.0,37974.0,54480.0,40998.0,37500.0,47791.0,33295.0,50639.0,112998.0,55991.0,49888.0,53910.0,43881.0,39988.0,28000.0,29052.0,75890.0,63491.0,27995.0,40900.0,27250.0,40495.0,31416.0,35475.0,59750.0,38995.0,41999.0,32230.0,40990.0,42995.0,45000.0,96510.0,23991.0,38999.0,33738.0,38594.0,50555.0,41981.0,51788.0,26199.0,28888.0,31593.0,88977.0,60900.0,43498.0,129991.0,31652.0,31756.0,46998.0,169995.0,47100.0,27990.0,52983.0,33750.0,32998.0,87851.0,42995.0,33991.0,30998.0,39634.0,52892.0,32485.0,35988.0,72666.0,58998.0,54999.0,54994.0,38314.0,34650.0,224891.0,41999.0,37500.0,29996.0,43345.0,29500.0,31933.0,36988.0,38989.0,36390.0,70164.0,52777.0,36852.0,71994.0,43888.0,36900.0,55900.0,49444.0,55777.0,61999.0,58880.0,51469.0,54329.0,23199.0,34500.0,89500.0,144999.0,46690.0,43992.0,71892.0,40684.0,41888.0,40102.0,46315.0,27825.0,43988.0,42700.0,30979.0,34238.0,58988.0,39400.0,47644.0,42344.0,40600.0,41594.0,44888.0,51933.0,40997.0,53686.0,30840.0,50997.0,43000.0,209894.0,60566.0,55998.0,34991.0,50792.0,57774.0,60470.0,47990.0,51555.0,29990.0,37890.0,216999.0,50991.0,53000.0,39997.0,42892.0,31152.0,44990.0,45925.0,32685.0,61997.0,31562.0,41200.0,48400.0,46977.0,50788.0,56739.0,71892.0,77945.0,53965.0,28013.0,26433.0,47044.0,218937.0,62900.0,67897.0,40000.0,41964.0,45693.0,42963.0,60000.0,39485.0,49779.0,34900.0,85999.0,34522.0,47952.0,189888.0,43905.0,48006.0,46995.0,40989.0,48999.0,44941.0,51971.0,76765.0,29526.0,28999.0,35988.0,44932.0,55900.0,39480.0,41999.0,44989.0,57505.0,75999.0,46999.0,47804.0,95997.0,54395.0,40894.0,51995.0,61998.0,53997.0,53717.0,72007.0,39964.0,41495.0,49991.0,47900.0,37496.0,34833.0,137900.0,29275.0,36839.0,219894.0,34977.0,39400.0,69867.0,29960.0,39988.0,49998.0,41498.0,31998.0,34746.0,42994.0,35888.0,32928.0,72550.0,24990.0,50999.0,30735.0,70226.0,51507.0,100804.0,53698.0,45244.0,27695.0,86989.0,46939.0,65991.0,46500.0,64986.0,50481.0,42495.0,30960.0,129351.0,59991.0,41900.0,47983.0,40983.0,46220.0,30995.0,50999.0,55110.0,42200.0,47888.0,43977.0,53974.0,165994.0,34690.0,43998.0,38000.0,21958.0,85777.0,60125.0,84994.0,38795.0,31495.0,77991.0,43481.0,50384.0,36888.0,41600.0,33896.0,103995.0,51999.0,26565.0,30987.0,55990.0,41990.0,32395.0,46446.0,52835.0,52835.0,30972.0,38899.0,67035.0,45844.0,58884.0,79988.0,34998.0,29892.0,49475.0,165997.0,26900.0,41000.0,31988.0,41300.0,48777.0,30995.0,57425.0,48000.0,128388.0,54776.0,48991.0,45994.0,109995.0,30756.0,49777.0,39989.0,52496.0,47300.0,33233.0,36985.0,56991.0,48000.0,44900.0,43777.0,79991.0,35986.0,41900.0,60200.0,66876.0,69777.0,41000.0,38000.0,35900.0,45777.0,55999.0,92857.0,67997.0,77995.0,30888.0,40563.0,61994.0,97892.0,54493.0,42000.0,47988.0,41990.0,46000.0,56997.0,53555.0,46908.0,33795.0,54954.0,73890.0,56444.0,57990.0,51555.0,169999.0,30999.0,44480.0,74999.0,53840.0,61777.0,94001.0,55212.0,39874.0,43440.0,78820.0,119995.0,71988.0,51696.0,52473.0,29527.0,69991.0,44998.0,68204.0,30994.0,38999.0,34410.0,38893.0,43555.0,85883.0,41000.0,29990.0,94983.0,69846.0,45777.0,43987.0,31987.0,77000.0,34989.0,36400.0,32817.0,58000.0,32995.0,52399.0,39988.0,33950.0,55521.0,36742.0,46295.0,51860.0,28496.0,49999.0,42988.0,55414.0,82994.0,41884.0,45998.0,29695.0,41432.0,28999.0,52466.0,47735.0,42986.0,67994.0,41973.0,43988.0,30500.0,47695.0,79799.0,33833.0,31773.0,29883.0,40900.0,63874.0,56481.0,55963.0,41985.0,29997.0,55900.0,38597.0,72765.0,24899.0,53441.0,29999.0,55995.0,38692.0,46398.0,21360.0,79700.0,52894.0,27981.0,33890.0,41990.0,43777.0,25632.0,48000.0,55349.0,41492.0,37616.0,48777.0,31925.0,38494.0,29890.0,43777.0,43983.0,51788.0,41883.0,32999.0,30933.0,45998.0,38995.0,35991.0,36884.0,98482.0,46692.0,24998.0,39800.0,128991.0,39988.0,27598.0,53332.0,51901.0,46098.0,83890.0,68853.0,81004.0,75007.0,34354.0,79991.0,51991.0,198888.0,55888.0,37295.0,43883.0,42588.0,64991.0,33989.0,48988.0,45000.0,43573.0,56894.0,65030.0,49983.0,38991.0,49145.0,46235.0,59990.0,31994.0,32309.0,32000.0,79990.0,42496.0,56775.0,45996.0,43600.0,45997.0,30988.0,45992.0,187987.0,176988.0,29893.0,34683.0,50000.0,31890.0,44800.0,29888.0,36789.0,34995.0,43990.0,42763.0,42775.0,114895.0,25983.0,182975.0,50000.0,44157.0,37700.0,42793.0,26880.0,39562.0,37259.0,36990.0,47998.0,35250.0,34597.0,48888.0,40039.0,41777.0,38761.0,30989.0,53983.0,47840.0,39699.0,48250.0,45699.0,52777.0,55004.0,42916.0,39495.0,32900.0,170882.0,30295.0,45768.0,41990.0,55498.0,98900.0,70990.0,53476.0,30337.0,32204.0,45990.0,72488.0,29894.0,52997.0,50435.0,69963.0,41991.0,28989.0,97999.0,29128.0,48700.0,53637.0,109977.0,43892.0,36866.0,39894.0,31894.0,63486.0,199860.0,77900.0,62988.0,49983.0,54995.0,39997.0,42880.0,40996.0,41100.0,44998.0,21260.0,35500.0,38494.0,36999.0,95984.0,87993.0,47845.0,39981.0,159990.0,60777.0,75445.0,67960.0,73980.0,35678.0,44988.0,28886.0,169999.0,47261.0,44995.0,29999.0,39948.0,36221.0,43995.0,84950.0,45000.0,29946.0,53484.0,69888.0,51989.0,46307.0,47983.0,35233.0,46000.0,42595.0,24586.0,84444.0,165966.0,55997.0,63899.0,47800.0,26999.0,26140.0,25994.0,37900.0,51200.0,41000.0,47983.0,27998.0,41990.0,87770.0,51499.0,40900.0,49983.0,67984.0,40501.0,64484.0,91433.0,29886.0,19990.0,47131.0,27750.0,37417.0,58594.0,61988.0,36851.0,91888.0,48500.0,133448.0,48787.0,42999.0,27583.0,54840.0,41998.0,45994.0,66990.0,75450.0,47851.0,56555.0,99900.0,32221.0,132228.0,48000.0,37393.0,75008.0,38991.0,36991.0,40122.0,66690.0,39900.0,64991.0,64900.0,27749.0,46800.0,49000.0,29989.0,53988.0,43545.0,44562.0,64000.0,63933.0,44000.0,39894.0,47618.0,81285.0,79900.0,15888.0,41403.0,212894.0,31881.0,49885.0,81894.0,30698.0,31986.0,56999.0,73362.0,47983.0,37882.0,36991.0,44977.0,118500.0,92676.0,27497.0,42000.0,50490.0,170388.0,129900.0,53900.0,47675.0,37894.0,37661.0,34984.0,192997.0,43777.0,42900.0,41990.0,39877.0,41939.0,39984.0,31000.0,42974.0,43465.0,38986.0,32998.0,45991.0,38595.0,32900.0,29999.0,71356.0,56633.0,47307.0,61988.0,32995.0,39459.0,38741.0,31997.0,30990.0,71995.0,41874.0,119255.0,43781.0,66164.0,77933.0,49900.0,29862.0,54800.0,31654.0,50500.0,26615.0,93100.0,91358.0,29999.0,53999.0,27990.0,70497.0,51997.0,30998.0,37844.0,47990.0,40441.0,44998.0,44995.0,62500.0,206885.0,25900.0,35998.0,50997.0,46997.0,73777.0,30978.0,56777.0,36595.0,43712.0,47575.0,69111.0,43803.0,55988.0,112999.0,25998.0,40877.0,29984.0,54995.0,117999.0,25991.0,68523.0,31383.0,70994.0,35988.0,55265.0,47988.0,30439.0,26650.0,29853.0,39900.0,39600.0,91999.0,39983.0,29990.0,39891.0,41600.0,51990.0,218554.0,32294.0,30991.0,79900.0,62988.0,62988.0,33495.0,41888.0,43820.0,217994.0,29795.0,54999.0,46809.0,39870.0,46999.0,37995.0,36395.0,41990.0,78500.0,49722.0,35993.0,31789.0,32855.0,29933.0,65555.0,47870.0,108670.0,58900.0,33639.0,71360.0,39890.0,39500.0,57835.0,51999.0,34994.0,29995.0,44611.0,44004.0,40364.0,30304.0,55992.0,24888.0,52900.0,43892.0,48999.0,52995.0,31900.0,32500.0,48794.0,41855.0,92925.0,56499.0,25594.0,33977.0,47755.0,28997.0,54777.0,162900.0,139891.0,45988.0,25998.0,45688.0,62545.0,50393.0,45493.0,45450.0,28892.0,60950.0,44029.0,30998.0,29997.0,43590.0,47750.0,25790.0,41500.0,45991.0,36988.0,51933.0,44900.0,49795.0,63989.0,40984.0,41994.0,27496.0,28999.0,53896.0,73993.0,43995.0,63995.0,66983.0,49990.0,36980.0,42990.0,51588.0,29994.0,43592.0,64500.0,45500.0,44900.0,96991.0,71500.0,46777.0,49983.0,29690.0,41880.0,48000.0,104995.0,46822.0,73922.0,63998.0,26690.0,73887.0,35494.0,81225.0,80893.0,29991.0,41221.0,24999.0,45997.0,52983.0,39699.0,38961.0,62988.0,31500.0,37350.0,64493.0,32190.0,74893.0,59933.0,40495.0,82444.0,44995.0,33865.0,37988.0,51989.0,61693.0,114500.0,43848.0,64498.0,41492.0,49988.0,26999.0,39974.0,45798.0,35900.0,30489.0,23980.0,44555.0,94898.0,39885.0,49500.0,45440.0,39493.0,65999.0,44990.0,34958.0,31563.0,163981.0,32882.0,31000.0,47000.0,24991.0,45777.0,63995.0,33756.0,45624.0,56998.0,41900.0,63798.0,116963.0,33392.0,34111.0,38368.0,43890.0,59900.0,35696.0,43400.0,36882.0,35998.0,48488.0,66991.0,198888.0,43997.0,48885.0,60888.0,53225.0,39999.0,31756.0,45898.0,130000.0,50426.0,33493.0,33989.0,39562.0,65973.0,90972.0,58999.0,52777.0,58903.0,35248.0,57555.0,44876.0,63500.0,41400.0,55490.0,20991.0,53777.0,20647.0,32928.0,32282.0,90971.0,30840.0,38880.0,47894.0,31195.0,39997.0,69986.0,56895.0,46273.0,71995.0,55777.0,37442.0,35987.0,41555.0,60880.0,42900.0,45990.0,27890.0,30993.0,70977.0,26979.0,38562.0,40650.0,38300.0,67991.0,41986.0,44000.0,53771.0,32900.0,37989.0,49998.0,43498.0,31488.0,43800.0,74320.0,47050.0,32892.0,27998.0,42495.0,53505.0,49184.0,67393.0,54596.0,61577.0,36991.0,76990.0,39661.0,40490.0,41451.0,37997.0,44480.0,76994.0,29795.0,31750.0,178000.0,44580.0,32495.0,41780.0,38523.0,52777.0,41507.0,39988.0,37990.0,56954.0,166230.0,50992.0,36900.0,69801.0,45495.0,17998.0,29946.0,31933.0,30995.0,47598.0,20754.0,32596.0,66500.0,99998.0,41444.0,42991.0,72563.0,49990.0,52996.0,30500.0,45651.0,32962.0,25894.0,33989.0,28998.0,39692.0,30986.0,40981.0,43777.0,99884.0,42250.0,54894.0,66777.0,45393.0,117981.0,97973.0,33988.0,64777.0,40594.0,52990.0,28818.0,32818.0,35500.0,31500.0,32000.0,29999.0,37988.0,149495.0,26995.0,72999.0,27999.0,36958.0,72990.0,33988.0,33390.0,36393.0,25736.0,40490.0,30880.0,41000.0,34656.0,61991.0,45990.0,48043.0,51045.0,39672.0,47500.0,31998.0,47840.0,53991.0,34409.0,135640.0,53342.0,46998.0,57315.0,127598.0,78585.0,72895.0,59000.0,89984.0,45950.0,32725.0,39991.0,51505.0,38798.0,49695.0,228323.0,43149.0,54998.0,67750.0,65893.0,49888.0,55998.0,38779.0,147900.0,30383.0,55791.0,31145.0,43995.0,41800.0,31990.0,42999.0,48253.0,43884.0,52887.0,32988.0,60575.0,99999.0,75982.0,40777.0,112999.0,36854.0,115991.0,48125.0,40987.0,26988.0,33534.0,27933.0,39490.0,45888.0,30000.0,82900.0,53555.0,40499.0,68777.0,104992.0,56705.0,33960.0,30595.0,32933.0,61844.0,36784.0,34900.0,44000.0,31707.0,42998.0,31750.0,84000.0,53299.0,40777.0,29933.0,33555.0,52905.0,34888.0,55901.0,44000.0,49480.0,40385.0,27996.0,73478.0,33498.0,38998.0,37918.0,32544.0,41863.0,36990.0,46545.0,39997.0,40982.0,37916.0,61291.0,34444.0,150299.0,52989.0,38000.0,63750.0,45822.0,25200.0,50997.0,49989.0,34991.0,46900.0,29584.0,34895.0,19677.0,163998.0,38794.0,31998.0,88933.0,45977.0,30490.0,52995.0,38780.0,47995.0,65991.0,94000.0,65480.0,31379.0,46500.0,33933.0,34890.0,71998.0,116991.0,43900.0,60000.0,46900.0,89585.0,173998.0,40525.0,29950.0,60805.0,37999.0,39390.0,35780.0,39862.0,54991.0,29894.0,43993.0,72999.0,30834.0,31998.0,209991.0,43893.0,44965.0,59993.0,49741.0,52900.0,48499.0,48890.0,48890.0,29524.0,32998.0,37840.0,40000.0,47588.0,33870.0,37989.0,36992.0,59995.0,51994.0,47777.0,48989.0,49987.0,72661.0,37813.0,79990.0,34988.0,49900.0,72374.0,28860.0,29998.0,44834.0,70065.0,97000.0,75000.0,39965.0,56299.0,75900.0,41996.0,54500.0,49680.0,25499.0,57140.0,45484.0,61930.0,51565.0,73994.0,33999.0,30799.0,33892.0,55000.0,41999.0,56795.0,29995.0,39879.0,172500.0,64107.0,40000.0,38865.0,51900.0,27685.0,35933.0,72991.0,36994.0,57499.0,33990.0,54888.0,67755.0,65936.0,32633.0,48777.0,24997.0,34250.0,77506.0,50777.0,77850.0,34449.0,97892.0,43777.0,61577.0,67989.0,46225.0,48558.0,28500.0,50400.0,44987.0,60597.0,46900.0,49988.0,53900.0,51998.0,48777.0,68999.0,75994.0,54900.0,34887.0,30945.0,39597.0,44984.0,50778.0,45777.0,32544.0,29893.0,68290.0,31950.0,54998.0,85894.0,29889.0,41291.0,41291.0,28550.0,49581.0,91988.0,25976.0,31000.0,27999.0,29599.0,30497.0,36750.0,39991.0,73994.0,45990.0,41558.0,193901.0,89989.0,54498.0,47781.0,31995.0,30740.0,33850.0,88799.0,40558.0,31489.0,20995.0,30799.0,56995.0,39900.0,56999.0,45777.0,32000.0,46900.0,27500.0,36792.0,39998.0,36382.0,48157.0,28351.0,40500.0,46777.0,69880.0,33950.0,152986.0,31195.0,23168.0,40999.0,229918.0,72545.0,97575.0,64215.0,63994.0,87599.0,39200.0,57499.0,63995.0,56695.0,38760.0,94989.0,74989.0,44933.0,29500.0,34995.0,57153.0,51763.0,30900.0,20562.0,57555.0,89998.0,45877.0,117495.0,31992.0,35416.0,85999.0,65886.0,75577.0,53490.0,89599.0,32890.0,53983.0,46035.0],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Rating"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Price"}},"legend":{"tracegroupgap":0},"title":{"text":"Scatter plot of Rating vs Price"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('5e6ee982-c4a7-4332-b18d-d8b8002523c9');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div><div class="output text_html"><html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="0143ce7a-1c63-46e9-9338-8b0167567635" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("0143ce7a-1c63-46e9-9338-8b0167567635")) {                    Plotly.newPlot(                        "0143ce7a-1c63-46e9-9338-8b0167567635",                        [{"hovertemplate":"Review Count=%{x}\u003cbr\u003ePrice=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[419.00943396226415,419.00943396226415,419.00943396226415,752.0,419.00943396226415,419.00943396226415,419.00943396226415,98.0,902.0,344.0,419.00943396226415,585.0,377.0,899.0,352.0,419.00943396226415,377.0,991.0,419.00943396226415,419.00943396226415,150.0,419.00943396226415,146.0,526.0,352.0,419.00943396226415,419.00943396226415,419.00943396226415,192.0,716.0,483.0,419.00943396226415,94.0,662.0,352.0,37.0,419.00943396226415,662.0,419.00943396226415,821.0,344.0,33.0,419.00943396226415,344.0,50.0,788.0,419.00943396226415,15.0,244.0,15.0,150.0,991.0,419.00943396226415,419.00943396226415,419.00943396226415,419.0,526.0,419.00943396226415,192.0,894.0,991.0,419.00943396226415,419.00943396226415,419.00943396226415,1.0,483.0,344.0,419.00943396226415,419.00943396226415,1.0,112.0,15.0,419.00943396226415,419.00943396226415,526.0,419.00943396226415,352.0,819.0,12.0,150.0,28.0,79.0,0.0,98.0,419.00943396226415,819.0,419.0,1.0,419.00943396226415,419.00943396226415,15.0,899.0,419.00943396226415,419.00943396226415,419.00943396226415,902.0,192.0,1.0,192.0,716.0,419.0,902.0,377.0,543.0,419.00943396226415,419.00943396226415,15.0,468.0,419.00943396226415,352.0,716.0,417.0,274.0,443.0,419.00943396226415,419.00943396226415,981.0,899.0,419.00943396226415,585.0,419.00943396226415,15.0,696.0,419.00943396226415,899.0,419.00943396226415,419.00943396226415,266.0,110.0,419.00943396226415,752.0,1.0,316.0,419.00943396226415,419.00943396226415,527.0,419.00943396226415,419.0,696.0,297.0,419.00943396226415,1.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,696.0,862.0,344.0,29.0,15.0,902.0,662.0,894.0,419.00943396226415,419.00943396226415,15.0,419.00943396226415,37.0,150.0,419.00943396226415,419.00943396226415,419.00943396226415,969.0,419.00943396226415,419.00943396226415,696.0,110.0,696.0,843.0,696.0,419.00943396226415,696.0,50.0,419.00943396226415,419.00943396226415,557.0,557.0,696.0,788.0,112.0,862.0,297.0,419.0,821.0,113.0,419.00943396226415,419.00943396226415,526.0,419.00943396226415,419.00943396226415,526.0,98.0,894.0,526.0,297.0,113.0,244.0,696.0,991.0,419.00943396226415,911.0,522.0,419.00943396226415,419.00943396226415,696.0,419.00943396226415,419.00943396226415,752.0,113.0,65.0,184.0,894.0,526.0,527.0,419.00943396226415,318.0,419.00943396226415,501.0,419.00943396226415,558.0,819.0,419.00943396226415,696.0,110.0,122.0,419.00943396226415,848.0,419.00943396226415,192.0,911.0,419.00943396226415,192.0,322.0,902.0,902.0,419.00943396226415,419.00943396226415,419.00943396226415,654.0,522.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,788.0,110.0,352.0,419.00943396226415,419.00943396226415,752.0,5.0,98.0,615.0,11.0,295.0,419.00943396226415,352.0,112.0,522.0,344.0,110.0,419.00943396226415,558.0,654.0,420.0,122.0,483.0,911.0,192.0,419.00943396226415,526.0,821.0,419.00943396226415,526.0,295.0,419.00943396226415,419.00943396226415,615.0,775.0,112.0,8.0,977.0,295.0,752.0,112.0,11.0,60.0,823.0,557.0,348.0,419.00943396226415,419.00943396226415,662.0,419.00943396226415,285.0,133.0,129.0,419.00943396226415,419.00943396226415,419.00943396226415,752.0,297.0,419.00943396226415,522.0,15.0,419.00943396226415,468.0,98.0,526.0,60.0,419.00943396226415,37.0,184.0,419.00943396226415,29.0,15.0,376.0,991.0,419.00943396226415,98.0,112.0,526.0,419.00943396226415,483.0,419.00943396226415,29.0,11.0,673.0,419.00943396226415,419.00943396226415,558.0,419.00943396226415,419.00943396226415,244.0,419.00943396226415,60.0,419.00943396226415,615.0,419.00943396226415,848.0,59.0,419.00943396226415,471.0,1.0,557.0,662.0,419.00943396226415,133.0,147.0,419.0,821.0,316.0,419.00943396226415,419.00943396226415,557.0,526.0,10.0,352.0,29.0,11.0,110.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,2.0,419.00943396226415,419.00943396226415,419.0,203.0,272.0,419.00943396226415,419.00943396226415,419.00943396226415,37.0,419.00943396226415,752.0,993.0,902.0,483.0,419.00943396226415,344.0,352.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,991.0,452.0,419.00943396226415,902.0,419.00943396226415,112.0,786.0,94.0,419.00943396226415,274.0,526.0,9.0,786.0,526.0,352.0,786.0,419.00943396226415,902.0,419.00943396226415,419.0,344.0,526.0,98.0,949.0,827.0,186.0,848.0,899.0,419.00943396226415,611.0,376.0,419.00943396226415,419.00943396226415,352.0,419.00943396226415,786.0,419.00943396226415,184.0,821.0,297.0,522.0,110.0,24.0,133.0,449.0,419.00943396226415,419.00943396226415,419.00943396226415,848.0,419.00943396226415,419.00943396226415,112.0,558.0,479.0,911.0,157.0,662.0,419.00943396226415,419.00943396226415,304.0,827.0,557.0,419.00943396226415,752.0,662.0,419.00943396226415,662.0,60.0,65.0,419.00943396226415,86.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,322.0,98.0,244.0,526.0,419.00943396226415,419.00943396226415,244.0,419.00943396226415,122.0,419.00943396226415,113.0,543.0,344.0,419.00943396226415,969.0,419.0,419.00943396226415,419.00943396226415,274.0,47.0,526.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,9.0,50.0,50.0,295.0,419.00943396226415,297.0,522.0,862.0,557.0,419.00943396226415,419.00943396226415,557.0,133.0,419.00943396226415,65.0,543.0,295.0,112.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,29.0,696.0,419.00943396226415,2.0,352.0,79.0,419.0,70.0,735.0,522.0,800.0,419.00943396226415,419.00943396226415,295.0,419.00943396226415,696.0,522.0,981.0,378.0,419.00943396226415,752.0,471.0,996.0,419.00943396226415,33.0,696.0,419.00943396226415,184.0,911.0,526.0,419.00943396226415,419.00943396226415,419.00943396226415,197.0,122.0,50.0,2.0,416.0,318.0,419.00943396226415,419.00943396226415,419.00943396226415,94.0,295.0,419.00943396226415,24.0,649.0,991.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,649.0,419.00943396226415,419.00943396226415,274.0,419.00943396226415,419.00943396226415,419.00943396226415,159.0,419.00943396226415,526.0,159.0,236.0,419.00943396226415,157.0,97.0,254.0,419.00943396226415,419.00943396226415,821.0,236.0,236.0,419.00943396226415,419.00943396226415,9.0,344.0,1.0,50.0,173.0,0.0,526.0,244.0,419.00943396226415,696.0,98.0,70.0,98.0,8.0,419.00943396226415,417.0,696.0,419.00943396226415,98.0,295.0,419.00943396226415,159.0,50.0,899.0,862.0,419.00943396226415,419.00943396226415,79.0,344.0,112.0,7.0,696.0,98.0,344.0,0.0,419.00943396226415,977.0,419.00943396226415,47.0,344.0,419.00943396226415,419.00943396226415,295.0,696.0,899.0,449.0,419.00943396226415,12.0,0.0,0.0,79.0,696.0,419.00943396226415,821.0,419.00943396226415,419.00943396226415,12.0,419.00943396226415,9.0,236.0,419.00943396226415,419.00943396226415,98.0,899.0,348.0,419.00943396226415,419.00943396226415,2.0,819.0,419.00943396226415,29.0,348.0,419.00943396226415,98.0,147.0,268.0,526.0,344.0,316.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,65.0,344.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,12.0,419.00943396226415,526.0,419.00943396226415,662.0,557.0,12.0,234.0,827.0,419.00943396226415,419.00943396226415,12.0,50.0,419.00943396226415,419.00943396226415,64.0,827.0,450.0,419.00943396226415,419.00943396226415,0.0,419.00943396226415,848.0,274.0,150.0,827.0,419.00943396226415,122.0,295.0,377.0,344.0,419.00943396226415,419.00943396226415,7.0,110.0,419.00943396226415,862.0,894.0,192.0,696.0,2.0,419.00943396226415,157.0,344.0,526.0,419.00943396226415,157.0,418.0,823.0,419.00943396226415,662.0,419.00943396226415,568.0,419.00943396226415,696.0,419.00943396226415,419.00943396226415,274.0,419.00943396226415,50.0,236.0,344.0,12.0,146.0,419.00943396226415,696.0,184.0,827.0,192.0,419.00943396226415,419.00943396226415,821.0,344.0,419.00943396226415,419.00943396226415,419.00943396226415,7.0,274.0,348.0,419.0,662.0,902.0,969.0,83.0,1.0,192.0,419.00943396226415,186.0,419.00943396226415,419.00943396226415,419.00943396226415,752.0,543.0,0.0,526.0,344.0,419.0,662.0,401.0,419.00943396226415,991.0,568.0,419.00943396226415,911.0,419.00943396226415,419.00943396226415,696.0,64.0,419.00943396226415,419.00943396226415,33.0,419.00943396226415,419.00943396226415,6.0,419.00943396226415,419.00943396226415,419.00943396226415,894.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,192.0,419.00943396226415,716.0,419.00943396226415,184.0,2.0,608.0,419.00943396226415,9.0,419.00943396226415,147.0,69.0,925.0,79.0,419.00943396226415,419.00943396226415,527.0,527.0,419.00943396226415,274.0,192.0,419.00943396226415,419.00943396226415,64.0,97.0,419.00943396226415,474.0,1.0,991.0,419.00943396226415,991.0,526.0,419.00943396226415,192.0,0.0,419.00943396226415,419.00943396226415,192.0,919.0,59.0,112.0,59.0,419.00943396226415,192.0,419.00943396226415,419.00943396226415,122.0,419.00943396226415,192.0,79.0,419.00943396226415,344.0,0.0,522.0,419.00943396226415,821.0,848.0,419.00943396226415,501.0,419.00943396226415,419.00943396226415,689.0,76.0,297.0,991.0,344.0,716.0,611.0,419.00943396226415,295.0,419.00943396226415,419.00943396226415,557.0,599.0,419.00943396226415,443.0,5.0,419.00943396226415,526.0,419.00943396226415,716.0,97.0,819.0,419.00943396226415,419.00943396226415,419.00943396226415,285.0,419.00943396226415,311.0,507.0,479.0,419.00943396226415,483.0,696.0,311.0,419.00943396226415,79.0,419.00943396226415,419.00943396226415,132.0,419.00943396226415,911.0,419.00943396226415,527.0,568.0,558.0,419.00943396226415,418.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,0.0,419.00943396226415,419.00943396226415,15.0,419.00943396226415,716.0,911.0,419.00943396226415,716.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,274.0,419.00943396226415,615.0,981.0,304.0,419.00943396226415,173.0,419.00943396226415,419.00943396226415,29.0,419.00943396226415,236.0,527.0,419.00943396226415,419.00943396226415,419.00943396226415,417.0,696.0,37.0,843.0,419.00943396226415,192.0,419.00943396226415,236.0,419.00943396226415,0.0,192.0,419.00943396226415,130.0,79.0,419.00943396226415,419.00943396226415,285.0,419.00943396226415,419.00943396226415,526.0,416.0,599.0,419.00943396226415,419.00943396226415,752.0,775.0,419.00943396226415,133.0,419.00943396226415,274.0,823.0,419.00943396226415,949.0,827.0,311.0,699.0,419.00943396226415,11.0,526.0,236.0,274.0,527.0,419.00943396226415,419.00943396226415,689.0,236.0,60.0,419.00943396226415,419.00943396226415,344.0,12.0,993.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,294.0,419.00943396226415,419.00943396226415,991.0,304.0,186.0,611.0,585.0,285.0,419.00943396226415,98.0,97.0,285.0,419.00943396226415,150.0,318.0,752.0,87.0,419.00943396226415,419.00943396226415,501.0,37.0,29.0,419.00943396226415,419.00943396226415,12.0,419.00943396226415,558.0,186.0,418.0,419.00943396226415,419.00943396226415,949.0,662.0,419.00943396226415,37.0,419.00943396226415,419.00943396226415,419.00943396226415,22.0,419.00943396226415,419.00943396226415,419.00943396226415,329.0,419.00943396226415,419.00943396226415,608.0,419.00943396226415,419.00943396226415,419.00943396226415,527.0,419.00943396226415,911.0,419.00943396226415,419.00943396226415,285.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,311.0,192.0,419.00943396226415,147.0,419.00943396226415,79.0,147.0,419.00943396226415,419.00943396226415,419.00943396226415,419.0,419.00943396226415,419.00943396226415,344.0,419.00943396226415,419.00943396226415,15.0,419.00943396226415,1.0,419.00943396226415,419.00943396226415,419.00943396226415,76.0,843.0,8.0,419.00943396226415,419.00943396226415,543.0,419.00943396226415,419.00943396226415,848.0,419.00943396226415,76.0,419.00943396226415,401.0,419.0,285.0,821.0,192.0,419.00943396226415,344.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,919.0,419.00943396226415,419.00943396226415,419.00943396226415,197.0,297.0,147.0,419.00943396226415,419.00943396226415,419.00943396226415,919.0,318.0,348.0,775.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,902.0,419.00943396226415,419.00943396226415,894.0,348.0,419.00943396226415,919.0,419.00943396226415,419.00943396226415,902.0,969.0,419.00943396226415,419.00943396226415,79.0,419.00943396226415,568.0,419.00943396226415,419.00943396226415,419.00943396226415,192.0,419.0,819.0,419.00943396226415,377.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,274.0,419.00943396226415,419.00943396226415,76.0,418.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,79.0,147.0,848.0,419.00943396226415,419.00943396226415,419.00943396226415,328.0,419.00943396226415,419.00943396226415,911.0,419.00943396226415,112.0,113.0,8.0,348.0,316.0,585.0,192.0,654.0,419.00943396226415,419.00943396226415,526.0,419.00943396226415,526.0,543.0,419.00943396226415,67.0,157.0,274.0,192.0,419.00943396226415,419.00943396226415,418.0,419.00943396226415,419.00943396226415,894.0,419.00943396226415,348.0,0.0,147.0,696.0,419.00943396226415,419.00943396226415,419.00943396226415,192.0,419.00943396226415,607.0,775.0,911.0,419.00943396226415,150.0,419.00943396226415,419.00943396226415,352.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,377.0,348.0,419.00943396226415,752.0,419.00943396226415,76.0,344.0,419.00943396226415,919.0,419.00943396226415,419.00943396226415,0.0,419.00943396226415,419.00943396226415,419.00943396226415,752.0,328.0,419.00943396226415,192.0,419.00943396226415,38.0,443.0,79.0,419.00943396226415,419.00943396226415,274.0,419.00943396226415,443.0,419.00943396226415,419.00943396226415,543.0,911.0,112.0,419.00943396226415,419.00943396226415,419.00943396226415,516.0,304.0,418.0,318.0,419.00943396226415,37.0,977.0,419.00943396226415,419.00943396226415,67.0,977.0,37.0,969.0,419.00943396226415,12.0,419.00943396226415,775.0,419.00943396226415,419.00943396226415,587.0,11.0,599.0,981.0,419.00943396226415,419.00943396226415,419.00943396226415,716.0,828.0,752.0,639.0,419.00943396226415,419.00943396226415,843.0,192.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,843.0,150.0,419.00943396226415,79.0,419.00943396226415,419.00943396226415,419.00943396226415,919.0,419.00943396226415,902.0,329.0,419.00943396226415,419.00943396226415,599.0,112.0,419.00943396226415,419.00943396226415,1.0,419.00943396226415,419.00943396226415,419.00943396226415,11.0,419.00943396226415,611.0,843.0,419.00943396226415,294.0,322.0,899.0,419.00943396226415,419.00943396226415,752.0,376.0,419.00943396226415,0.0,418.0,419.00943396226415,294.0,9.0,47.0,752.0,419.00943396226415,419.00943396226415,662.0,419.00943396226415,419.00943396226415,981.0,419.00943396226415,752.0,419.00943396226415,2.0,419.00943396226415,419.00943396226415,419.00943396226415,627.0,365.0,32.0,32.0,67.0,419.00943396226415,627.0,419.00943396226415,417.0,419.00943396226415,788.0,418.0,419.00943396226415,996.0,419.00943396226415,419.00943396226415,64.0,911.0,401.0,419.0,419.00943396226415,64.0,419.00943396226415,419.00943396226415,925.0,419.00943396226415,274.0,419.00943396226415,295.0,197.0,419.00943396226415,419.00943396226415,157.0,316.0,419.00943396226415,419.00943396226415,419.00943396226415,9.0,192.0,160.0,419.00943396226415,419.00943396226415,558.0,60.0,64.0,419.00943396226415,419.00943396226415,344.0,419.00943396226415,419.00943396226415,419.00943396226415,344.0,419.00943396226415,543.0,419.00943396226415,419.00943396226415,827.0,819.0,23.0,501.0,419.00943396226415,28.0,419.00943396226415,419.00943396226415,452.0,133.0,848.0,419.00943396226415,419.00943396226415,419.00943396226415,483.0,344.0,419.00943396226415,87.0,146.0,419.00943396226415,419.00943396226415,97.0,568.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,238.0,419.00943396226415,12.0,295.0,419.00943396226415,419.0,752.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,527.0,419.00943396226415,827.0,419.00943396226415,236.0,419.00943396226415,991.0,419.00943396226415,419.00943396226415,419.00943396226415,449.0,654.0,419.00943396226415,419.00943396226415,113.0,419.00943396226415,197.0,236.0,419.00943396226415,419.00943396226415,419.0,599.0,419.00943396226415,419.00943396226415,23.0,568.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,627.0,38.0,419.00943396226415,419.00943396226415,827.0,76.0,295.0,823.0,348.0,419.00943396226415,419.00943396226415,419.00943396226415,76.0,599.0,419.00943396226415,827.0,733.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,827.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,823.0,419.00943396226415,419.00943396226415,274.0,64.0,419.00943396226415,419.00943396226415,2.0,419.00943396226415,419.00943396226415,419.00943396226415,60.0,66.0,419.00943396226415,419.00943396226415,419.00943396226415,7.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,266.0,827.0,752.0,419.00943396226415,419.00943396226415,157.0,419.00943396226415,419.00943396226415,386.0,192.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,568.0,150.0,899.0,418.0,607.0,419.00943396226415,159.0,295.0,419.00943396226415,419.00943396226415,419.00943396226415,199.0,419.00943396226415,419.00943396226415,911.0,977.0,419.00943396226415,37.0,419.00943396226415,419.00943396226415,236.0,419.00943396226415,66.0,419.00943396226415,419.00943396226415,899.0,919.0,419.00943396226415,419.00943396226415,67.0,112.0,716.0,419.00943396226415,64.0,911.0,419.00943396226415,311.0,419.00943396226415,449.0,848.0,752.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,301.0,419.00943396226415,419.00943396226415,419.00943396226415,843.0,823.0,311.0,848.0,419.00943396226415,419.00943396226415,419.00943396226415,689.0,925.0,15.0,419.00943396226415,419.00943396226415,501.0,419.00943396226415,192.0,304.0,449.0,568.0,79.0,236.0,419.00943396226415,419.00943396226415,419.00943396226415,527.0,60.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,15.0,419.00943396226415,996.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,97.0,419.00943396226415,419.00943396226415,295.0,419.00943396226415,607.0,419.00943396226415,558.0,823.0,823.0,991.0,925.0,925.0,419.00943396226415,94.0,419.00943396226415,419.00943396226415,236.0,991.0,236.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,67.0,419.00943396226415,419.00943396226415,991.0,419.00943396226415,419.00943396226415,419.00943396226415,295.0,919.0,419.00943396226415,419.00943396226415,352.0,925.0,419.00943396226415,76.0,949.0,419.00943396226415,419.00943396226415,344.0,419.00943396226415,419.00943396226415,419.00943396226415,5.0,2.0,419.00943396226415,443.0,419.00943396226415,419.00943396226415,848.0,419.00943396226415,419.00943396226415,8.0,419.00943396226415,419.0,419.00943396226415,419.00943396226415,419.00943396226415,925.0,419.00943396226415,419.00943396226415,828.0,911.0,2.0,419.00943396226415,419.00943396226415,301.0,419.00943396226415,419.00943396226415,419.00943396226415,450.0,295.0,419.00943396226415,47.0,733.0,752.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,133.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,268.0,752.0,733.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,76.0,925.0,419.00943396226415,76.0,911.0,419.00943396226415,419.00943396226415,479.0,419.00943396226415,419.00943396226415,386.0,419.00943396226415,236.0,848.0,419.00943396226415,419.00943396226415,159.0,673.0,419.00943396226415,419.00943396226415,450.0,419.00943396226415,112.0,199.0,419.00943396226415,827.0,316.0,418.0,419.00943396226415,673.0,419.00943396226415,27.0,159.0,419.00943396226415,76.0,1.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,79.0,0.0,419.00943396226415,7.0,419.00943396226415,894.0,823.0,76.0,419.00943396226415,419.00943396226415,419.00943396226415,344.0,344.0,419.00943396226415,419.00943396226415,236.0,911.0,419.00943396226415,419.00943396226415,827.0,419.00943396226415,38.0,419.00943396226415,419.00943396226415,419.00943396226415,352.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,899.0,328.0,969.0,344.0,386.0,419.00943396226415,419.00943396226415,97.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,568.0,419.00943396226415,468.0,981.0,133.0,419.00943396226415,419.00943396226415,800.0,419.00943396226415,419.00943396226415,419.00943396226415,654.0,236.0,419.00943396226415,112.0,419.00943396226415,419.00943396226415,935.0,197.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,329.0,419.00943396226415,112.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,60.0,419.00943396226415,611.0,419.00943396226415,15.0,419.00943396226415,94.0,419.00943396226415,419.00943396226415,925.0,419.00943396226415,419.00943396226415,6.0,899.0,238.0,419.00943396226415,419.00943396226415,899.0,419.00943396226415,50.0,419.00943396226415,419.00943396226415,568.0,419.00943396226415,416.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,173.0,28.0,419.00943396226415,827.0,419.00943396226415,843.0,419.00943396226415,419.00943396226415,419.00943396226415,329.0,899.0,419.00943396226415,238.0,899.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,160.0,419.00943396226415,419.00943396226415,419.00943396226415,843.0,318.0,823.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,821.0,133.0,419.00943396226415,419.00943396226415,147.0,419.00943396226415,419.00943396226415,599.0,627.0,419.00943396226415,419.00943396226415,318.0,419.00943396226415,848.0,419.00943396226415,419.00943396226415,419.00943396226415,265.0,419.00943396226415,899.0,419.00943396226415,419.00943396226415,696.0,419.00943396226415,419.00943396226415,419.00943396226415,192.0,419.00943396226415,843.0,827.0,419.00943396226415,59.0,236.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,266.0,316.0,329.0,419.00943396226415,911.0,419.00943396226415,67.0,419.00943396226415,50.0,733.0,419.00943396226415,419.00943396226415,295.0,419.00943396226415,329.0,419.00943396226415,76.0,133.0,827.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,67.0,607.0,76.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,418.0,821.0,419.00943396226415,419.00943396226415,419.00943396226415,274.0,848.0,848.0,848.0,848.0,419.00943396226415,483.0,236.0,236.0,236.0,419.00943396226415,419.0,79.0,376.0,244.0,699.0,935.0,79.0,419.00943396226415,419.00943396226415,419.00943396226415,911.0,236.0,419.00943396226415,76.0,419.00943396226415,419.00943396226415,76.0,147.0,911.0,419.00943396226415,981.0,419.00943396226415,419.00943396226415,419.00943396226415,819.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,236.0,468.0,419.00943396226415,76.0,800.0,322.0,419.00943396226415,50.0,419.00943396226415,568.0,419.00943396226415,419.00943396226415,67.0,419.00943396226415,419.00943396226415,419.00943396226415,443.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,899.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,911.0,419.00943396226415,186.0,935.0,981.0,79.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,386.0,735.0,79.0,949.0,419.00943396226415,654.0,419.00943396226415,329.0,419.00943396226415,419.00943396226415,543.0,419.00943396226415,419.00943396226415,788.0,15.0,419.00943396226415,419.00943396226415,419.00943396226415,501.0,911.0,419.00943396226415,848.0,419.00943396226415,419.00943396226415,819.0,419.00943396226415,419.00943396226415,56.0,419.00943396226415,649.0,348.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,819.0,925.0,265.0,419.00943396226415,419.00943396226415,419.00943396226415,133.0,304.0,236.0,981.0,716.0,419.00943396226415,112.0,752.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,50.0,112.0,419.00943396226415,733.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,236.0,376.0,607.0,38.0,843.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,654.0,419.00943396226415,843.0,419.00943396226415,419.00943396226415,419.00943396226415,654.0,419.00943396226415,419.00943396226415,527.0,50.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,981.0,192.0,443.0,79.0,318.0,419.00943396226415,419.00943396226415,328.0,925.0,419.00943396226415,1.0,827.0,419.00943396226415,735.0,899.0,419.00943396226415,419.00943396226415,419.00943396226415,15.0,419.00943396226415,0.0,419.00943396226415,419.00943396226415,419.00943396226415,1.0,94.0,568.0,419.00943396226415,419.00943396226415,419.00943396226415,450.0,1.0,419.00943396226415,925.0,285.0,419.00943396226415,775.0,419.00943396226415,733.0,79.0,419.00943396226415,419.00943396226415,527.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,133.0,419.00943396226415,76.0,419.00943396226415,419.00943396226415,419.00943396226415,318.0,419.00943396226415,419.00943396226415,344.0,419.00943396226415,419.00943396226415,419.00943396226415,79.0,5.0,419.00943396226415,419.00943396226415,419.00943396226415,189.0,285.0,328.0,97.0,419.00943396226415,27.0,800.0,419.00943396226415,823.0,419.00943396226415,419.00943396226415,419.00943396226415,285.0,419.00943396226415,419.00943396226415,419.00943396226415,79.0,64.0,568.0,419.00943396226415,24.0,419.00943396226415,419.00943396226415,568.0,419.00943396226415,419.00943396226415,192.0,419.00943396226415,752.0,419.00943396226415,419.00943396226415,0.0,2.0,2.0,607.0,59.0,607.0,1.0,17.0,23.0,419.00943396226415,443.0,419.00943396226415,981.0,419.00943396226415,419.00943396226415,420.0,419.00943396226415,468.0,419.00943396226415,419.00943396226415,419.00943396226415,449.0,419.00943396226415,419.00943396226415,419.00943396226415,925.0,649.0,65.0,419.00943396226415,316.0,419.00943396226415,401.0,419.00943396226415,468.0,419.00943396226415,788.0,419.00943396226415,419.00943396226415,419.00943396226415,56.0,419.00943396226415,419.00943396226415,419.0,419.00943396226415,919.0,419.00943396226415,821.0,285.0,419.00943396226415,47.0,419.00943396226415,0.0,819.0,662.0,419.00943396226415,449.0,419.00943396226415,419.0,419.00943396226415,419.00943396226415,775.0,419.00943396226415,911.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,268.0,419.00943396226415,419.00943396226415,819.0,419.00943396226415,419.00943396226415,268.0,265.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,689.0,699.0,265.0,911.0,419.00943396226415,419.00943396226415,344.0,568.0,419.00943396226415,819.0,419.00943396226415,316.0,386.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,526.0,419.00943396226415,981.0,911.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,819.0,419.00943396226415,1.0,819.0,329.0,419.00943396226415,67.0,419.00943396226415,474.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,522.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415,522.0,59.0,316.0,568.0,419.00943396226415,419.00943396226415,449.0,522.0,419.00943396226415,848.0,419.00943396226415,419.00943396226415,843.0,419.00943396226415,775.0,419.00943396226415,160.0,716.0,419.00943396226415,419.00943396226415,419.00943396226415,236.0,419.00943396226415,419.00943396226415,419.00943396226415,419.00943396226415],"xaxis":"x","y":[30900.0,139999.0,132999.0,58587.0,95990.0,74999.0,130999.0,55995.0,71999.0,62975.0,82895.0,50226.0,22495.0,63998.0,65024.0,69771.0,51995.0,57877.0,53977.0,22498.0,28784.0,67800.0,42595.0,74788.0,58770.0,53690.0,46900.0,38996.0,69306.0,39442.0,112749.0,57900.0,49995.0,153722.0,43855.0,50563.0,74887.0,59413.0,68983.0,46300.0,62421.0,198000.0,36900.0,62292.0,45991.0,50299.0,98984.0,65992.0,77700.0,65514.0,54660.0,143554.0,67800.0,54000.0,57463.0,84997.0,74377.0,57900.0,49913.0,21082.0,161775.0,79711.0,48500.0,50900.0,39950.0,54018.0,38963.0,107552.0,50595.0,29999.0,47562.0,50888.0,95981.0,98555.0,57445.0,94995.0,43105.0,55998.0,41000.0,39237.0,58798.0,30880.0,26499.0,33497.0,177595.0,69998.0,120997.0,142000.0,48000.0,73998.0,65444.0,69997.0,157500.0,45705.0,114750.0,41948.0,99805.0,39950.0,67910.0,60312.0,58997.0,35824.0,55995.0,70991.0,86690.0,50900.0,49400.0,60994.0,53922.0,67585.0,56940.0,62704.0,209995.0,55000.0,41690.0,44744.0,65999.0,49983.0,87390.0,111326.0,68500.0,67515.0,33999.0,31293.0,69997.0,99999.0,78900.0,36880.0,42499.0,20999.0,74988.0,28000.0,26540.0,94984.0,86975.0,77000.0,53900.0,112997.0,46888.0,97397.0,49777.0,41200.0,43777.0,68398.0,73990.0,49500.0,48846.0,45900.0,93888.0,47222.0,33951.0,59900.0,37650.0,59700.0,27412.0,57453.0,44196.0,45551.0,39864.0,41929.0,44777.0,30400.0,31336.0,107999.0,89999.0,98444.0,99795.0,105999.0,43900.0,47222.0,78899.0,44222.0,112498.0,46888.0,67983.0,55089.0,68274.0,76777.0,46777.0,199865.0,178899.0,43888.0,92499.0,48998.0,61991.0,33797.0,125997.0,149995.0,30880.0,46900.0,93882.0,53445.0,50754.0,108337.0,38740.0,38997.0,47721.0,34600.0,29397.0,173880.0,48995.0,50888.0,164882.0,157771.0,58702.0,35595.0,60146.0,79987.0,46888.0,60977.0,36295.0,85987.0,158880.0,65987.0,30502.0,35887.0,49995.0,35000.0,67500.0,45955.0,79894.0,64911.0,54750.0,108946.0,58998.0,139994.0,52888.0,41069.0,79998.0,101998.0,119899.0,78900.0,87902.0,39955.0,64992.0,77900.0,135990.0,20657.0,37870.0,69795.0,84690.0,61992.0,73499.0,47399.0,130994.0,46519.0,29598.0,119510.0,35878.0,46999.0,50477.0,142106.0,36295.0,52988.0,24862.0,54495.0,86881.0,46999.0,33316.0,32390.0,53436.0,43497.0,98995.0,28975.0,45999.0,79876.0,81921.0,85175.0,71998.0,48998.0,80991.0,39843.0,77802.0,57900.0,74995.0,52000.0,45712.0,106777.0,57642.0,49999.0,44777.0,49881.0,126995.0,30955.0,28990.0,104900.0,64443.0,34988.0,79997.0,58997.0,41994.0,127500.0,27620.0,44990.0,164900.0,177595.0,223414.0,72555.0,32998.0,73890.0,36991.0,77033.0,85500.0,89888.0,28988.0,51897.0,55900.0,89995.0,61805.0,98999.0,164994.0,49995.0,82980.0,81994.0,125690.0,50563.0,50905.0,98509.0,75850.0,49445.0,63987.0,45375.0,54777.0,61997.0,27477.0,47995.0,110950.0,30323.0,45950.0,41850.0,39999.0,144870.0,80594.0,109554.0,66413.0,99997.0,65595.0,99998.0,103044.0,50994.0,75998.0,123881.0,50194.0,45142.0,62000.0,63777.0,57499.0,103500.0,18578.0,96713.0,49988.0,35999.0,73800.0,89597.0,72490.0,42640.0,48932.0,65595.0,159990.0,56998.0,60495.0,34551.0,48350.0,41999.0,43399.0,86925.0,52744.0,89900.0,57007.0,28484.0,24999.0,49988.0,128887.0,61998.0,46995.0,43495.0,47647.0,46777.0,30400.0,85292.0,69287.0,50997.0,78999.0,72293.0,142825.0,53988.0,86249.0,56004.0,45982.0,29598.0,37995.0,28954.0,44999.0,80994.0,72865.0,36295.0,35897.0,44210.0,40000.0,221994.0,53895.0,86995.0,57988.0,41280.0,49995.0,40999.0,38200.0,23949.0,98140.0,27995.0,56997.0,47875.0,99777.0,39497.0,37490.0,76589.0,51500.0,67466.0,64876.0,33397.0,47893.0,87987.0,44898.0,36131.0,36991.0,64815.0,51490.0,47999.0,32300.0,85000.0,54987.0,58375.0,46099.0,78000.0,39999.0,150000.0,62988.0,63550.0,30904.0,46912.0,54998.0,157771.0,44997.0,38482.0,164900.0,74994.0,54981.0,169661.0,62988.0,73495.0,39100.0,38689.0,76286.0,64955.0,50988.0,203999.0,38288.0,179120.0,56994.0,64995.0,24688.0,65507.0,47882.0,80995.0,58883.0,45998.0,36125.0,42997.0,49495.0,49995.0,219995.0,44150.0,129995.0,46288.0,69998.0,33498.0,89990.0,58991.0,124988.0,39982.0,22913.0,109997.0,53983.0,98444.0,52495.0,55699.0,80654.0,55990.0,38492.0,44777.0,74999.0,40493.0,129618.0,53791.0,32339.0,63208.0,21900.0,59410.0,24991.0,20437.0,46980.0,41584.0,40970.0,30791.0,45994.0,26840.0,50991.0,27500.0,38497.0,43900.0,47777.0,49988.0,37539.0,44995.0,49850.0,43888.0,43860.0,61000.0,40016.0,52188.0,55997.0,141215.0,32000.0,34495.0,69992.0,109554.0,8999.0,44299.0,46417.0,44222.0,28995.0,28999.0,54981.0,26667.0,76987.0,40699.0,49884.0,45553.0,81000.0,43888.0,30966.0,48220.0,74442.0,70500.0,37037.0,69612.0,109554.0,42900.0,32298.0,29211.0,30500.0,29991.0,26885.0,69990.0,58003.0,77306.0,48982.0,44933.0,41497.0,40000.0,48950.0,41574.0,29000.0,21844.0,44185.0,134495.0,30985.0,58003.0,68990.0,27495.0,69892.0,32984.0,62777.0,42991.0,56111.0,61285.0,39987.0,41988.0,53777.0,42981.0,157991.0,74519.0,45994.0,72777.0,38995.0,58990.0,41990.0,85777.0,58883.0,118988.0,55994.378500823725,53900.0,43299.0,44421.0,51990.0,26995.0,45485.0,65595.0,40888.0,39995.0,37872.0,29495.0,41990.0,53275.0,104357.0,48888.0,80239.0,32795.0,38866.0,45553.0,48487.0,44699.0,69459.0,39577.0,49777.0,52627.0,61880.0,52500.0,36998.0,57981.0,50515.0,31695.0,85991.0,38000.0,77586.0,56900.0,30900.0,30877.0,66888.0,48000.0,57995.0,42989.0,55123.0,37893.0,69500.0,45500.0,44500.0,55989.0,57495.0,31777.0,40888.0,45900.0,36900.0,49997.0,44998.0,90000.0,49988.0,81494.0,41988.0,56001.0,45900.0,77898.0,37983.0,65466.0,97888.0,47950.0,38746.0,19498.0,72992.0,55650.0,52990.0,99495.0,39495.0,38900.0,41874.0,64777.0,39975.0,68910.0,50777.0,22992.0,58441.0,84614.0,44810.0,45988.0,43900.0,37978.0,43991.0,62706.0,29933.0,164900.0,32000.0,55984.0,77400.0,55777.0,159565.0,52751.0,42000.0,37994.0,33967.0,43900.0,46789.0,55184.0,53899.0,49990.0,36900.0,30752.0,95879.0,33695.0,49777.0,45968.0,53489.0,45900.0,55651.0,54495.0,55803.0,32989.0,29816.0,41998.0,58199.0,40995.0,90988.0,96838.0,41488.0,62981.0,30999.0,90000.0,42777.0,44162.0,46401.0,55117.0,59000.0,42300.0,28481.0,101888.0,43895.0,46900.0,46411.0,36633.0,76900.0,56777.0,219805.0,32337.0,42989.0,66888.0,55101.0,110084.0,30653.0,47495.0,30775.0,46999.0,109990.0,44788.0,66205.0,28395.0,16998.0,40670.0,27904.0,44963.0,55904.0,50988.0,54900.0,67800.0,47888.0,55936.0,221994.0,30423.0,43981.0,42495.0,69999.0,44997.0,229654.0,42787.0,46987.0,39490.0,33600.0,47605.0,28725.0,44990.0,28995.0,33395.0,40952.0,48988.0,43991.0,25999.0,84800.0,52888.0,41997.0,56156.0,46685.0,50900.0,46206.0,69989.0,154200.0,38954.0,67399.0,34984.0,50257.0,47346.0,56900.0,55444.0,73870.0,78900.0,42888.0,57980.0,58883.0,38350.0,42900.0,35918.0,65712.0,81891.0,43888.0,73777.0,49995.0,78933.0,45305.0,41500.0,48474.0,31991.0,24400.0,54900.0,78499.0,129999.0,37494.0,53658.0,55800.0,34537.0,57441.0,45988.0,43498.0,112999.0,44000.0,52000.0,25894.0,26995.0,47203.0,83982.0,42681.0,34726.0,49491.0,46444.0,45677.0,49000.0,26497.0,38599.0,42574.0,58999.0,27500.0,47305.0,40999.0,47882.0,30998.0,47308.0,48306.0,69800.0,32140.0,57899.0,35922.0,47509.0,30991.0,71888.0,45998.0,34377.0,64502.0,125880.0,68500.0,54811.0,44499.0,52660.0,50977.0,28795.0,39154.0,139250.0,39491.0,36988.0,83824.0,42991.0,48994.0,42487.0,33877.0,28475.0,41940.0,47893.0,49700.0,55835.0,38000.0,29625.0,33937.0,38444.0,62756.0,29250.0,32997.0,67890.0,51000.0,53484.0,35598.0,53148.0,68998.0,94777.0,72390.0,30583.0,45994.0,73345.0,38552.0,90000.0,154900.0,44990.0,54317.0,45888.0,39521.0,76992.0,90988.0,92896.0,57900.0,37823.0,77465.0,83777.0,39894.0,39500.0,67989.0,43992.0,36885.0,42300.0,37899.0,45645.0,77482.0,46991.0,38098.0,39659.0,44851.0,32589.0,52075.0,28995.0,46328.0,39494.0,45935.0,52713.0,34445.0,60833.0,44900.0,40007.0,68113.0,46688.0,63910.0,139977.0,39995.0,81593.0,50881.0,58796.0,36000.0,35595.0,41733.0,159979.0,45281.0,35650.0,45900.0,43988.0,63000.0,37995.0,70490.0,39900.0,42735.0,59546.0,44000.0,39592.0,54999.0,30934.0,38995.0,50284.0,40775.0,71989.0,47502.0,53510.0,37974.0,54480.0,40998.0,37500.0,47791.0,33295.0,50639.0,112998.0,55991.0,49888.0,53910.0,43881.0,39988.0,28000.0,29052.0,75890.0,63491.0,27995.0,40900.0,27250.0,40495.0,31416.0,35475.0,59750.0,38995.0,41999.0,32230.0,40990.0,42995.0,45000.0,96510.0,23991.0,38999.0,33738.0,38594.0,50555.0,41981.0,51788.0,26199.0,28888.0,31593.0,88977.0,60900.0,43498.0,129991.0,31652.0,31756.0,46998.0,169995.0,47100.0,27990.0,52983.0,33750.0,32998.0,87851.0,42995.0,33991.0,30998.0,39634.0,52892.0,32485.0,35988.0,72666.0,58998.0,54999.0,54994.0,38314.0,34650.0,224891.0,41999.0,37500.0,29996.0,43345.0,29500.0,31933.0,36988.0,38989.0,36390.0,70164.0,52777.0,36852.0,71994.0,43888.0,36900.0,55900.0,49444.0,55777.0,61999.0,58880.0,51469.0,54329.0,23199.0,34500.0,89500.0,144999.0,46690.0,43992.0,71892.0,40684.0,41888.0,40102.0,46315.0,27825.0,43988.0,42700.0,30979.0,34238.0,58988.0,39400.0,47644.0,42344.0,40600.0,41594.0,44888.0,51933.0,40997.0,53686.0,30840.0,50997.0,43000.0,209894.0,60566.0,55998.0,34991.0,50792.0,57774.0,60470.0,47990.0,51555.0,29990.0,37890.0,216999.0,50991.0,53000.0,39997.0,42892.0,31152.0,44990.0,45925.0,32685.0,61997.0,31562.0,41200.0,48400.0,46977.0,50788.0,56739.0,71892.0,77945.0,53965.0,28013.0,26433.0,47044.0,218937.0,62900.0,67897.0,40000.0,41964.0,45693.0,42963.0,60000.0,39485.0,49779.0,34900.0,85999.0,34522.0,47952.0,189888.0,43905.0,48006.0,46995.0,40989.0,48999.0,44941.0,51971.0,76765.0,29526.0,28999.0,35988.0,44932.0,55900.0,39480.0,41999.0,44989.0,57505.0,75999.0,46999.0,47804.0,95997.0,54395.0,40894.0,51995.0,61998.0,53997.0,53717.0,72007.0,39964.0,41495.0,49991.0,47900.0,37496.0,34833.0,137900.0,29275.0,36839.0,219894.0,34977.0,39400.0,69867.0,29960.0,39988.0,49998.0,41498.0,31998.0,34746.0,42994.0,35888.0,32928.0,72550.0,24990.0,50999.0,30735.0,70226.0,51507.0,100804.0,53698.0,45244.0,27695.0,86989.0,46939.0,65991.0,46500.0,64986.0,50481.0,42495.0,30960.0,129351.0,59991.0,41900.0,47983.0,40983.0,46220.0,30995.0,50999.0,55110.0,42200.0,47888.0,43977.0,53974.0,165994.0,34690.0,43998.0,38000.0,21958.0,85777.0,60125.0,84994.0,38795.0,31495.0,77991.0,43481.0,50384.0,36888.0,41600.0,33896.0,103995.0,51999.0,26565.0,30987.0,55990.0,41990.0,32395.0,46446.0,52835.0,52835.0,30972.0,38899.0,67035.0,45844.0,58884.0,79988.0,34998.0,29892.0,49475.0,165997.0,26900.0,41000.0,31988.0,41300.0,48777.0,30995.0,57425.0,48000.0,128388.0,54776.0,48991.0,45994.0,109995.0,30756.0,49777.0,39989.0,52496.0,47300.0,33233.0,36985.0,56991.0,48000.0,44900.0,43777.0,79991.0,35986.0,41900.0,60200.0,66876.0,69777.0,41000.0,38000.0,35900.0,45777.0,55999.0,92857.0,67997.0,77995.0,30888.0,40563.0,61994.0,97892.0,54493.0,42000.0,47988.0,41990.0,46000.0,56997.0,53555.0,46908.0,33795.0,54954.0,73890.0,56444.0,57990.0,51555.0,169999.0,30999.0,44480.0,74999.0,53840.0,61777.0,94001.0,55212.0,39874.0,43440.0,78820.0,119995.0,71988.0,51696.0,52473.0,29527.0,69991.0,44998.0,68204.0,30994.0,38999.0,34410.0,38893.0,43555.0,85883.0,41000.0,29990.0,94983.0,69846.0,45777.0,43987.0,31987.0,77000.0,34989.0,36400.0,32817.0,58000.0,32995.0,52399.0,39988.0,33950.0,55521.0,36742.0,46295.0,51860.0,28496.0,49999.0,42988.0,55414.0,82994.0,41884.0,45998.0,29695.0,41432.0,28999.0,52466.0,47735.0,42986.0,67994.0,41973.0,43988.0,30500.0,47695.0,79799.0,33833.0,31773.0,29883.0,40900.0,63874.0,56481.0,55963.0,41985.0,29997.0,55900.0,38597.0,72765.0,24899.0,53441.0,29999.0,55995.0,38692.0,46398.0,21360.0,79700.0,52894.0,27981.0,33890.0,41990.0,43777.0,25632.0,48000.0,55349.0,41492.0,37616.0,48777.0,31925.0,38494.0,29890.0,43777.0,43983.0,51788.0,41883.0,32999.0,30933.0,45998.0,38995.0,35991.0,36884.0,98482.0,46692.0,24998.0,39800.0,128991.0,39988.0,27598.0,53332.0,51901.0,46098.0,83890.0,68853.0,81004.0,75007.0,34354.0,79991.0,51991.0,198888.0,55888.0,37295.0,43883.0,42588.0,64991.0,33989.0,48988.0,45000.0,43573.0,56894.0,65030.0,49983.0,38991.0,49145.0,46235.0,59990.0,31994.0,32309.0,32000.0,79990.0,42496.0,56775.0,45996.0,43600.0,45997.0,30988.0,45992.0,187987.0,176988.0,29893.0,34683.0,50000.0,31890.0,44800.0,29888.0,36789.0,34995.0,43990.0,42763.0,42775.0,114895.0,25983.0,182975.0,50000.0,44157.0,37700.0,42793.0,26880.0,39562.0,37259.0,36990.0,47998.0,35250.0,34597.0,48888.0,40039.0,41777.0,38761.0,30989.0,53983.0,47840.0,39699.0,48250.0,45699.0,52777.0,55004.0,42916.0,39495.0,32900.0,170882.0,30295.0,45768.0,41990.0,55498.0,98900.0,70990.0,53476.0,30337.0,32204.0,45990.0,72488.0,29894.0,52997.0,50435.0,69963.0,41991.0,28989.0,97999.0,29128.0,48700.0,53637.0,109977.0,43892.0,36866.0,39894.0,31894.0,63486.0,199860.0,77900.0,62988.0,49983.0,54995.0,39997.0,42880.0,40996.0,41100.0,44998.0,21260.0,35500.0,38494.0,36999.0,95984.0,87993.0,47845.0,39981.0,159990.0,60777.0,75445.0,67960.0,73980.0,35678.0,44988.0,28886.0,169999.0,47261.0,44995.0,29999.0,39948.0,36221.0,43995.0,84950.0,45000.0,29946.0,53484.0,69888.0,51989.0,46307.0,47983.0,35233.0,46000.0,42595.0,24586.0,84444.0,165966.0,55997.0,63899.0,47800.0,26999.0,26140.0,25994.0,37900.0,51200.0,41000.0,47983.0,27998.0,41990.0,87770.0,51499.0,40900.0,49983.0,67984.0,40501.0,64484.0,91433.0,29886.0,19990.0,47131.0,27750.0,37417.0,58594.0,61988.0,36851.0,91888.0,48500.0,133448.0,48787.0,42999.0,27583.0,54840.0,41998.0,45994.0,66990.0,75450.0,47851.0,56555.0,99900.0,32221.0,132228.0,48000.0,37393.0,75008.0,38991.0,36991.0,40122.0,66690.0,39900.0,64991.0,64900.0,27749.0,46800.0,49000.0,29989.0,53988.0,43545.0,44562.0,64000.0,63933.0,44000.0,39894.0,47618.0,81285.0,79900.0,15888.0,41403.0,212894.0,31881.0,49885.0,81894.0,30698.0,31986.0,56999.0,73362.0,47983.0,37882.0,36991.0,44977.0,118500.0,92676.0,27497.0,42000.0,50490.0,170388.0,129900.0,53900.0,47675.0,37894.0,37661.0,34984.0,192997.0,43777.0,42900.0,41990.0,39877.0,41939.0,39984.0,31000.0,42974.0,43465.0,38986.0,32998.0,45991.0,38595.0,32900.0,29999.0,71356.0,56633.0,47307.0,61988.0,32995.0,39459.0,38741.0,31997.0,30990.0,71995.0,41874.0,119255.0,43781.0,66164.0,77933.0,49900.0,29862.0,54800.0,31654.0,50500.0,26615.0,93100.0,91358.0,29999.0,53999.0,27990.0,70497.0,51997.0,30998.0,37844.0,47990.0,40441.0,44998.0,44995.0,62500.0,206885.0,25900.0,35998.0,50997.0,46997.0,73777.0,30978.0,56777.0,36595.0,43712.0,47575.0,69111.0,43803.0,55988.0,112999.0,25998.0,40877.0,29984.0,54995.0,117999.0,25991.0,68523.0,31383.0,70994.0,35988.0,55265.0,47988.0,30439.0,26650.0,29853.0,39900.0,39600.0,91999.0,39983.0,29990.0,39891.0,41600.0,51990.0,218554.0,32294.0,30991.0,79900.0,62988.0,62988.0,33495.0,41888.0,43820.0,217994.0,29795.0,54999.0,46809.0,39870.0,46999.0,37995.0,36395.0,41990.0,78500.0,49722.0,35993.0,31789.0,32855.0,29933.0,65555.0,47870.0,108670.0,58900.0,33639.0,71360.0,39890.0,39500.0,57835.0,51999.0,34994.0,29995.0,44611.0,44004.0,40364.0,30304.0,55992.0,24888.0,52900.0,43892.0,48999.0,52995.0,31900.0,32500.0,48794.0,41855.0,92925.0,56499.0,25594.0,33977.0,47755.0,28997.0,54777.0,162900.0,139891.0,45988.0,25998.0,45688.0,62545.0,50393.0,45493.0,45450.0,28892.0,60950.0,44029.0,30998.0,29997.0,43590.0,47750.0,25790.0,41500.0,45991.0,36988.0,51933.0,44900.0,49795.0,63989.0,40984.0,41994.0,27496.0,28999.0,53896.0,73993.0,43995.0,63995.0,66983.0,49990.0,36980.0,42990.0,51588.0,29994.0,43592.0,64500.0,45500.0,44900.0,96991.0,71500.0,46777.0,49983.0,29690.0,41880.0,48000.0,104995.0,46822.0,73922.0,63998.0,26690.0,73887.0,35494.0,81225.0,80893.0,29991.0,41221.0,24999.0,45997.0,52983.0,39699.0,38961.0,62988.0,31500.0,37350.0,64493.0,32190.0,74893.0,59933.0,40495.0,82444.0,44995.0,33865.0,37988.0,51989.0,61693.0,114500.0,43848.0,64498.0,41492.0,49988.0,26999.0,39974.0,45798.0,35900.0,30489.0,23980.0,44555.0,94898.0,39885.0,49500.0,45440.0,39493.0,65999.0,44990.0,34958.0,31563.0,163981.0,32882.0,31000.0,47000.0,24991.0,45777.0,63995.0,33756.0,45624.0,56998.0,41900.0,63798.0,116963.0,33392.0,34111.0,38368.0,43890.0,59900.0,35696.0,43400.0,36882.0,35998.0,48488.0,66991.0,198888.0,43997.0,48885.0,60888.0,53225.0,39999.0,31756.0,45898.0,130000.0,50426.0,33493.0,33989.0,39562.0,65973.0,90972.0,58999.0,52777.0,58903.0,35248.0,57555.0,44876.0,63500.0,41400.0,55490.0,20991.0,53777.0,20647.0,32928.0,32282.0,90971.0,30840.0,38880.0,47894.0,31195.0,39997.0,69986.0,56895.0,46273.0,71995.0,55777.0,37442.0,35987.0,41555.0,60880.0,42900.0,45990.0,27890.0,30993.0,70977.0,26979.0,38562.0,40650.0,38300.0,67991.0,41986.0,44000.0,53771.0,32900.0,37989.0,49998.0,43498.0,31488.0,43800.0,74320.0,47050.0,32892.0,27998.0,42495.0,53505.0,49184.0,67393.0,54596.0,61577.0,36991.0,76990.0,39661.0,40490.0,41451.0,37997.0,44480.0,76994.0,29795.0,31750.0,178000.0,44580.0,32495.0,41780.0,38523.0,52777.0,41507.0,39988.0,37990.0,56954.0,166230.0,50992.0,36900.0,69801.0,45495.0,17998.0,29946.0,31933.0,30995.0,47598.0,20754.0,32596.0,66500.0,99998.0,41444.0,42991.0,72563.0,49990.0,52996.0,30500.0,45651.0,32962.0,25894.0,33989.0,28998.0,39692.0,30986.0,40981.0,43777.0,99884.0,42250.0,54894.0,66777.0,45393.0,117981.0,97973.0,33988.0,64777.0,40594.0,52990.0,28818.0,32818.0,35500.0,31500.0,32000.0,29999.0,37988.0,149495.0,26995.0,72999.0,27999.0,36958.0,72990.0,33988.0,33390.0,36393.0,25736.0,40490.0,30880.0,41000.0,34656.0,61991.0,45990.0,48043.0,51045.0,39672.0,47500.0,31998.0,47840.0,53991.0,34409.0,135640.0,53342.0,46998.0,57315.0,127598.0,78585.0,72895.0,59000.0,89984.0,45950.0,32725.0,39991.0,51505.0,38798.0,49695.0,228323.0,43149.0,54998.0,67750.0,65893.0,49888.0,55998.0,38779.0,147900.0,30383.0,55791.0,31145.0,43995.0,41800.0,31990.0,42999.0,48253.0,43884.0,52887.0,32988.0,60575.0,99999.0,75982.0,40777.0,112999.0,36854.0,115991.0,48125.0,40987.0,26988.0,33534.0,27933.0,39490.0,45888.0,30000.0,82900.0,53555.0,40499.0,68777.0,104992.0,56705.0,33960.0,30595.0,32933.0,61844.0,36784.0,34900.0,44000.0,31707.0,42998.0,31750.0,84000.0,53299.0,40777.0,29933.0,33555.0,52905.0,34888.0,55901.0,44000.0,49480.0,40385.0,27996.0,73478.0,33498.0,38998.0,37918.0,32544.0,41863.0,36990.0,46545.0,39997.0,40982.0,37916.0,61291.0,34444.0,150299.0,52989.0,38000.0,63750.0,45822.0,25200.0,50997.0,49989.0,34991.0,46900.0,29584.0,34895.0,19677.0,163998.0,38794.0,31998.0,88933.0,45977.0,30490.0,52995.0,38780.0,47995.0,65991.0,94000.0,65480.0,31379.0,46500.0,33933.0,34890.0,71998.0,116991.0,43900.0,60000.0,46900.0,89585.0,173998.0,40525.0,29950.0,60805.0,37999.0,39390.0,35780.0,39862.0,54991.0,29894.0,43993.0,72999.0,30834.0,31998.0,209991.0,43893.0,44965.0,59993.0,49741.0,52900.0,48499.0,48890.0,48890.0,29524.0,32998.0,37840.0,40000.0,47588.0,33870.0,37989.0,36992.0,59995.0,51994.0,47777.0,48989.0,49987.0,72661.0,37813.0,79990.0,34988.0,49900.0,72374.0,28860.0,29998.0,44834.0,70065.0,97000.0,75000.0,39965.0,56299.0,75900.0,41996.0,54500.0,49680.0,25499.0,57140.0,45484.0,61930.0,51565.0,73994.0,33999.0,30799.0,33892.0,55000.0,41999.0,56795.0,29995.0,39879.0,172500.0,64107.0,40000.0,38865.0,51900.0,27685.0,35933.0,72991.0,36994.0,57499.0,33990.0,54888.0,67755.0,65936.0,32633.0,48777.0,24997.0,34250.0,77506.0,50777.0,77850.0,34449.0,97892.0,43777.0,61577.0,67989.0,46225.0,48558.0,28500.0,50400.0,44987.0,60597.0,46900.0,49988.0,53900.0,51998.0,48777.0,68999.0,75994.0,54900.0,34887.0,30945.0,39597.0,44984.0,50778.0,45777.0,32544.0,29893.0,68290.0,31950.0,54998.0,85894.0,29889.0,41291.0,41291.0,28550.0,49581.0,91988.0,25976.0,31000.0,27999.0,29599.0,30497.0,36750.0,39991.0,73994.0,45990.0,41558.0,193901.0,89989.0,54498.0,47781.0,31995.0,30740.0,33850.0,88799.0,40558.0,31489.0,20995.0,30799.0,56995.0,39900.0,56999.0,45777.0,32000.0,46900.0,27500.0,36792.0,39998.0,36382.0,48157.0,28351.0,40500.0,46777.0,69880.0,33950.0,152986.0,31195.0,23168.0,40999.0,229918.0,72545.0,97575.0,64215.0,63994.0,87599.0,39200.0,57499.0,63995.0,56695.0,38760.0,94989.0,74989.0,44933.0,29500.0,34995.0,57153.0,51763.0,30900.0,20562.0,57555.0,89998.0,45877.0,117495.0,31992.0,35416.0,85999.0,65886.0,75577.0,53490.0,89599.0,32890.0,53983.0,46035.0],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Review Count"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Price"}},"legend":{"tracegroupgap":0},"title":{"text":"Scatter plot of Review Count vs Price"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('0143ce7a-1c63-46e9-9338-8b0167567635');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div><div class="output text_html"><html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="1954d615-2ffc-41ee-aacd-c903a71df0fe" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("1954d615-2ffc-41ee-aacd-c903a71df0fe")) {                    Plotly.newPlot(                        "1954d615-2ffc-41ee-aacd-c903a71df0fe",                        [{"hovertemplate":"Year=%{x}\u003cbr\u003ePrice=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[2021,2022,2022,2020,2021,2022,2022,2023,2021,2023,2021,2023,2019,2023,2020,2023,2023,2020,2021,2017,2021,2020,2020,2023,2023,2022,2023,2019,2024,2019,2024,2021,2023,2022,2023,2020,2019,2020,2022,2021,2019,2022,2020,2021,2019,2020,2019,2023,2024,2023,2021,2021,2020,2020,2020,2021,2023,2023,2023,2016,2023,2021,2017,2020,2023,2023,2021,2023,2017,2021,2020,2022,2020,2019,2024,2019,2023,2016,2018,2023,2023,2021,2020,2020,2022,2016,2024,2020,2023,2021,2023,2023,2020,2023,2021,2022,2023,2023,2023,2023,2019,2020,2023,2024,2023,2020,2023,2023,2019,2024,2022,2021,2023,2018,2015,2023,2020,2019,2023,2020,2016,2023,2022,2020,2023,2022,2024,2021,2023,2016,2023,2020,2021,2021,2023,2023,2023,2023,2023,2023,2023,2023,2023,2023,2018,2019,2023,2023,2022,2024,2021,2020,2020,2023,2020,2023,2023,2020,2023,2022,2023,2021,2021,2023,2022,2022,2021,2023,2023,2023,2023,2024,2019,2024,2021,2024,2020,2021,2023,2023,2021,2023,2023,2021,2021,2018,2023,2020,2021,2023,2019,2024,2020,2021,2021,2023,2023,2022,2021,2024,2019,2024,2021,2023,2022,2022,2020,2019,2023,2020,2022,2022,2023,2022,2020,2022,2019,2022,2020,2021,2021,2020,2023,2023,2020,2021,2024,2023,2021,2024,2023,2024,2023,2023,2021,2023,2023,2018,2023,2023,2022,2023,2023,2022,2024,2019,2020,2022,2023,2020,2021,2022,2022,2023,2019,2021,2019,2023,2017,2020,2022,2021,2023,2020,2023,2021,2018,2021,2024,2023,2018,2023,2023,2023,2019,2023,2019,2023,2023,2019,2021,2021,2021,2021,2020,2023,2023,2022,2023,2022,2020,2021,2019,2024,2023,2022,2023,2022,2020,2021,2018,2022,2021,2023,2021,2023,2023,2021,2023,2020,2023,2023,2024,2023,2020,2020,2023,2020,2021,2022,2022,2019,2023,2023,2021,2023,2021,2021,2019,2022,2023,2022,2022,2023,2021,2019,2023,2023,2023,2021,2024,2021,2022,2020,2021,2023,2023,2023,2016,2023,2023,2017,2024,2020,2021,2023,2023,2023,2021,2024,2023,2019,2019,2023,2020,2021,2023,2024,2020,2019,2019,2023,2021,2022,2021,2020,2021,2023,2021,2023,2023,2020,2020,2024,2021,2023,2022,2023,2022,2022,2021,2018,2023,2023,2024,2022,2022,2020,2023,2023,2023,2016,2019,2020,2024,2023,2020,2021,2023,2019,2023,2020,2021,2020,2020,2024,2023,2024,2023,2019,2023,2019,2022,2021,2019,2024,2020,2019,2021,2023,2023,2023,2023,2022,2022,2018,2024,2023,2022,2023,2019,2023,2021,2021,2020,2023,2022,2021,2024,2024,2021,2021,2022,2024,2023,2023,2023,2021,2022,2022,2019,2024,2023,2021,2023,2023,2020,2023,2020,2024,2022,2023,2021,2023,2019,2020,2023,2024,2023,2020,2019,2023,2023,2022,2023,2022,2023,2023,2019,2023,2017,2022,2021,2021,2020,2024,2019,2023,2020,2015,2023,2022,2021,2019,2023,2020,2020,2021,2023,2023,2023,2022,2020,2023,2022,2023,2023,2021,2020,2022,2023,2023,2020,2020,2022,2023,2017,2023,2023,2023,2020,2021,2021,2020,2023,2023,2023,2023,2023,2023,2021,2023,2023,2023,2021,2021,2023,2023,2020,2020,2020,2021,2020,2021,2023,2022,2023,2023,2021,2023,2021,2023,2020,2014,2021,2023,2021,2023,2023,2021,2019,2020,2023,2023,2023,2024,2022,2022,2023,2023,2023,2023,2023,2023,2023,2021,2022,2023,2023,2024,2023,2022,2023,2023,2016,2020,2023,2023,2023,2022,2020,2021,2018,2023,2023,2024,2022,2021,2023,2023,2020,2023,2023,2023,2023,2023,2023,2022,2023,2022,2024,2020,2021,2022,2023,2021,2021,2020,2023,2023,2023,2023,2024,2023,2020,2023,2023,2023,2018,2020,2023,2023,2022,2023,2023,2023,2023,2023,2022,2023,2023,2016,2023,2020,2021,2023,2020,2016,2023,2023,2024,2023,2023,2023,2018,2023,2023,2023,2023,2018,2023,2022,2023,2023,2023,2021,2022,2023,2021,2023,2019,2023,2023,2023,2023,2019,2023,2023,2021,2023,2023,2023,2023,2020,2020,2021,2022,2020,2023,2021,2022,2023,2023,2024,2020,2021,2020,2020,2023,2019,2023,2021,2021,2023,2020,2024,2023,2020,2023,2024,2020,2023,2020,2023,2019,2023,2023,2021,2021,2023,2023,2021,2023,2023,2024,2023,2021,2023,2021,2023,2023,2023,2023,2020,2016,2022,2020,2020,2024,2022,2020,2024,2023,2023,2023,2021,2022,2023,2023,2023,2024,2023,2021,2022,2021,2023,2022,2023,2021,2020,2020,2021,2020,2020,2023,2023,2023,2020,2021,2020,2019,2024,2023,2023,2023,2021,2024,2022,2020,2023,2021,2024,2023,2021,2023,2023,2021,2019,2023,2021,2021,2023,2023,2023,2023,2023,2023,2018,2019,2022,2023,2024,2022,2023,2021,2022,2023,2023,2023,2023,2023,2024,2017,2020,2023,2023,2023,2020,2019,2023,2016,2023,2020,2021,2023,2024,2020,2023,2023,2023,2020,2023,2022,2020,2022,2023,2021,2023,2021,2022,2023,2019,2023,2024,2023,2022,2019,2024,2024,2021,2021,2023,2021,2023,2023,2023,2022,2023,2016,2020,2022,2023,2023,2023,2021,2020,2021,2022,2023,2021,2020,2021,2021,2023,2021,2023,2017,2022,2023,2021,2021,2023,2023,2021,2022,2020,2023,2023,2023,2023,2022,2021,2023,2020,2023,2023,2019,2022,2023,2021,2021,2023,2023,2023,2023,2020,2021,2023,2021,2020,2023,2021,2023,2023,2022,2023,2021,2023,2023,2021,2023,2021,2023,2022,2023,2023,2020,2023,2022,2022,2020,2022,2023,2021,2023,2022,2024,2021,2023,2023,2020,2021,2023,2021,2024,2021,2022,2023,2024,2022,2023,2021,2019,2023,2023,2020,2021,2021,2021,2023,2019,2024,2023,2022,2021,2018,2020,2020,2023,2020,2023,2020,2022,2020,2021,2024,2022,2023,2021,2022,2023,2024,2022,2021,2022,2022,2023,2020,2023,2023,2019,2021,2021,2023,2023,2021,2024,2021,2020,2020,2023,2023,2020,2023,2021,2021,2022,2023,2020,2020,2023,2023,2021,2021,2021,2021,2024,2024,2021,2020,2023,2023,2019,2020,2020,2020,2021,2023,2023,2021,2021,2024,2022,2023,2021,2020,2023,2023,2024,2021,2023,2021,2023,2019,2021,2024,2021,2024,2023,2023,2021,2021,2021,2021,2020,2021,2023,2021,2020,2023,2023,2023,2023,2023,2020,2021,2022,2023,2023,2021,2021,2023,2022,2023,2022,2021,2023,2023,2023,2021,2023,2021,2019,2022,2022,2023,2023,2020,2019,2022,2023,2021,2023,2021,2020,2023,2024,2023,2021,2022,2023,2023,2021,2020,2023,2023,2023,2024,2023,2023,2023,2023,2023,2021,2023,2021,2024,2020,2023,2021,2020,2023,2020,2023,2023,2022,2023,2023,2020,2019,2022,2021,2015,2023,2023,2022,2023,2023,2023,2023,2023,2023,2023,2023,2024,2023,2023,2023,2022,2024,2023,2023,2023,2021,2021,2021,2021,2022,2020,2023,2024,2021,2023,2024,2020,2021,2021,2020,2021,2021,2024,2019,2023,2020,2023,2023,2023,2023,2023,2020,2024,2023,2022,2023,2023,2023,2023,2020,2022,2023,2023,2023,2023,2023,2019,2023,2023,2023,2023,2023,2020,2020,2020,2023,2020,2018,2023,2023,2024,2023,2021,2022,2023,2023,2021,2023,2021,2023,2023,2020,2021,2020,2023,2021,2023,2023,2024,2021,2019,2023,2021,2020,2021,2021,2020,2023,2020,2021,2020,2020,2023,2023,2020,2023,2021,2023,2023,2020,2023,2021,2019,2024,2023,2023,2023,2020,2021,2022,2023,2020,2023,2023,2020,2020,2023,2024,2023,2023,2021,2021,2023,2023,2024,2022,2019,2021,2023,2023,2023,2023,2023,2021,2020,2023,2023,2024,2023,2021,2020,2024,2023,2023,2024,2023,2021,2021,2023,2022,2024,2024,2023,2020,2019,2023,2023,2024,2022,2023,2019,2021,2023,2023,2019,2021,2021,2020,2023,2023,2021,2021,2022,2023,2024,2020,2022,2021,2021,2022,2021,2022,2021,2023,2021,2020,2023,2021,2024,2023,2021,2024,2021,2023,2021,2022,2023,2020,2023,2019,2022,2022,2022,2024,2023,2023,2020,2021,2018,2021,2021,2017,2023,2019,2021,2023,2023,2020,2022,2022,2024,2019,2022,2019,2021,2021,2023,2018,2023,2022,2020,2022,2023,2023,2019,2020,2023,2023,2023,2024,2021,2022,2020,2023,2023,2023,2023,2018,2021,2021,2021,2021,2021,2023,2022,2017,2023,2024,2023,2020,2020,2021,2022,2022,2023,2023,2023,2022,2023,2023,2023,2020,2020,2023,2021,2022,2020,2023,2023,2021,2023,2023,2023,2021,2024,2023,2023,2020,2020,2020,2018,2020,2022,2023,2023,2023,2021,2021,2023,2021,2021,2020,2024,2021,2021,2021,2021,2023,2021,2023,2023,2023,2021,2021,2022,2021,2022,2022,2020,2020,2022,2020,2022,2020,2021,2022,2022,2023,2023,2021,2023,2023,2020,2024,2021,2024,2023,2022,2023,2021,2021,2021,2022,2023,2024,2023,2024,2023,2021,2020,2023,2024,2021,2023,2023,2021,2023,2021,2021,2020,2020,2023,2022,2023,2019,2023,2021,2023,2021,2022,2024,2023,2023,2020,2021,2023,2020,2023,2018,2021,2022,2022,2022,2023,2021,2021,2021,2024,2024,2024,2023,2020,2021,2020,2020,2023,2023,2021,2022,2021,2023,2021,2023,2019,2023,2022,2021,2022,2022,2021,2023,2022,2018,2023,2020,2021,2019,2020,2020,2019,2021,2020,2022,2023,2023,2020,2022,2021,2023,2023,2023,2024,2023,2024,2024,2021,2018,2023,2020,2022,2023,2024,2021,2022,2023,2020,2021,2021,2020,2023,2024,2023,2024,2023,2021,2023,2021,2020,2021,2023,2021,2021,2023,2023,2023,2023,2023,2024,2024,2018,2023,2023,2020,2020,2023,2022,2020,2023,2024,2023,2020,2024,2023,2017,2023,2022,2021,2020,2023,2021,2021,2021,2023,2023,2023,2021,2023,2024,2024,2020,2023,2023,2023,2023,2023,2023,2023,2023,2022,2022,2021,2021,2023,2021,2022,2023,2020,2020,2023,2023,2021,2021,2020,2021,2020,2021,2024,2021,2024,2021,2022,2023,2021,2021,2023,2023,2019,2023,2022,2021,2021,2020,2024,2020,2022,2020,2024,2022,2020,2024,2020,2021,2021,2021,2020,2023,2023,2023,2020,2023,2023,2018,2022,2024,2020,2024,2020,2023,2021,2023,2020,2022,2023,2022,2022,2016,2023,2020,2022,2022,2020,2024,2021,2022,2021,2022,2021,2021,2020,2021,2023,2023,2024,2021,2021,2023,2023,2023,2023,2021,2021,2021,2024,2024,2021,2023,2023,2023,2020,2024,2023,2019,2021,2021,2021,2022,2023,2020,2021,2020,2020,2021,2023,2020,2019,2021,2016,2024,2023,2023,2024,2023,2021,2021,2023,2023,2023,2021,2023,2020,2023,2021,2021,2023,2020,2020,2021,2021,2024,2021,2020,2020,2024,2021,2023,2023,2022,2023,2018,2023,2024,2021,2021,2021,2021,2023,2020,2021,2019,2023,2021,2018,2022,2023,2023,2022,2023,2023,2023,2023,2023,2021,2020,2023,2024,2023,2023,2023,2023,2020,2022,2023,2020,2023,2024,2020,2023,2022,2024,2024,2023,2020,2021,2020,2021,2023,2021,2022,2020,2023,2021,2023,2021,2020,2023,2017,2022,2023,2019,2023,2024,2021,2023,2024,2021,2021,2022,2020,2023,2023,2023,2023,2023,2021,2022,2024,2023,2023,2021,2021,2023,2023,2020,2021,2019,2023,2022,2023,2023,2021,2021,2022,2023,2021,2021,2020,2021,2020,2023,2020,2023,2023,2021,2023,2023,2023,2024,2021,2021,2021,2021,2023,2021,2021,2023,2023,2017,2024,2021,2021,2019,2021,2021,2023,2021,2021,2023,2023,2023,2021,2020,2021,2024,2024,2021,2023,2023,2021,2024,2023,2024,2023,2024,2017,2023,2018,2020,2020,2024,2021,2020,2022,2021,2023,2024,2023,2023,2020,2023,2023,2021,2020,2023,2023,2022,2020,2020,2024,2020,2020,2021,2023,2023,2020,2023,2023,2020,2021,2021,2024,2021,2023,2023,2024,2021,2021,2023,2021,2021,2024,2022,2021,2022,2024,2023,2022,2021,2020,2023,2024,2021,2020,2021,2023,2021,2020,2022,2024,2023,2023,2023,2023,2020,2023,2023,2023,2020,2016,2020,2021,2020,2020,2019,2021,2022,2022,2023,2021,2023,2021,2023,2021,2020,2021,2019,2021,2021,2022,2021,2020,2023,2023,2021,2023,2024,2023,2024,2023,2021,2024,2022,2023,2020,2021,2022,2021,2020,2021,2021,2024,2020,2023,2019,2021,2022,2020,2021,2023,2019,2020,2020,2023,2021,2023,2022,2021,2023,2022,2021,2021,2023,2024,2021,2023,2023,2023,2023,2024,2024,2021,2022,2024,2023,2021,2022,2021,2023,2024,2023,2023,2021,2023,2022,2022,2022,2023,2020,2021,2021,2021,2020,2023,2021,2023,2023,2023,2020,2021,2022,2023,2021,2023,2023,2022,2023,2023,2021,2020,2021,2020,2022,2024,2020,2021,2024,2019,2023,2023,2021,2019,2020,2021,2021,2018,2021,2023,2021,2020,2020,2024,2023,2023,2021,2021,2023,2021,2022,2023,2023,2021,2019,2023,2021,2023,2022,2021,2021,2021,2021,2021,2021,2022,2021,2023,2019,2023,2022,2021,2021,2019,2021,2023,2021,2023,2021,2022,2017,2023,2023,2021,2021,2024,2021,2022,2020,2020,2022,2024,2023,2020,2022,2021,2020,2020,2020,2023,2021,2023,2023,2023,2023,2020,2022,2023,2023,2020,2022,2022,2021,2021,2021,2021,2021,2022,2023,2023,2020,2021,2023,2023,2021,2021,2020,2020,2020,2021,2020,2020,2023,2023,2022,2023,2023,2023,2022,2021,2020,2023,2021,2024,2023,2021,2020,2023,2023,2023,2023,2022,2019,2023,2023,2023,2023,2021,2023,2020,2021,2023,2023,2021,2021,2021,2023,2020,2023,2021,2023,2020,2024,2023,2023,2023,2020,2022,2024,2022,2023,2021,2023,2023,2021,2021,2020,2020,2020,2024,2023,2023,2022,2022,2023,2021,2021,2023,2023,2021,2023,2022,2023,2023,2023,2021,2021,2023,2020,2024,2023,2021,2021,2023,2023,2023,2023,2020,2021,2024,2021,2023,2019,2022,2021,2021,2021,2019,2020,2020,2021,2020,2021,2021,2021,2023,2023,2023,2021,2022,2023,2023,2023,2021,2022,2021,2024,2023,2020,2016,2021,2022,2022,2023,2024,2020,2021,2020,2021,2023,2023,2023,2019,2019,2023,2022,2021,2023,2021,2018,2021,2023,2024,2023,2024,2023,2022,2023,2023,2024,2020,2023,2024,2024,2023,2020,2021,2023,2023,2022,2018,2023,2023,2024,2023,2020,2021,2023,2022,2020,2021,2022,2021,2023,2023],"xaxis":"x","y":[30900.0,139999.0,132999.0,58587.0,95990.0,74999.0,130999.0,55995.0,71999.0,62975.0,82895.0,50226.0,22495.0,63998.0,65024.0,69771.0,51995.0,57877.0,53977.0,22498.0,28784.0,67800.0,42595.0,74788.0,58770.0,53690.0,46900.0,38996.0,69306.0,39442.0,112749.0,57900.0,49995.0,153722.0,43855.0,50563.0,74887.0,59413.0,68983.0,46300.0,62421.0,198000.0,36900.0,62292.0,45991.0,50299.0,98984.0,65992.0,77700.0,65514.0,54660.0,143554.0,67800.0,54000.0,57463.0,84997.0,74377.0,57900.0,49913.0,21082.0,161775.0,79711.0,48500.0,50900.0,39950.0,54018.0,38963.0,107552.0,50595.0,29999.0,47562.0,50888.0,95981.0,98555.0,57445.0,94995.0,43105.0,55998.0,41000.0,39237.0,58798.0,30880.0,26499.0,33497.0,177595.0,69998.0,120997.0,142000.0,48000.0,73998.0,65444.0,69997.0,157500.0,45705.0,114750.0,41948.0,99805.0,39950.0,67910.0,60312.0,58997.0,35824.0,55995.0,70991.0,86690.0,50900.0,49400.0,60994.0,53922.0,67585.0,56940.0,62704.0,209995.0,55000.0,41690.0,44744.0,65999.0,49983.0,87390.0,111326.0,68500.0,67515.0,33999.0,31293.0,69997.0,99999.0,78900.0,36880.0,42499.0,20999.0,74988.0,28000.0,26540.0,94984.0,86975.0,77000.0,53900.0,112997.0,46888.0,97397.0,49777.0,41200.0,43777.0,68398.0,73990.0,49500.0,48846.0,45900.0,93888.0,47222.0,33951.0,59900.0,37650.0,59700.0,27412.0,57453.0,44196.0,45551.0,39864.0,41929.0,44777.0,30400.0,31336.0,107999.0,89999.0,98444.0,99795.0,105999.0,43900.0,47222.0,78899.0,44222.0,112498.0,46888.0,67983.0,55089.0,68274.0,76777.0,46777.0,199865.0,178899.0,43888.0,92499.0,48998.0,61991.0,33797.0,125997.0,149995.0,30880.0,46900.0,93882.0,53445.0,50754.0,108337.0,38740.0,38997.0,47721.0,34600.0,29397.0,173880.0,48995.0,50888.0,164882.0,157771.0,58702.0,35595.0,60146.0,79987.0,46888.0,60977.0,36295.0,85987.0,158880.0,65987.0,30502.0,35887.0,49995.0,35000.0,67500.0,45955.0,79894.0,64911.0,54750.0,108946.0,58998.0,139994.0,52888.0,41069.0,79998.0,101998.0,119899.0,78900.0,87902.0,39955.0,64992.0,77900.0,135990.0,20657.0,37870.0,69795.0,84690.0,61992.0,73499.0,47399.0,130994.0,46519.0,29598.0,119510.0,35878.0,46999.0,50477.0,142106.0,36295.0,52988.0,24862.0,54495.0,86881.0,46999.0,33316.0,32390.0,53436.0,43497.0,98995.0,28975.0,45999.0,79876.0,81921.0,85175.0,71998.0,48998.0,80991.0,39843.0,77802.0,57900.0,74995.0,52000.0,45712.0,106777.0,57642.0,49999.0,44777.0,49881.0,126995.0,30955.0,28990.0,104900.0,64443.0,34988.0,79997.0,58997.0,41994.0,127500.0,27620.0,44990.0,164900.0,177595.0,223414.0,72555.0,32998.0,73890.0,36991.0,77033.0,85500.0,89888.0,28988.0,51897.0,55900.0,89995.0,61805.0,98999.0,164994.0,49995.0,82980.0,81994.0,125690.0,50563.0,50905.0,98509.0,75850.0,49445.0,63987.0,45375.0,54777.0,61997.0,27477.0,47995.0,110950.0,30323.0,45950.0,41850.0,39999.0,144870.0,80594.0,109554.0,66413.0,99997.0,65595.0,99998.0,103044.0,50994.0,75998.0,123881.0,50194.0,45142.0,62000.0,63777.0,57499.0,103500.0,18578.0,96713.0,49988.0,35999.0,73800.0,89597.0,72490.0,42640.0,48932.0,65595.0,159990.0,56998.0,60495.0,34551.0,48350.0,41999.0,43399.0,86925.0,52744.0,89900.0,57007.0,28484.0,24999.0,49988.0,128887.0,61998.0,46995.0,43495.0,47647.0,46777.0,30400.0,85292.0,69287.0,50997.0,78999.0,72293.0,142825.0,53988.0,86249.0,56004.0,45982.0,29598.0,37995.0,28954.0,44999.0,80994.0,72865.0,36295.0,35897.0,44210.0,40000.0,221994.0,53895.0,86995.0,57988.0,41280.0,49995.0,40999.0,38200.0,23949.0,98140.0,27995.0,56997.0,47875.0,99777.0,39497.0,37490.0,76589.0,51500.0,67466.0,64876.0,33397.0,47893.0,87987.0,44898.0,36131.0,36991.0,64815.0,51490.0,47999.0,32300.0,85000.0,54987.0,58375.0,46099.0,78000.0,39999.0,150000.0,62988.0,63550.0,30904.0,46912.0,54998.0,157771.0,44997.0,38482.0,164900.0,74994.0,54981.0,169661.0,62988.0,73495.0,39100.0,38689.0,76286.0,64955.0,50988.0,203999.0,38288.0,179120.0,56994.0,64995.0,24688.0,65507.0,47882.0,80995.0,58883.0,45998.0,36125.0,42997.0,49495.0,49995.0,219995.0,44150.0,129995.0,46288.0,69998.0,33498.0,89990.0,58991.0,124988.0,39982.0,22913.0,109997.0,53983.0,98444.0,52495.0,55699.0,80654.0,55990.0,38492.0,44777.0,74999.0,40493.0,129618.0,53791.0,32339.0,63208.0,21900.0,59410.0,24991.0,20437.0,46980.0,41584.0,40970.0,30791.0,45994.0,26840.0,50991.0,27500.0,38497.0,43900.0,47777.0,49988.0,37539.0,44995.0,49850.0,43888.0,43860.0,61000.0,40016.0,52188.0,55997.0,141215.0,32000.0,34495.0,69992.0,109554.0,8999.0,44299.0,46417.0,44222.0,28995.0,28999.0,54981.0,26667.0,76987.0,40699.0,49884.0,45553.0,81000.0,43888.0,30966.0,48220.0,74442.0,70500.0,37037.0,69612.0,109554.0,42900.0,32298.0,29211.0,30500.0,29991.0,26885.0,69990.0,58003.0,77306.0,48982.0,44933.0,41497.0,40000.0,48950.0,41574.0,29000.0,21844.0,44185.0,134495.0,30985.0,58003.0,68990.0,27495.0,69892.0,32984.0,62777.0,42991.0,56111.0,61285.0,39987.0,41988.0,53777.0,42981.0,157991.0,74519.0,45994.0,72777.0,38995.0,58990.0,41990.0,85777.0,58883.0,118988.0,55994.378500823725,53900.0,43299.0,44421.0,51990.0,26995.0,45485.0,65595.0,40888.0,39995.0,37872.0,29495.0,41990.0,53275.0,104357.0,48888.0,80239.0,32795.0,38866.0,45553.0,48487.0,44699.0,69459.0,39577.0,49777.0,52627.0,61880.0,52500.0,36998.0,57981.0,50515.0,31695.0,85991.0,38000.0,77586.0,56900.0,30900.0,30877.0,66888.0,48000.0,57995.0,42989.0,55123.0,37893.0,69500.0,45500.0,44500.0,55989.0,57495.0,31777.0,40888.0,45900.0,36900.0,49997.0,44998.0,90000.0,49988.0,81494.0,41988.0,56001.0,45900.0,77898.0,37983.0,65466.0,97888.0,47950.0,38746.0,19498.0,72992.0,55650.0,52990.0,99495.0,39495.0,38900.0,41874.0,64777.0,39975.0,68910.0,50777.0,22992.0,58441.0,84614.0,44810.0,45988.0,43900.0,37978.0,43991.0,62706.0,29933.0,164900.0,32000.0,55984.0,77400.0,55777.0,159565.0,52751.0,42000.0,37994.0,33967.0,43900.0,46789.0,55184.0,53899.0,49990.0,36900.0,30752.0,95879.0,33695.0,49777.0,45968.0,53489.0,45900.0,55651.0,54495.0,55803.0,32989.0,29816.0,41998.0,58199.0,40995.0,90988.0,96838.0,41488.0,62981.0,30999.0,90000.0,42777.0,44162.0,46401.0,55117.0,59000.0,42300.0,28481.0,101888.0,43895.0,46900.0,46411.0,36633.0,76900.0,56777.0,219805.0,32337.0,42989.0,66888.0,55101.0,110084.0,30653.0,47495.0,30775.0,46999.0,109990.0,44788.0,66205.0,28395.0,16998.0,40670.0,27904.0,44963.0,55904.0,50988.0,54900.0,67800.0,47888.0,55936.0,221994.0,30423.0,43981.0,42495.0,69999.0,44997.0,229654.0,42787.0,46987.0,39490.0,33600.0,47605.0,28725.0,44990.0,28995.0,33395.0,40952.0,48988.0,43991.0,25999.0,84800.0,52888.0,41997.0,56156.0,46685.0,50900.0,46206.0,69989.0,154200.0,38954.0,67399.0,34984.0,50257.0,47346.0,56900.0,55444.0,73870.0,78900.0,42888.0,57980.0,58883.0,38350.0,42900.0,35918.0,65712.0,81891.0,43888.0,73777.0,49995.0,78933.0,45305.0,41500.0,48474.0,31991.0,24400.0,54900.0,78499.0,129999.0,37494.0,53658.0,55800.0,34537.0,57441.0,45988.0,43498.0,112999.0,44000.0,52000.0,25894.0,26995.0,47203.0,83982.0,42681.0,34726.0,49491.0,46444.0,45677.0,49000.0,26497.0,38599.0,42574.0,58999.0,27500.0,47305.0,40999.0,47882.0,30998.0,47308.0,48306.0,69800.0,32140.0,57899.0,35922.0,47509.0,30991.0,71888.0,45998.0,34377.0,64502.0,125880.0,68500.0,54811.0,44499.0,52660.0,50977.0,28795.0,39154.0,139250.0,39491.0,36988.0,83824.0,42991.0,48994.0,42487.0,33877.0,28475.0,41940.0,47893.0,49700.0,55835.0,38000.0,29625.0,33937.0,38444.0,62756.0,29250.0,32997.0,67890.0,51000.0,53484.0,35598.0,53148.0,68998.0,94777.0,72390.0,30583.0,45994.0,73345.0,38552.0,90000.0,154900.0,44990.0,54317.0,45888.0,39521.0,76992.0,90988.0,92896.0,57900.0,37823.0,77465.0,83777.0,39894.0,39500.0,67989.0,43992.0,36885.0,42300.0,37899.0,45645.0,77482.0,46991.0,38098.0,39659.0,44851.0,32589.0,52075.0,28995.0,46328.0,39494.0,45935.0,52713.0,34445.0,60833.0,44900.0,40007.0,68113.0,46688.0,63910.0,139977.0,39995.0,81593.0,50881.0,58796.0,36000.0,35595.0,41733.0,159979.0,45281.0,35650.0,45900.0,43988.0,63000.0,37995.0,70490.0,39900.0,42735.0,59546.0,44000.0,39592.0,54999.0,30934.0,38995.0,50284.0,40775.0,71989.0,47502.0,53510.0,37974.0,54480.0,40998.0,37500.0,47791.0,33295.0,50639.0,112998.0,55991.0,49888.0,53910.0,43881.0,39988.0,28000.0,29052.0,75890.0,63491.0,27995.0,40900.0,27250.0,40495.0,31416.0,35475.0,59750.0,38995.0,41999.0,32230.0,40990.0,42995.0,45000.0,96510.0,23991.0,38999.0,33738.0,38594.0,50555.0,41981.0,51788.0,26199.0,28888.0,31593.0,88977.0,60900.0,43498.0,129991.0,31652.0,31756.0,46998.0,169995.0,47100.0,27990.0,52983.0,33750.0,32998.0,87851.0,42995.0,33991.0,30998.0,39634.0,52892.0,32485.0,35988.0,72666.0,58998.0,54999.0,54994.0,38314.0,34650.0,224891.0,41999.0,37500.0,29996.0,43345.0,29500.0,31933.0,36988.0,38989.0,36390.0,70164.0,52777.0,36852.0,71994.0,43888.0,36900.0,55900.0,49444.0,55777.0,61999.0,58880.0,51469.0,54329.0,23199.0,34500.0,89500.0,144999.0,46690.0,43992.0,71892.0,40684.0,41888.0,40102.0,46315.0,27825.0,43988.0,42700.0,30979.0,34238.0,58988.0,39400.0,47644.0,42344.0,40600.0,41594.0,44888.0,51933.0,40997.0,53686.0,30840.0,50997.0,43000.0,209894.0,60566.0,55998.0,34991.0,50792.0,57774.0,60470.0,47990.0,51555.0,29990.0,37890.0,216999.0,50991.0,53000.0,39997.0,42892.0,31152.0,44990.0,45925.0,32685.0,61997.0,31562.0,41200.0,48400.0,46977.0,50788.0,56739.0,71892.0,77945.0,53965.0,28013.0,26433.0,47044.0,218937.0,62900.0,67897.0,40000.0,41964.0,45693.0,42963.0,60000.0,39485.0,49779.0,34900.0,85999.0,34522.0,47952.0,189888.0,43905.0,48006.0,46995.0,40989.0,48999.0,44941.0,51971.0,76765.0,29526.0,28999.0,35988.0,44932.0,55900.0,39480.0,41999.0,44989.0,57505.0,75999.0,46999.0,47804.0,95997.0,54395.0,40894.0,51995.0,61998.0,53997.0,53717.0,72007.0,39964.0,41495.0,49991.0,47900.0,37496.0,34833.0,137900.0,29275.0,36839.0,219894.0,34977.0,39400.0,69867.0,29960.0,39988.0,49998.0,41498.0,31998.0,34746.0,42994.0,35888.0,32928.0,72550.0,24990.0,50999.0,30735.0,70226.0,51507.0,100804.0,53698.0,45244.0,27695.0,86989.0,46939.0,65991.0,46500.0,64986.0,50481.0,42495.0,30960.0,129351.0,59991.0,41900.0,47983.0,40983.0,46220.0,30995.0,50999.0,55110.0,42200.0,47888.0,43977.0,53974.0,165994.0,34690.0,43998.0,38000.0,21958.0,85777.0,60125.0,84994.0,38795.0,31495.0,77991.0,43481.0,50384.0,36888.0,41600.0,33896.0,103995.0,51999.0,26565.0,30987.0,55990.0,41990.0,32395.0,46446.0,52835.0,52835.0,30972.0,38899.0,67035.0,45844.0,58884.0,79988.0,34998.0,29892.0,49475.0,165997.0,26900.0,41000.0,31988.0,41300.0,48777.0,30995.0,57425.0,48000.0,128388.0,54776.0,48991.0,45994.0,109995.0,30756.0,49777.0,39989.0,52496.0,47300.0,33233.0,36985.0,56991.0,48000.0,44900.0,43777.0,79991.0,35986.0,41900.0,60200.0,66876.0,69777.0,41000.0,38000.0,35900.0,45777.0,55999.0,92857.0,67997.0,77995.0,30888.0,40563.0,61994.0,97892.0,54493.0,42000.0,47988.0,41990.0,46000.0,56997.0,53555.0,46908.0,33795.0,54954.0,73890.0,56444.0,57990.0,51555.0,169999.0,30999.0,44480.0,74999.0,53840.0,61777.0,94001.0,55212.0,39874.0,43440.0,78820.0,119995.0,71988.0,51696.0,52473.0,29527.0,69991.0,44998.0,68204.0,30994.0,38999.0,34410.0,38893.0,43555.0,85883.0,41000.0,29990.0,94983.0,69846.0,45777.0,43987.0,31987.0,77000.0,34989.0,36400.0,32817.0,58000.0,32995.0,52399.0,39988.0,33950.0,55521.0,36742.0,46295.0,51860.0,28496.0,49999.0,42988.0,55414.0,82994.0,41884.0,45998.0,29695.0,41432.0,28999.0,52466.0,47735.0,42986.0,67994.0,41973.0,43988.0,30500.0,47695.0,79799.0,33833.0,31773.0,29883.0,40900.0,63874.0,56481.0,55963.0,41985.0,29997.0,55900.0,38597.0,72765.0,24899.0,53441.0,29999.0,55995.0,38692.0,46398.0,21360.0,79700.0,52894.0,27981.0,33890.0,41990.0,43777.0,25632.0,48000.0,55349.0,41492.0,37616.0,48777.0,31925.0,38494.0,29890.0,43777.0,43983.0,51788.0,41883.0,32999.0,30933.0,45998.0,38995.0,35991.0,36884.0,98482.0,46692.0,24998.0,39800.0,128991.0,39988.0,27598.0,53332.0,51901.0,46098.0,83890.0,68853.0,81004.0,75007.0,34354.0,79991.0,51991.0,198888.0,55888.0,37295.0,43883.0,42588.0,64991.0,33989.0,48988.0,45000.0,43573.0,56894.0,65030.0,49983.0,38991.0,49145.0,46235.0,59990.0,31994.0,32309.0,32000.0,79990.0,42496.0,56775.0,45996.0,43600.0,45997.0,30988.0,45992.0,187987.0,176988.0,29893.0,34683.0,50000.0,31890.0,44800.0,29888.0,36789.0,34995.0,43990.0,42763.0,42775.0,114895.0,25983.0,182975.0,50000.0,44157.0,37700.0,42793.0,26880.0,39562.0,37259.0,36990.0,47998.0,35250.0,34597.0,48888.0,40039.0,41777.0,38761.0,30989.0,53983.0,47840.0,39699.0,48250.0,45699.0,52777.0,55004.0,42916.0,39495.0,32900.0,170882.0,30295.0,45768.0,41990.0,55498.0,98900.0,70990.0,53476.0,30337.0,32204.0,45990.0,72488.0,29894.0,52997.0,50435.0,69963.0,41991.0,28989.0,97999.0,29128.0,48700.0,53637.0,109977.0,43892.0,36866.0,39894.0,31894.0,63486.0,199860.0,77900.0,62988.0,49983.0,54995.0,39997.0,42880.0,40996.0,41100.0,44998.0,21260.0,35500.0,38494.0,36999.0,95984.0,87993.0,47845.0,39981.0,159990.0,60777.0,75445.0,67960.0,73980.0,35678.0,44988.0,28886.0,169999.0,47261.0,44995.0,29999.0,39948.0,36221.0,43995.0,84950.0,45000.0,29946.0,53484.0,69888.0,51989.0,46307.0,47983.0,35233.0,46000.0,42595.0,24586.0,84444.0,165966.0,55997.0,63899.0,47800.0,26999.0,26140.0,25994.0,37900.0,51200.0,41000.0,47983.0,27998.0,41990.0,87770.0,51499.0,40900.0,49983.0,67984.0,40501.0,64484.0,91433.0,29886.0,19990.0,47131.0,27750.0,37417.0,58594.0,61988.0,36851.0,91888.0,48500.0,133448.0,48787.0,42999.0,27583.0,54840.0,41998.0,45994.0,66990.0,75450.0,47851.0,56555.0,99900.0,32221.0,132228.0,48000.0,37393.0,75008.0,38991.0,36991.0,40122.0,66690.0,39900.0,64991.0,64900.0,27749.0,46800.0,49000.0,29989.0,53988.0,43545.0,44562.0,64000.0,63933.0,44000.0,39894.0,47618.0,81285.0,79900.0,15888.0,41403.0,212894.0,31881.0,49885.0,81894.0,30698.0,31986.0,56999.0,73362.0,47983.0,37882.0,36991.0,44977.0,118500.0,92676.0,27497.0,42000.0,50490.0,170388.0,129900.0,53900.0,47675.0,37894.0,37661.0,34984.0,192997.0,43777.0,42900.0,41990.0,39877.0,41939.0,39984.0,31000.0,42974.0,43465.0,38986.0,32998.0,45991.0,38595.0,32900.0,29999.0,71356.0,56633.0,47307.0,61988.0,32995.0,39459.0,38741.0,31997.0,30990.0,71995.0,41874.0,119255.0,43781.0,66164.0,77933.0,49900.0,29862.0,54800.0,31654.0,50500.0,26615.0,93100.0,91358.0,29999.0,53999.0,27990.0,70497.0,51997.0,30998.0,37844.0,47990.0,40441.0,44998.0,44995.0,62500.0,206885.0,25900.0,35998.0,50997.0,46997.0,73777.0,30978.0,56777.0,36595.0,43712.0,47575.0,69111.0,43803.0,55988.0,112999.0,25998.0,40877.0,29984.0,54995.0,117999.0,25991.0,68523.0,31383.0,70994.0,35988.0,55265.0,47988.0,30439.0,26650.0,29853.0,39900.0,39600.0,91999.0,39983.0,29990.0,39891.0,41600.0,51990.0,218554.0,32294.0,30991.0,79900.0,62988.0,62988.0,33495.0,41888.0,43820.0,217994.0,29795.0,54999.0,46809.0,39870.0,46999.0,37995.0,36395.0,41990.0,78500.0,49722.0,35993.0,31789.0,32855.0,29933.0,65555.0,47870.0,108670.0,58900.0,33639.0,71360.0,39890.0,39500.0,57835.0,51999.0,34994.0,29995.0,44611.0,44004.0,40364.0,30304.0,55992.0,24888.0,52900.0,43892.0,48999.0,52995.0,31900.0,32500.0,48794.0,41855.0,92925.0,56499.0,25594.0,33977.0,47755.0,28997.0,54777.0,162900.0,139891.0,45988.0,25998.0,45688.0,62545.0,50393.0,45493.0,45450.0,28892.0,60950.0,44029.0,30998.0,29997.0,43590.0,47750.0,25790.0,41500.0,45991.0,36988.0,51933.0,44900.0,49795.0,63989.0,40984.0,41994.0,27496.0,28999.0,53896.0,73993.0,43995.0,63995.0,66983.0,49990.0,36980.0,42990.0,51588.0,29994.0,43592.0,64500.0,45500.0,44900.0,96991.0,71500.0,46777.0,49983.0,29690.0,41880.0,48000.0,104995.0,46822.0,73922.0,63998.0,26690.0,73887.0,35494.0,81225.0,80893.0,29991.0,41221.0,24999.0,45997.0,52983.0,39699.0,38961.0,62988.0,31500.0,37350.0,64493.0,32190.0,74893.0,59933.0,40495.0,82444.0,44995.0,33865.0,37988.0,51989.0,61693.0,114500.0,43848.0,64498.0,41492.0,49988.0,26999.0,39974.0,45798.0,35900.0,30489.0,23980.0,44555.0,94898.0,39885.0,49500.0,45440.0,39493.0,65999.0,44990.0,34958.0,31563.0,163981.0,32882.0,31000.0,47000.0,24991.0,45777.0,63995.0,33756.0,45624.0,56998.0,41900.0,63798.0,116963.0,33392.0,34111.0,38368.0,43890.0,59900.0,35696.0,43400.0,36882.0,35998.0,48488.0,66991.0,198888.0,43997.0,48885.0,60888.0,53225.0,39999.0,31756.0,45898.0,130000.0,50426.0,33493.0,33989.0,39562.0,65973.0,90972.0,58999.0,52777.0,58903.0,35248.0,57555.0,44876.0,63500.0,41400.0,55490.0,20991.0,53777.0,20647.0,32928.0,32282.0,90971.0,30840.0,38880.0,47894.0,31195.0,39997.0,69986.0,56895.0,46273.0,71995.0,55777.0,37442.0,35987.0,41555.0,60880.0,42900.0,45990.0,27890.0,30993.0,70977.0,26979.0,38562.0,40650.0,38300.0,67991.0,41986.0,44000.0,53771.0,32900.0,37989.0,49998.0,43498.0,31488.0,43800.0,74320.0,47050.0,32892.0,27998.0,42495.0,53505.0,49184.0,67393.0,54596.0,61577.0,36991.0,76990.0,39661.0,40490.0,41451.0,37997.0,44480.0,76994.0,29795.0,31750.0,178000.0,44580.0,32495.0,41780.0,38523.0,52777.0,41507.0,39988.0,37990.0,56954.0,166230.0,50992.0,36900.0,69801.0,45495.0,17998.0,29946.0,31933.0,30995.0,47598.0,20754.0,32596.0,66500.0,99998.0,41444.0,42991.0,72563.0,49990.0,52996.0,30500.0,45651.0,32962.0,25894.0,33989.0,28998.0,39692.0,30986.0,40981.0,43777.0,99884.0,42250.0,54894.0,66777.0,45393.0,117981.0,97973.0,33988.0,64777.0,40594.0,52990.0,28818.0,32818.0,35500.0,31500.0,32000.0,29999.0,37988.0,149495.0,26995.0,72999.0,27999.0,36958.0,72990.0,33988.0,33390.0,36393.0,25736.0,40490.0,30880.0,41000.0,34656.0,61991.0,45990.0,48043.0,51045.0,39672.0,47500.0,31998.0,47840.0,53991.0,34409.0,135640.0,53342.0,46998.0,57315.0,127598.0,78585.0,72895.0,59000.0,89984.0,45950.0,32725.0,39991.0,51505.0,38798.0,49695.0,228323.0,43149.0,54998.0,67750.0,65893.0,49888.0,55998.0,38779.0,147900.0,30383.0,55791.0,31145.0,43995.0,41800.0,31990.0,42999.0,48253.0,43884.0,52887.0,32988.0,60575.0,99999.0,75982.0,40777.0,112999.0,36854.0,115991.0,48125.0,40987.0,26988.0,33534.0,27933.0,39490.0,45888.0,30000.0,82900.0,53555.0,40499.0,68777.0,104992.0,56705.0,33960.0,30595.0,32933.0,61844.0,36784.0,34900.0,44000.0,31707.0,42998.0,31750.0,84000.0,53299.0,40777.0,29933.0,33555.0,52905.0,34888.0,55901.0,44000.0,49480.0,40385.0,27996.0,73478.0,33498.0,38998.0,37918.0,32544.0,41863.0,36990.0,46545.0,39997.0,40982.0,37916.0,61291.0,34444.0,150299.0,52989.0,38000.0,63750.0,45822.0,25200.0,50997.0,49989.0,34991.0,46900.0,29584.0,34895.0,19677.0,163998.0,38794.0,31998.0,88933.0,45977.0,30490.0,52995.0,38780.0,47995.0,65991.0,94000.0,65480.0,31379.0,46500.0,33933.0,34890.0,71998.0,116991.0,43900.0,60000.0,46900.0,89585.0,173998.0,40525.0,29950.0,60805.0,37999.0,39390.0,35780.0,39862.0,54991.0,29894.0,43993.0,72999.0,30834.0,31998.0,209991.0,43893.0,44965.0,59993.0,49741.0,52900.0,48499.0,48890.0,48890.0,29524.0,32998.0,37840.0,40000.0,47588.0,33870.0,37989.0,36992.0,59995.0,51994.0,47777.0,48989.0,49987.0,72661.0,37813.0,79990.0,34988.0,49900.0,72374.0,28860.0,29998.0,44834.0,70065.0,97000.0,75000.0,39965.0,56299.0,75900.0,41996.0,54500.0,49680.0,25499.0,57140.0,45484.0,61930.0,51565.0,73994.0,33999.0,30799.0,33892.0,55000.0,41999.0,56795.0,29995.0,39879.0,172500.0,64107.0,40000.0,38865.0,51900.0,27685.0,35933.0,72991.0,36994.0,57499.0,33990.0,54888.0,67755.0,65936.0,32633.0,48777.0,24997.0,34250.0,77506.0,50777.0,77850.0,34449.0,97892.0,43777.0,61577.0,67989.0,46225.0,48558.0,28500.0,50400.0,44987.0,60597.0,46900.0,49988.0,53900.0,51998.0,48777.0,68999.0,75994.0,54900.0,34887.0,30945.0,39597.0,44984.0,50778.0,45777.0,32544.0,29893.0,68290.0,31950.0,54998.0,85894.0,29889.0,41291.0,41291.0,28550.0,49581.0,91988.0,25976.0,31000.0,27999.0,29599.0,30497.0,36750.0,39991.0,73994.0,45990.0,41558.0,193901.0,89989.0,54498.0,47781.0,31995.0,30740.0,33850.0,88799.0,40558.0,31489.0,20995.0,30799.0,56995.0,39900.0,56999.0,45777.0,32000.0,46900.0,27500.0,36792.0,39998.0,36382.0,48157.0,28351.0,40500.0,46777.0,69880.0,33950.0,152986.0,31195.0,23168.0,40999.0,229918.0,72545.0,97575.0,64215.0,63994.0,87599.0,39200.0,57499.0,63995.0,56695.0,38760.0,94989.0,74989.0,44933.0,29500.0,34995.0,57153.0,51763.0,30900.0,20562.0,57555.0,89998.0,45877.0,117495.0,31992.0,35416.0,85999.0,65886.0,75577.0,53490.0,89599.0,32890.0,53983.0,46035.0],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Price"}},"legend":{"tracegroupgap":0},"title":{"text":"Scatter plot of Year vs Price"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('1954d615-2ffc-41ee-aacd-c903a71df0fe');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="c1"># Preparar los datos para el modelo</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Year&#39;</span><span class="p">,</span> <span class="s1">&#39;Mileage&#39;</span><span class="p">,</span> <span class="s1">&#39;Rating&#39;</span><span class="p">,</span> <span class="s1">&#39;Review Count&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">]</span>

<span class="c1"># Añadir una constante a las variables independientes (necesario para statsmodels)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Dividir los datos en conjuntos de entrenamiento y prueba</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Crear y ajustar el modelo de regresión lineal usando OLS (Ordinary Least Squares)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Ver el resumen del modelo</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  Price   R-squared:                       0.080
Model:                            OLS   Adj. R-squared:                  0.079
Method:                 Least Squares   F-statistic:                     42.41
Date:                Fri, 10 May 2024   Prob (F-statistic):           3.82e-34
Time:                        14:06:50   Log-Likelihood:                -22805.
No. Observations:                1943   AIC:                         4.562e+04
Df Residuals:                    1938   BIC:                         4.565e+04
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
================================================================================
                   coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const        -1.814e+06   1.37e+06     -1.320      0.187   -4.51e+06     8.8e+05
Year           925.2419    679.079      1.362      0.173    -406.560    2257.044
Mileage         -0.4244      0.061     -6.969      0.000      -0.544      -0.305
Rating         855.5688   1887.632      0.453      0.650   -2846.433    4557.571
Review Count     8.2971      3.111      2.667      0.008       2.196      14.398
==============================================================================
Omnibus:                     1142.504   Durbin-Watson:                   2.028
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8978.254
Skew:                           2.749   Prob(JB):                         0.00
Kurtosis:                      11.981   Cond. No.                     5.36e+07
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 5.36e+07. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Preparar los datos para el modelo</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Year&#39;</span><span class="p">,</span> <span class="s1">&#39;Mileage&#39;</span><span class="p">,</span> <span class="s1">&#39;Rating&#39;</span><span class="p">,</span> <span class="s1">&#39;Review Count&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">]</span>

<span class="c1"># Dividir los datos en conjuntos de entrenamiento y prueba</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Crear y entrenar el modelo de regresión lineal</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predecir y evaluar el modelo</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;El error cuadrático medio (MSE) del modelo es: </span><span class="si">{</span><span class="n">mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Coeficientes del modelo</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>El error cuadrático medio (MSE) del modelo es: 862994696.6759222
Coefficients: [ 9.25241897e+02 -4.24385254e-01  8.55568794e+02  8.29710501e+00]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="introduccion-a-la-regresion-logistica">
<h1><span class="section-number">25. </span>Introducción a la Regresión Logística<a class="headerlink" href="#introduccion-a-la-regresion-logistica" title="Link to this heading">#</a></h1>
<p>La regresión logística es un método estadístico y de aprendizaje automático ampliamente utilizado, lo cual lo hace un método fundamental en el análisis de clasificación binaria y es utilizado en el campo financiero para predecir eventos como el incumplimiento de crédito, la calificación crediticia, entre otros. Este modelo ofrece una probabilidad condicional de la variable de respuesta basada en los predictores utilizados.</p>
<section id="id2">
<h2><span class="section-number">25.1. </span>Fundamentos Teóricos<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<section id="definicion-de-regresion-logistica">
<h3><span class="section-number">25.1.1. </span>Definición de Regresión Logística<a class="headerlink" href="#definicion-de-regresion-logistica" title="Link to this heading">#</a></h3>
<p>Las familias lineales generalizadas (GLM) proporcionan un marco teórico para relacionar una variable dependiente, <span class="math notranslate nohighlight">\(Y\)</span>, con una o más variables independientes, <span class="math notranslate nohighlight">\(X\)</span>. Este marco generaliza la regresión lineal al permitir que la variable dependiente siga una distribución de probabilidad de una familia exponencial y vinculando el valor esperado de la distribución a la combinación lineal de las variables independientes a través de una función de enlace.</p>
<p>La forma general de un GLM se puede describir con la siguiente ecuación:</p>
<div class="math notranslate nohighlight">
\[
g(E(Y)) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n
\]</div>
<p>donde <span class="math notranslate nohighlight">\(g\)</span> es la función de enlace y <span class="math notranslate nohighlight">\(\beta_i\)</span> son los coeficientes que se estiman a partir de los datos.</p>
</section>
</section>
<section id="relacion-con-la-regresion-lineal">
<h2><span class="section-number">25.2. </span>Relación con la Regresión Lineal<a class="headerlink" href="#relacion-con-la-regresion-lineal" title="Link to this heading">#</a></h2>
<p>La regresión lineal es un caso especial de GLM donde la variable dependiente <span class="math notranslate nohighlight">\(Y\)</span> se presume que sigue una distribución normal y la función de enlace es la identidad, es decir, <span class="math notranslate nohighlight">\(g(x) = x\)</span>. La ecuación de regresión lineal es:</p>
<div class="math notranslate nohighlight">
\[
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
\]</div>
<p>donde <span class="math notranslate nohighlight">\(\epsilon\)</span> es el término de error, normalmente distribuido con media cero y varianza constante.</p>
</section>
<section id="diferencias-entre-regresion-lineal-y-logistica">
<h2><span class="section-number">25.3. </span>Diferencias entre Regresión Lineal y Logística<a class="headerlink" href="#diferencias-entre-regresion-lineal-y-logistica" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Regresión Lineal</strong>: Predice un valor continuo basado en la suma ponderada de variables independientes.</p></li>
<li><p><strong>Regresión Logística</strong>: Predice la probabilidad de ocurrencia de un evento mediante la función logística, que siempre produce un resultado entre 0 y 1, adecuado para clasificación.</p></li>
</ul>
<section id="importancia-de-la-funcion-logit">
<h3><span class="section-number">25.3.1. </span>Importancia de la Función Logit<a class="headerlink" href="#importancia-de-la-funcion-logit" title="Link to this heading">#</a></h3>
<p>La ecuación de la regresión logística es:</p>
<div class="math notranslate nohighlight">
\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n
\]</div>
<p>donde <span class="math notranslate nohighlight">\(p\)</span> es la probabilidad de que <span class="math notranslate nohighlight">\(Y=1\)</span>. Reorganizando la ecuación, podemos expresar <span class="math notranslate nohighlight">\(p\)</span> como:</p>
<div class="math notranslate nohighlight">
\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
\]</div>
</section>
</section>
</section>
<section id="calculo-de-parametros">
<h1><span class="section-number">26. </span>Cálculo de Parámetros<a class="headerlink" href="#calculo-de-parametros" title="Link to this heading">#</a></h1>
<p>Los parámetros <span class="math notranslate nohighlight">\(\beta_i\)</span> en la regresión logística se calculan generalmente mediante el método de máxima verosimilitud. El objetivo es encontrar los valores de <span class="math notranslate nohighlight">\(\beta\)</span> que maximicen la función de verosimilitud, que indica qué tan bien el modelo con ciertos parámetros podría haber producido los datos observados.</p>
<p>Para un conjunto de datos con <span class="math notranslate nohighlight">\(n\)</span> observaciones, la función de verosimilitud <span class="math notranslate nohighlight">\(L(\beta)\)</span> para la regresión logística es:</p>
<div class="math notranslate nohighlight">
\[
L(\beta) = \prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}
\]</div>
<p>donde <span class="math notranslate nohighlight">\(y_i\)</span> es el valor observado de la variable respuesta (0 o 1) para la observación <span class="math notranslate nohighlight">\(i\)</span>, y <span class="math notranslate nohighlight">\(p_i\)</span> es la probabilidad predicha de que <span class="math notranslate nohighlight">\(Y_i=1\)</span> dado <span class="math notranslate nohighlight">\(X_i\)</span>. Los parámetros se estiman a través de métodos numéricos, como el algoritmo de Newton-Raphson o el método de descenso de gradiente, debido a la complejidad de la solución analítica.</p>
</section>
<section id="preprocesamiento-de-datos-para-regresion-logistica">
<h1><span class="section-number">27. </span>Preprocesamiento de Datos para Regresión Logística<a class="headerlink" href="#preprocesamiento-de-datos-para-regresion-logistica" title="Link to this heading">#</a></h1>
<p>Para garantizar que la regresión logística funcione eficazmente y genere resultados precisos, es crucial transformar y preparar adecuadamente las variables predictoras antes de aplicar el modelo. A continuación, se detallan algunas de las transformaciones más comunes aplicadas a los predictores en un modelo de regresión logística.</p>
<section id="variables-dummy">
<h2><span class="section-number">27.1. </span>1. Variables Dummy<a class="headerlink" href="#variables-dummy" title="Link to this heading">#</a></h2>
<p>En la regresión logística, todas las variables predictoras deben ser numéricas. Sin embargo, a menudo nos encontramos con variables categóricas (como género, estado civil, etc.) que son cualitativas y no se pueden incluir directamente en los modelos matemáticos. Para incorporar estas variables en el análisis, las convertimos en variables dummy o indicadoras.</p>
<p>Una variable dummy toma el valor 0 o 1 para indicar la ausencia o presencia de cierta categoría. Por cada categoría de una variable categórica, se crea una nueva variable dummy, excepto una para evitar la multicolinealidad (el problema de las variables predictoras altamente correlacionadas).</p>
<p>Por ejemplo, para una variable categórica como color con tres categorías (rojo, verde, azul), podríamos crear dos dummies:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">color_rojo</span></code>: 1 si el color es rojo, 0 de lo contrario.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">color_verde</span></code>: 1 si el color es verde, 0 de lo contrario.</p></li>
</ul>
<p>No necesitamos una dummy para “azul” porque se infiere cuando <code class="docutils literal notranslate"><span class="pre">color_rojo</span></code> y <code class="docutils literal notranslate"><span class="pre">color_verde</span></code> son ambos 0.</p>
</section>
<section id="normalizacion">
<h2><span class="section-number">27.2. </span>2. Normalización<a class="headerlink" href="#normalizacion" title="Link to this heading">#</a></h2>
<p>La normalización (o estandarización) de las variables predictoras es crucial cuando las escalas de las variables difieren significativamente o cuando se utiliza un método de optimización, como el descenso de gradiente, que es sensible a la escala de las características de entrada. Normalizar las variables significa ajustar los datos para que tengan una media de cero y una desviación estándar de uno. Esto se logra con la fórmula:</p>
<div class="math notranslate nohighlight">
\[
X_{\text{norm}} = \frac{X - \mu}{\sigma}
\]</div>
<p>donde <span class="math notranslate nohighlight">\(\mu\)</span> es la media de la variable y <span class="math notranslate nohighlight">\(\sigma\)</span> es su desviación estándar.</p>
</section>
<section id="transformaciones-logaritmicas">
<h2><span class="section-number">27.3. </span>3. Transformaciones Logarítmicas<a class="headerlink" href="#transformaciones-logaritmicas" title="Link to this heading">#</a></h2>
<p>Las transformaciones logarítmicas son útiles cuando una variable predictora no sigue una distribución normal, especialmente si la variable tiene una distribución sesgada (como ingresos o población de una ciudad). Aplicar una transformación logarítmica puede ayudar a estabilizar la varianza y normalizar la distribución de los datos.</p>
<p>La transformación se realiza como sigue:</p>
<div class="math notranslate nohighlight">
\[
X_{\text{log}} = \log(X + 1)
\]</div>
<p>El “+1” se asegura de que no haya problemas al tomar el logaritmo de cero.</p>
</section>
</section>
<section id="modelo-logit">
<h1><span class="section-number">28. </span>Modelo Logit<a class="headerlink" href="#modelo-logit" title="Link to this heading">#</a></h1>
<p>El modelo logit utiliza la función logística para vincular la probabilidad de ocurrencia del evento a las variables predictoras. La función logística transforma cualquier valor entre menos infinito y más infinito a un valor entre 0 y 1, interpretado como una probabilidad.</p>
<section id="ecuacion-del-modelo-logit">
<h2><span class="section-number">28.1. </span>Ecuación del Modelo Logit<a class="headerlink" href="#ecuacion-del-modelo-logit" title="Link to this heading">#</a></h2>
<p>La probabilidad de que la variable dependiente Y sea igual a 1, dado un conjunto de variables predictoras <span class="math notranslate nohighlight">\(X_1, X_2, ..., X_k \)</span> se modela como:</p>
<div class="math notranslate nohighlight">
\[
P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + ... + \beta_kX_k)}}
\]</div>
<p>donde <span class="math notranslate nohighlight">\( \beta_0, \beta_1, ..., \beta_k \)</span> son los coeficientes del modelo, y <span class="math notranslate nohighlight">\( e \)</span> es la base del logaritmo natural.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">plot_logit_curve</span><span class="p">(</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span><span class="p">):</span>
    <span class="c1"># Definir un rango de valores para X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
    <span class="c1"># Calcular la probabilidad P(Y=1|X) usando la función logística</span>
    <span class="n">P</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">X</span><span class="p">)))</span>

    <span class="c1"># Crear el gráfico</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Logit Model&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Logit Curve&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability P(Y=1|X)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_logit_curve</span><span class="p">(</span><span class="n">beta_0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bc970a8d2660237db815133bdcd0063c485903abe8fea940aa1e10da8d3ce3c4.png" src="_images/bc970a8d2660237db815133bdcd0063c485903abe8fea940aa1e10da8d3ce3c4.png" />
</div>
</div>
</section>
</section>
<section id="modelo-probit">
<h1><span class="section-number">29. </span>Modelo Probit<a class="headerlink" href="#modelo-probit" title="Link to this heading">#</a></h1>
<p>El modelo probit utiliza la función de distribución acumulativa normal estándar (CDF de una distribución normal estándar) para modelar la relación entre las variables independientes y la probabilidad de ocurrencia del evento.</p>
<section id="ecuacion-del-modelo-probit">
<h2><span class="section-number">29.1. </span>Ecuación del Modelo Probit<a class="headerlink" href="#ecuacion-del-modelo-probit" title="Link to this heading">#</a></h2>
<p>De manera similar al modelo logit, la probabilidad en el modelo probit se expresa como:</p>
<div class="math notranslate nohighlight">
\[
P(Y=1|X) = \Phi(\beta_0 + \beta_1X_1 + ... + \beta_kX_k)
\]</div>
<p>donde <span class="math notranslate nohighlight">\( \Phi \)</span> es la función de distribución acumulativa de la distribución normal estándar y <span class="math notranslate nohighlight">\( \beta_0, \beta_1, ..., \beta_k \)</span> son los coeficientes estimados del modelo.</p>
</section>
</section>
<section id="diferencias-clave">
<h1><span class="section-number">30. </span>Diferencias Clave<a class="headerlink" href="#diferencias-clave" title="Link to this heading">#</a></h1>
<ol class="arabic simple">
<li><p><strong>Función de Enlace</strong>: Logit usa la función logística, mientras que Probit utiliza la CDF de la distribución normal estándar. Esto lleva a diferentes formas de las curvas de probabilidad.</p></li>
<li><p><strong>Interpretación</strong>: En la mayoría de los casos, los modelos logit y probit proporcionan resultados similares, aunque pueden diferir en casos donde las colas de la distribución son importantes debido a las propiedades de las funciones de enlace.</p></li>
<li><p><strong>Computacional</strong>: El modelo logit es generalmente más simple y rápido de estimar en comparación con el modelo probit, que requiere la integración de la función de distribución normal.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_probit_curve</span><span class="p">(</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span><span class="p">):</span>
    <span class="c1"># Definir un rango de valores para X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
    <span class="c1"># Calcular la probabilidad P(Y=1|X) usando la función CDF de la distribución normal estándar</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># Crear el gráfico</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Probit Model&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Probit Curve&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability P(Y=1|X)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_probit_curve</span><span class="p">(</span><span class="n">beta_0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7b313cde111cbd7490a65fee4079a84e5095dc4761ed43fab77bfcd8b1c13827.png" src="_images/7b313cde111cbd7490a65fee4079a84e5095dc4761ed43fab77bfcd8b1c13827.png" />
</div>
</div>
<p>Si bien ambas curvas son similares, hay ligeras diferencias como se puede apreciar a continuación:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">plot_logit_and_probit_curves</span><span class="p">(</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span><span class="p">):</span>
    <span class="c1"># Definir un rango de valores para X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>

    <span class="c1"># Calcular la probabilidad P(Y=1|X) usando la función logística para Logit</span>
    <span class="n">P_logit</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">X</span><span class="p">)))</span>

    <span class="c1"># Calcular la probabilidad P(Y=1|X) usando la función CDF de la distribución normal estándar para Probit</span>
    <span class="n">P_probit</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># Crear el gráfico</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">P_logit</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Logit Model&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">P_probit</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Probit Model&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparison of Logit and Probit Curves&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability P(Y=1|X)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_logit_and_probit_curves</span><span class="p">(</span><span class="n">beta_0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/88414e8c418c91f2e6398eb4dc8f311e4e005add8065caeffd56a2122f6c2c4c.png" src="_images/88414e8c418c91f2e6398eb4dc8f311e4e005add8065caeffd56a2122f6c2c4c.png" />
</div>
</div>
</section>
<section id="ejemplo-de-clasificacion-binaria-usando-regresion-caso-de-pronostico-de-riesgo">
<h1><span class="section-number">31. </span>Ejemplo de clasificación binaria usando regresión: caso de pronóstico de riesgo<a class="headerlink" href="#ejemplo-de-clasificacion-binaria-usando-regresion-caso-de-pronostico-de-riesgo" title="Link to this heading">#</a></h1>
<p>En este caso de estudio, exploraremos un conjunto de datos financieros para predecir el riesgo de incumplimiento en el pago de créditos, lo cual es esencial para las instituciones financieras. El objetivo es implementar un modelo de regresión logística para prever la probabilidad de que un cliente no pague su crédito, conocido como “default”.</p>
<section id="descripcion-de-las-columnas-del-dataset">
<h2><span class="section-number">31.1. </span>Descripción de las columnas del dataset<a class="headerlink" href="#descripcion-de-las-columnas-del-dataset" title="Link to this heading">#</a></h2>
<p>El dataset contiene las siguientes columnas:</p>
<ul class="simple">
<li><p><strong>RIESGO</strong>: Variable objetivo (binaria). <code class="docutils literal notranslate"><span class="pre">1</span></code> indica que el cliente paga el crédito (‘SI PAGA’), y <code class="docutils literal notranslate"><span class="pre">0</span></code> que el cliente incumple el pago (‘NO PAGA’).</p></li>
<li><p><strong>MONTODELCREDITO</strong>: Cantidad total del crédito otorgado.</p></li>
<li><p><strong>NUMERODECUOTAS</strong>: Número total de cuotas en las que el crédito debe ser devuelto.</p></li>
<li><p><strong>INGRESOS</strong>: Ingresos mensuales del cliente.</p></li>
<li><p><strong>INGRESOSFAMILIARES</strong>: Ingresos mensuales totales de la familia del cliente.</p></li>
<li><p><strong>EDAD</strong>: Edad del cliente.</p></li>
<li><p><strong>DESTINODELCREDITO</strong>: Propósito del crédito (<code class="docutils literal notranslate"><span class="pre">0</span></code> para estudios de pregrado, <code class="docutils literal notranslate"><span class="pre">1</span></code> para estudios de posgrado).</p></li>
<li><p><strong>GENERO</strong>: Género del cliente (<code class="docutils literal notranslate"><span class="pre">0</span></code> para masculino, <code class="docutils literal notranslate"><span class="pre">1</span></code> para femenino).</p></li>
<li><p><strong>CASAPROPIA</strong>: Indica si el cliente posee casa propia (<code class="docutils literal notranslate"><span class="pre">1</span></code>) o no (<code class="docutils literal notranslate"><span class="pre">0</span></code>).</p></li>
<li><p><strong>CODEUDOR</strong>: Indica si el cliente tiene un codeudor (<code class="docutils literal notranslate"><span class="pre">1</span></code>) o no (<code class="docutils literal notranslate"><span class="pre">0</span></code>).</p></li>
<li><p><strong>SITUACIONLABORAL</strong>: Estado laboral del cliente (<code class="docutils literal notranslate"><span class="pre">0</span></code> para independiente, <code class="docutils literal notranslate"><span class="pre">1</span></code> para empleado).</p></li>
<li><p><strong>ESTADOCIVIL</strong>: Estado civil del cliente (<code class="docutils literal notranslate"><span class="pre">0</span></code> para soltero, viudo, etc., <code class="docutils literal notranslate"><span class="pre">1</span></code> para casado).</p></li>
<li><p><strong>ESTRATO</strong>: Estrato socioeconómico del cliente, valor entero de 1 a 5.</p></li>
</ul>
</section>
<section id="implementacion-del-modelo-de-regresion-logistica">
<h2><span class="section-number">31.2. </span>Implementación del modelo de regresión logística<a class="headerlink" href="#implementacion-del-modelo-de-regresion-logistica" title="Link to this heading">#</a></h2>
<p>Primero, importaremos las librerías necesarias y prepararemos los datos para el modelo. Luego, ajustaremos el modelo de regresión logística y evaluaremos su rendimiento utilizando la matriz de confusión y la curva ROC.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Cargar el dataset</span>
<span class="c1"># Supongamos que el DataFrame se llama &#39;data&#39;</span>

<span class="c1"># Dividir el dataset en conjuntos de entrenamiento y prueba</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Crear y entrenar el modelo de regresión logística</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Realizar predicciones sobre el conjunto de prueba</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Evaluar el modelo</span>
<span class="c1"># Matriz de confusión</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matriz de confusión:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>

<span class="c1"># Curva ROC y AUC</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">)</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC curve (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">auc</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver Operating Characteristic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Cargar el dataset</span>
<span class="c1"># Supongamos que el DataFrame se llama &#39;data&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;/content/Taller # 2.xls&#39;</span><span class="p">,</span><span class="n">sheet_name</span><span class="o">=</span><span class="s2">&quot;Poblacion Logit-Probit&quot;</span><span class="p">)</span>
<span class="c1"># Dividir el dataset en conjuntos de entrenamiento y prueba</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1952</span><span class="p">)</span>

<span class="c1"># Crear y entrenar el modelo de regresión logística</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Realizar predicciones sobre el conjunto de prueba</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Evaluar el modelo</span>
<span class="c1"># Matriz de confusión</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matriz de confusión:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>

<span class="c1"># Curva ROC y AUC</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">)</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC curve (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">auc</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver Operating Characteristic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-1-d89142e333c6&gt;</span> in <span class="ni">&lt;cell line: 10&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="c1"># Cargar el dataset</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="c1"># Supongamos que el DataFrame se llama &#39;data&#39;</span>
<span class="ne">---&gt; </span><span class="mi">10</span> <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;/content/Taller # 2.xls&#39;</span><span class="p">,</span><span class="n">sheet_name</span><span class="o">=</span><span class="s2">&quot;Poblacion Logit-Probit&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="c1"># Dividir el dataset en conjuntos de entrenamiento y prueba</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span> <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py</span> in <span class="ni">read_excel</span><span class="nt">(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)</span>
<span class="g g-Whitespace">    </span><span class="mi">476</span>     <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">io</span><span class="p">,</span> <span class="n">ExcelFile</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">477</span>         <span class="n">should_close</span> <span class="o">=</span> <span class="kc">True</span>
<span class="ne">--&gt; </span><span class="mi">478</span>         <span class="n">io</span> <span class="o">=</span> <span class="n">ExcelFile</span><span class="p">(</span><span class="n">io</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="n">storage_options</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="n">engine</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">479</span>     <span class="k">elif</span> <span class="n">engine</span> <span class="ow">and</span> <span class="n">engine</span> <span class="o">!=</span> <span class="n">io</span><span class="o">.</span><span class="n">engine</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">480</span>         <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py</span> in <span class="ni">__init__</span><span class="nt">(self, path_or_buffer, engine, storage_options)</span>
<span class="g g-Whitespace">   </span><span class="mi">1494</span>                 <span class="n">ext</span> <span class="o">=</span> <span class="s2">&quot;xls&quot;</span>
<span class="g g-Whitespace">   </span><span class="mi">1495</span>             <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1496</span>                 <span class="n">ext</span> <span class="o">=</span> <span class="n">inspect_excel_format</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1497</span>                     <span class="n">content_or_path</span><span class="o">=</span><span class="n">path_or_buffer</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="n">storage_options</span>
<span class="g g-Whitespace">   </span><span class="mi">1498</span>                 <span class="p">)</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py</span> in <span class="ni">inspect_excel_format</span><span class="nt">(content_or_path, storage_options)</span>
<span class="g g-Whitespace">   </span><span class="mi">1369</span>         <span class="n">content_or_path</span> <span class="o">=</span> <span class="n">BytesIO</span><span class="p">(</span><span class="n">content_or_path</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1370</span> 
<span class="ne">-&gt; </span><span class="mi">1371</span>     <span class="k">with</span> <span class="n">get_handle</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1372</span>         <span class="n">content_or_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">,</span> <span class="n">storage_options</span><span class="o">=</span><span class="n">storage_options</span><span class="p">,</span> <span class="n">is_text</span><span class="o">=</span><span class="kc">False</span>
<span class="g g-Whitespace">   </span><span class="mi">1373</span>     <span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/pandas/io/common.py</span> in <span class="ni">get_handle</span><span class="nt">(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">866</span>         <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">867</span>             <span class="c1"># Binary mode</span>
<span class="ne">--&gt; </span><span class="mi">868</span>             <span class="n">handle</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">869</span>         <span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">870</span> 

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;/content/Taller # 2.xls&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="analisis-del-rendimiento-del-modelo-de-regresion-logistica">
<h1><span class="section-number">32. </span>Análisis del rendimiento del modelo de regresión logística<a class="headerlink" href="#analisis-del-rendimiento-del-modelo-de-regresion-logistica" title="Link to this heading">#</a></h1>
<section id="matriz-de-confusion">
<h2><span class="section-number">32.1. </span>Matriz de confusión<a class="headerlink" href="#matriz-de-confusion" title="Link to this heading">#</a></h2>
<p>La matriz de confusión es una herramienta poderosa para evaluar el rendimiento de un modelo de clasificación. La matriz para este modelo es:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><strong>Predicción</strong>: No Paga</p></th>
<th class="head"><p><strong>Predicción</strong>: Paga</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Real</strong>: No Paga</p></td>
<td><p>22</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Real</strong>: Paga</p></td>
<td><p>11</p></td>
<td><p>17</p></td>
</tr>
</tbody>
</table>
</div>
<ul class="simple">
<li><p><strong>Verdaderos Negativos (VN)</strong>: 22 clientes que realmente no pagan y fueron correctamente identificados por el modelo.</p></li>
<li><p><strong>Falsos Positivos (FP)</strong>: 4 clientes que no pagarían, pero el modelo predijo incorrectamente que sí lo harían.</p></li>
<li><p><strong>Falsos Negativos (FN)</strong>: 11 clientes que pagarían, pero el modelo predijo incorrectamente que no lo harían.</p></li>
<li><p><strong>Verdaderos Positivos (VP)</strong>: 17 clientes que realmente pagan y fueron correctamente identificados por el modelo.</p></li>
</ul>
</section>
<section id="curva-roc-y-auc">
<h2><span class="section-number">32.2. </span>Curva ROC y AUC<a class="headerlink" href="#curva-roc-y-auc" title="Link to this heading">#</a></h2>
<p>La Curva de Características Operativas del Receptor (ROC) es una representación gráfica que muestra el rendimiento de un modelo de clasificación en todos los umbrales de clasificación. Esta curva traza dos parámetros:</p>
<ul class="simple">
<li><p><strong>Tasa de Verdaderos Positivos (TPR, Sensibilidad)</strong>: Se calcula como <span class="math notranslate nohighlight">\( \text{TPR} = \frac{VP}{VP + FN} \)</span>. Indica qué proporción de positivos reales fue identificada correctamente.</p></li>
<li><p><strong>Tasa de Falsos Positivos (FPR)</strong>: Se calcula como <span class="math notranslate nohighlight">\( \text{FPR} = \frac{FP}{VN + FP} \)</span>. Indica qué proporción de negativos reales fue identificada incorrectamente.</p></li>
</ul>
<p>La curva ROC se grafica con FPR en el eje x y TPR en el eje y. Un modelo perfecto se ubicaría en la esquina superior izquierda del gráfico, donde TPR es 1 y FPR es 0. La línea diagonal desde el (0,0) hasta el (1,1) representa un modelo aleatorio. Cuanto más se acerque la curva hacia la esquina superior izquierda, mejor será el modelo.</p>
<p>El área bajo la curva ROC (AUC) proporciona una medida agregada de rendimiento en todos los umbrales posibles. Un AUC de 1 representa un modelo perfecto, mientras que un AUC de 0.5 representa un modelo no mejor que el azar. En este caso, un AUC de 0.82 indica un buen rendimiento del modelo.</p>
</section>
<section id="interpretacion-de-la-curva-roc">
<h2><span class="section-number">32.3. </span>Interpretación de la curva ROC<a class="headerlink" href="#interpretacion-de-la-curva-roc" title="Link to this heading">#</a></h2>
<p>La curva ROC proporcionada muestra que el modelo tiene una buena capacidad para diferenciar entre las clases de respuesta. A medida que se mueve el umbral de decisión para clasificar un pago como positivo (paga) o negativo (no paga), el modelo ajusta su sensibilidad y especificidad. Observar dónde se producen cambios significativos en la curva puede ayudar a seleccionar un umbral de decisión que equilibre entre capturar la mayoría de los pagos verdaderos y minimizar el error de clasificar a alguien como pagador cuando no lo es.</p>
<p>En scikit-learn, los modelos de regresión logística no utilizan un método directo como <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> que se encuentra en los árboles de decisión para indicar la importancia de las características. Sin embargo, en la regresión logística, la importancia de las características puede inferirse a partir de los coeficientes del modelo.</p>
</section>
<section id="coeficientes-en-la-regresion-logistica">
<h2><span class="section-number">32.4. </span>Coeficientes en la Regresión Logística<a class="headerlink" href="#coeficientes-en-la-regresion-logistica" title="Link to this heading">#</a></h2>
<p>Los coeficientes en la regresión logística representan la fuerza y la dirección de la relación entre cada característica independiente y la variable dependiente (la probabilidad logarítmica de que ocurra el evento de interés). En términos matemáticos, el modelo de regresión logística se define generalmente como:</p>
<div class="math notranslate nohighlight">
\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n
\]</div>
<p>donde:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( p \)</span> es la probabilidad de que el evento de interés ocurra.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0, \beta_1, \ldots, \beta_n \)</span> son los coeficientes del modelo para las características <span class="math notranslate nohighlight">\( x_1, x_2, \ldots, x_n \)</span>.</p></li>
</ul>
</section>
<section id="interpretacion-de-los-coeficientes">
<h2><span class="section-number">32.5. </span>Interpretación de los Coeficientes<a class="headerlink" href="#interpretacion-de-los-coeficientes" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Magnitud de los Coeficientes</strong>: Un coeficiente mayor en valor absoluto indica que la característica correspondiente tiene un mayor efecto sobre la respuesta. Esto sugiere que la característica es más importante para el modelo.</p></li>
<li><p><strong>Signo de los Coeficientes</strong>: El signo de cada coeficiente indica la dirección de la relación:</p>
<ul class="simple">
<li><p>Un coeficiente positivo indica que a medida que el valor de la característica aumenta, también lo hace la probabilidad logarítmica de que ocurra el evento (y viceversa).</p></li>
<li><p>Un coeficiente negativo indica que a medida que el valor de la característica aumenta, la probabilidad logarítmica de que ocurra el evento disminuye.</p></li>
</ul>
</li>
</ol>
</section>
<section id="estandares-y-regularizacion">
<h2><span class="section-number">32.6. </span>Estándares y Regularización<a class="headerlink" href="#estandares-y-regularizacion" title="Link to this heading">#</a></h2>
<p>En modelos con múltiples características, especialmente cuando las escalas varían, es común estandarizar las características antes de ajustar el modelo. Esto hace que los coeficientes sean comparables entre sí y proporciona una mejor interpretación de la importancia relativa de cada característica.</p>
<p>Además, técnicas de regularización como Lasso (L1) y Ridge (L2) pueden afectar directamente la interpretación de la importancia de las características:</p>
<ul class="simple">
<li><p><strong>Lasso (L1)</strong>: Tiende a reducir los coeficientes de las características menos importantes a cero, lo que puede ser útil para la selección de características.</p></li>
<li><p><strong>Ridge (L2)</strong>: Reduce todos los coeficientes hacia cero, pero no los hace exactamente cero, lo cual es útil para manejar la multicolinealidad y mejorar la estabilidad del modelo.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtener los nombres de las características después de cualquier transformación</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span>

<span class="c1"># Obtener los coeficientes del modelo</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Crear el DataFrame de importancias</span>
<span class="c1"># Tomamos el valor absoluto de los coeficientes para indicar la importancia sin considerar la dirección</span>
<span class="n">feature_importances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;Feature&#39;</span><span class="p">:</span> <span class="n">feature_names</span><span class="p">,</span>
    <span class="s1">&#39;Importance&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span>
<span class="p">})</span>

<span class="c1"># Ordenando las características por importancia</span>
<span class="n">feature_importances</span> <span class="o">=</span> <span class="n">feature_importances</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;Importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Creando la gráfica de barras</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>  <span class="c1"># Ajusta el tamaño de la figura aquí según tus necesidades</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">feature_importances</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">],</span> <span class="n">feature_importances</span><span class="p">[</span><span class="s1">&#39;Importance&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Características&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Importancia&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Importancia de las Características&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>  <span class="c1"># Rota las etiquetas del eje x para mejor visualización</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b77b113628c2a4b828e46a807255140ed5421df3bf2166d8410262dd1ea3a484.png" src="_images/b77b113628c2a4b828e46a807255140ed5421df3bf2166d8410262dd1ea3a484.png" />
</div>
</div>
<p>Usando statsmodel</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Asumimos que X_train y y_train ya están definidos</span>
<span class="c1"># Agregar una columna de unos para interceptar términos</span>
<span class="n">X_train_sm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># Añadir constante si el modelo lo requiere</span>

<span class="c1"># Ajustar el modelo de regresión logística</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">Logit</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">X_train_sm</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Obtener el resumen del modelo</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1"># Obtener los nombres de las características (incluyendo el intercepto si está presente)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">X_train_sm</span><span class="o">.</span><span class="n">columns</span>

<span class="c1"># Obtener los coeficientes del modelo</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span>

<span class="c1"># Crear el DataFrame de importancias</span>
<span class="c1"># Usamos el valor absoluto de los coeficientes para reflejar la importancia</span>
<span class="n">feature_importances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;Feature&#39;</span><span class="p">:</span> <span class="n">feature_names</span><span class="p">,</span>
    <span class="s1">&#39;Importance&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span>
<span class="p">})</span>

<span class="c1"># Ordenando las características por importancia</span>
<span class="n">feature_importances</span> <span class="o">=</span> <span class="n">feature_importances</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;Importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Creando la gráfica de barras</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>  <span class="c1"># Ajusta el tamaño de la figura aquí según tus necesidades</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">feature_importances</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">],</span> <span class="n">feature_importances</span><span class="p">[</span><span class="s1">&#39;Importance&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Características&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Importancia&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Importancia de las Características&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>  <span class="c1"># Rota las etiquetas del eje x para mejor visualización</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.116626
         Iterations 12
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                 RIESGO   No. Observations:                  126
Model:                          Logit   Df Residuals:                      113
Method:                           MLE   Df Model:                           12
Date:                Fri, 10 May 2024   Pseudo R-squ.:                  0.8317
Time:                        16:51:46   Log-Likelihood:                -14.695
converged:                       True   LL-Null:                       -87.321
Covariance Type:            nonrobust   LLR p-value:                 5.198e-25
======================================================================================
                         coef    std err          z      P&gt;|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
const                 -4.1647      7.880     -0.529      0.597     -19.609      11.280
MONTODELCREDITO     2.095e-05   8.13e-06      2.576      0.010    5.01e-06    3.69e-05
NUMERODECUOTAS         0.3356      0.146      2.303      0.021       0.050       0.621
INGRESOS           -3.591e-06   1.49e-06     -2.413      0.016   -6.51e-06   -6.74e-07
INGRESOSFAMILIARES  3.828e-06   2.83e-06      1.351      0.177   -1.73e-06    9.38e-06
EDAD                  -0.2303      0.109     -2.112      0.035      -0.444      -0.017
DESTINODELCREDITO    -21.2951      8.143     -2.615      0.009     -37.254      -5.336
GENERO                -3.5500      2.725     -1.303      0.193      -8.891       1.791
CASAPROPIA            -4.1388      3.449     -1.200      0.230     -10.900       2.622
CODEUDOR               0.9729      4.461      0.218      0.827      -7.771       9.716
SITUACIONLABORAL       5.4035      2.547      2.121      0.034       0.411      10.396
ESTADOCIVIL            4.9954      2.362      2.115      0.034       0.366       9.625
ESTRATO               -2.3794      1.397     -1.703      0.089      -5.117       0.359
======================================================================================

Possibly complete quasi-separation: A fraction 0.58 of observations can be
perfectly predicted. This might indicate that there is complete
quasi-separation. In this case some parameters will not be identified.
</pre></div>
</div>
<img alt="_images/8dbcfd8dd292092eb11606ea99f87be2c79cbf88b6c0a9bf6ab1231c6d875476.png" src="_images/8dbcfd8dd292092eb11606ea99f87be2c79cbf88b6c0a9bf6ab1231c6d875476.png" />
</div>
</div>
</section>
</section>
<section id="preprocesamiento">
<h1><span class="section-number">33. </span>Preprocesamiento<a class="headerlink" href="#preprocesamiento" title="Link to this heading">#</a></h1>
</section>
<section id="introduccion-a-los-arboles-de-decision">
<h1><span class="section-number">34. </span>Introducción a los Árboles de Decisión<a class="headerlink" href="#introduccion-a-los-arboles-de-decision" title="Link to this heading">#</a></h1>
<p>Los árboles de decisión, uno de los métodos de aprendizaje supervisado más accesibles y utilizados, fueron desarrollados inicialmente en los años 60 y 70. Quinlan, en 1986, popularizó los árboles de clasificación con su algoritmo ID3 (Iterative Dichotomiser 3) y más tarde C4.5, que mejoraba el ID3 incorporando el manejo de datos continuos y valores faltantes. De manera general, los árboles de decisión son modelos predictivos formados por reglas binarias de decisión, que se utilizan ampliamente tanto en clasificación como en regresión. Son fáciles de entender y pueden manejar tanto datos categóricos como numéricos, lo que los hace particularmente útiles en el sector financiero.</p>
<section id="algoritmo-id3-iterative-dichotomiser-3">
<h2><span class="section-number">34.1. </span>Algoritmo ID3 (Iterative Dichotomiser 3)<a class="headerlink" href="#algoritmo-id3-iterative-dichotomiser-3" title="Link to this heading">#</a></h2>
<p>El algoritmo ID3, desarrollado por Ross Quinlan en los años 80, es un método utilizado para generar un árbol de decisión a partir de un conjunto de datos. Se basa en el concepto de ganancia de información para seleccionar el atributo que mejor divide el conjunto de datos en subconjuntos más homogéneos respecto a la variable objetivo. Aquí se describe el funcionamiento básico del algoritmo ID3:</p>
<ol class="arabic simple">
<li><p><strong>Inicialización</strong>: Se empieza con el conjunto de datos completo en el nodo raíz.</p></li>
<li><p><strong>Selección de Atributo</strong>: En cada nodo del árbol, el algoritmo selecciona el atributo que maximiza la ganancia de información respecto a la variable objetivo. La ganancia de información se calcula comparando la entropía antes y después de dividir el conjunto de datos basado en cada atributo posible.</p></li>
<li><p><strong>Creación de Nodo</strong>: Para el atributo seleccionado, se crea un nodo de decisión, y el conjunto de datos se divide en subconjuntos correspondientes a los diferentes valores del atributo.</p></li>
<li><p><strong>Recursividad</strong>: Este proceso se repite de manera recursiva para cada subconjunto de datos. Si un subconjunto contiene solo ejemplos de una misma clase o ya no quedan atributos para considerar, se crea un nodo hoja con la clase predominante.</p></li>
<li><p><strong>Terminación</strong>: El proceso termina cuando todos los datos se han clasificado correctamente o no quedan atributos para dividir los datos.</p></li>
</ol>
<section id="limitaciones-del-id3">
<h3><span class="section-number">34.1.1. </span>Limitaciones del ID3:<a class="headerlink" href="#limitaciones-del-id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>No maneja directamente atributos numéricos o datos faltantes.</p></li>
<li><p>Puede generar árboles muy complejos que sobreajusten los datos, especialmente si el árbol crece demasiado profundo.</p></li>
<li><p>Solo es aplicable a tareas de clasificación.</p></li>
</ul>
</section>
</section>
<section id="algoritmo-c4-5">
<h2><span class="section-number">34.2. </span>Algoritmo C4.5<a class="headerlink" href="#algoritmo-c4-5" title="Link to this heading">#</a></h2>
<p>El C4.5, también desarrollado por Quinlan en la década de 1990, es una extensión y mejora del algoritmo ID3. Incorpora varias características nuevas para superar algunas de las limitaciones de ID3:</p>
<ol class="arabic simple">
<li><p><strong>Manejo de Atributos Continuos</strong>: C4.5 puede manejar datos numéricos dividiendo los valores en rangos que maximizan la ganancia de información en cada división.</p></li>
<li><p><strong>Manejo de Datos Faltantes</strong>: C4.5 permite manejar casos donde faltan valores de algunos atributos mediante la distribución proporcional de casos entre las diferentes ramificaciones del nodo basado en la disponibilidad de datos.</p></li>
<li><p><strong>Poda del Árbol</strong>: Para evitar el sobreajuste, C4.5 implementa un proceso de poda. Después de construir el árbol, elimina secciones del árbol que no proporcionan una mejora significativa en la capacidad de clasificación del árbol.</p></li>
<li><p><strong>Uso de la Razón de Ganancia</strong>: En lugar de usar solo la ganancia de información, C4.5 utiliza la razón de ganancia, que normaliza la ganancia de información utilizando la entropía de los valores del atributo, ayudando a evitar un sesgo hacia atributos con un mayor número de valores.</p></li>
<li><p><strong>Conversiones de Árbol a Reglas</strong>: C4.5 puede convertir el árbol de decisión generado en un conjunto de reglas de clasificación, simplificando la interpretación del modelo.</p></li>
</ol>
<p>Estas mejoras hacen que el C4.5 sea más robusto y versátil que ID3, especialmente en conjuntos de datos más complejos y variados.</p>
</section>
<section id="id3">
<h2><span class="section-number">34.3. </span>1. <strong>Fundamentos Teóricos</strong><a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<section id="conceptos-basicos">
<h3><span class="section-number">34.3.1. </span>Conceptos Básicos<a class="headerlink" href="#conceptos-basicos" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Nodos</strong>: Representan una pregunta o forma de dividir el conjunto de datos basado en una característica. El nodo inicial desde donde el árbol comienza es conocido como el nodo raíz.</p></li>
<li><p><strong>Ramas</strong>: Son las ‘reglas’ que conectan los nodos, basadas en las respuestas a las preguntas formuladas en cada nodo.</p></li>
<li><p><strong>Hojas</strong>: Los nodos finales del árbol, donde no hay más preguntas, representan la decisión o el resultado final.</p></li>
</ul>
</section>
<section id="criterios-de-division">
<h3><span class="section-number">34.3.2. </span>Criterios de División<a class="headerlink" href="#criterios-de-division" title="Link to this heading">#</a></h3>
</section>
</section>
</section>
<section id="id4">
<h1><span class="section-number">35. </span>Criterios de División<a class="headerlink" href="#id4" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>Ganancia de Información</strong>: Mide la mejora en la entropía desde un estado anterior a un estado posterior del árbol. Se utiliza para definir qué atributo en un conjunto de datos se debe utilizar de manera que la información sobre la clasificación final sea la más precisa. La Ganancia de Información se basa en el concepto de entropía, que es una medida de la impureza o la incertidumbre en un conjunto de datos. La entropía de un conjunto <span class="math notranslate nohighlight">\( S \)</span> con <span class="math notranslate nohighlight">\( k \)</span> clases se define como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ H(S) = -\sum_{i=1}^k p_i \log_2(p_i) \]</div>
<p>donde <span class="math notranslate nohighlight">\( p_i \)</span> es la proporción de la clase <span class="math notranslate nohighlight">\( i \)</span> en el conjunto <span class="math notranslate nohighlight">\( S \)</span>. La Ganancia de Información se calcula como la diferencia entre la entropía antes de la división y la suma ponderada de la entropía de cada subconjunto después de la división:</p>
<div class="math notranslate nohighlight">
\[ \text{Ganancia}(S, A) = H(S) - \sum_{v \in \text{valores}(A)} \frac{|S_v|}{|S|} H(S_v) \]</div>
<p>donde <span class="math notranslate nohighlight">\( A \)</span> es el atributo de división y <span class="math notranslate nohighlight">\( S_v \)</span> son los subconjuntos de <span class="math notranslate nohighlight">\( S \)</span> para cada valor <span class="math notranslate nohighlight">\( v \)</span> del atributo <span class="math notranslate nohighlight">\( A \)</span>.</p>
<ul class="simple">
<li><p><strong>Índice Gini</strong>: Es una métrica que mide la impureza o la pureza utilizada mientras se crea un árbol de decisión en el modelo CART (Classification and Regression Trees). Un nodo es “puro” si en su mayoría contiene clases similares.
El Índice Gini es utilizada especialmente en el algoritmo CART (Classification and Regression Trees). Se calcula como la probabilidad de clasificar incorrectamente cualquier elemento del conjunto, suponiendo que se elige aleatoriamente según la distribución de clases del conjunto. El Índice Gini de un conjunto <span class="math notranslate nohighlight">\( S \)</span> se define como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ G(S) = 1 - \sum_{i=1}^k p_i^2 \]</div>
<p>donde <span class="math notranslate nohighlight">\( p_i \)</span> es la proporción de la clase <span class="math notranslate nohighlight">\( i \)</span> en el conjunto <span class="math notranslate nohighlight">\( S \)</span>. La reducción de impureza basada en Gini para un atributo <span class="math notranslate nohighlight">\( A \)</span> se calcula como:</p>
<div class="math notranslate nohighlight">
\[ \text{Gini\_reduction}(S, A) = G(S) - \sum_{v \in \text{valores}(A)} \frac{|S_v|}{|S|} G(S_v) \]</div>
<ul class="simple">
<li><p><strong>Reducción de la varianza</strong>: Usualmente empleada en problemas de regresión, donde se selecciona la división que resulta en la menor varianza en los nodos <a class="reference external" href="http://hijos.La">hijos.La</a> varianza de un conjunto <span class="math notranslate nohighlight">\( S \)</span> se define como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \text{Var}(S) = \frac{1}{|S|} \sum_{x \in S} (x - \mu)^2 \]</div>
<p>donde <span class="math notranslate nohighlight">\( \mu \)</span> es la media de los valores en <span class="math notranslate nohighlight">\( S \)</span>. La reducción de la varianza después de una división por un atributo <span class="math notranslate nohighlight">\( A \)</span> se calcula como:</p>
<div class="math notranslate nohighlight">
\[ \text{Var\_reduction}(S, A) = \text{Var}(S) - \sum_{v \in \text{valores}(A)} \frac{|S_v|}{|S|} \text{Var}(S_v) \]</div>
<section id="ventajas-y-desventajas-frente-a-modelos-lineales">
<h2><span class="section-number">35.1. </span>Ventajas y Desventajas Frente a Modelos Lineales<a class="headerlink" href="#ventajas-y-desventajas-frente-a-modelos-lineales" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Ventajas</strong>:</p>
<ul>
<li><p>No lineales: Pueden capturar relaciones no lineales sin necesidad de transformar las variables.</p></li>
<li><p>Fáciles de entender y visualizar.</p></li>
<li><p>Manejan bien datos categóricos y numéricos.</p></li>
</ul>
</li>
<li><p><strong>Desventajas</strong>:</p>
<ul>
<li><p>Sobreajuste: Los árboles muy profundos pueden modelar demasiado el ruido del conjunto de datos de entrenamiento.</p></li>
<li><p>Menos efectivos en la extrapolación: No pueden predecir más allá del rango de las variables del modelo.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="ejemplo-de-clasificacion-binaria-usando-arboles-de-decision-caso-de-pronostico-de-riesgo">
<h1><span class="section-number">36. </span>Ejemplo de clasificación binaria usando árboles de decisión: caso de pronóstico de riesgo<a class="headerlink" href="#ejemplo-de-clasificacion-binaria-usando-arboles-de-decision-caso-de-pronostico-de-riesgo" title="Link to this heading">#</a></h1>
<p>En este caso de estudio, utilizaremos un conjunto de datos financieros para predecir el riesgo de incumplimiento en el pago de créditos. La metodología seleccionada en esta ocasión será un modelo basado en árboles de decisión, una técnica popular en el aprendizaje automático para la clasificación binaria debido a su interpretabilidad y efectividad.</p>
<section id="id5">
<h2><span class="section-number">36.1. </span>Descripción de las columnas del dataset<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>El dataset contiene las siguientes columnas:</p>
<ul class="simple">
<li><p><strong>RIESGO</strong>: Variable objetivo (binaria). <code class="docutils literal notranslate"><span class="pre">1</span></code> indica que el cliente paga el crédito (‘SI PAGA’), y <code class="docutils literal notranslate"><span class="pre">0</span></code> que el cliente incumple el pago (‘NO PAGA’).</p></li>
<li><p><strong>MONTODELCREDITO</strong>: Cantidad total del crédito otorgado.</p></li>
<li><p><strong>NUMERODECUOTAS</strong>: Número total de cuotas en las que el crédito debe ser devuelto.</p></li>
<li><p><strong>INGRESOS</strong>: Ingresos mensuales del cliente.</p></li>
<li><p><strong>INGRESOSFAMILIARES</strong>: Ingresos mensuales totales de la familia del cliente.</p></li>
<li><p><strong>EDAD</strong>: Edad del cliente.</p></li>
<li><p><strong>DESTINODELCREDITO</strong>: Propósito del crédito (<code class="docutils literal notranslate"><span class="pre">ESTUDIANTE</span> <span class="pre">PREGRADO</span></code> o <code class="docutils literal notranslate"><span class="pre">ESTUDIANTE</span> <span class="pre">POSGRADO</span></code>).</p></li>
<li><p><strong>GENERO</strong>: Género del cliente (<code class="docutils literal notranslate"><span class="pre">MASCULINO</span></code> o <code class="docutils literal notranslate"><span class="pre">FEMENINO</span></code>).</p></li>
<li><p><strong>CASAPROPIA</strong>: Indica si el cliente posee casa propia (<code class="docutils literal notranslate"><span class="pre">SI</span> <span class="pre">TIENE</span></code> o <code class="docutils literal notranslate"><span class="pre">NO</span> <span class="pre">TIENE</span></code>).</p></li>
<li><p><strong>CODEUDOR</strong>: Indica si el cliente tiene un codeudor (<code class="docutils literal notranslate"><span class="pre">SI</span> <span class="pre">TIENE</span></code> o <code class="docutils literal notranslate"><span class="pre">NO</span> <span class="pre">TIENE</span></code>).</p></li>
<li><p><strong>SITUACIONLABORAL</strong>: Estado laboral del cliente (<code class="docutils literal notranslate"><span class="pre">INDEPENDIENTE</span></code> o <code class="docutils literal notranslate"><span class="pre">EMPLEADO</span></code>).</p></li>
<li><p><strong>ESTADOCIVIL</strong>: Estado civil del cliente (<code class="docutils literal notranslate"><span class="pre">OTRO</span></code> o <code class="docutils literal notranslate"><span class="pre">CASADO</span></code>).</p></li>
<li><p><strong>ESTRATO</strong>: Estrato socioeconómico del cliente, valor entero de 1 a 5.</p></li>
</ul>
</section>
<section id="implementacion-del-modelo-de-arboles-de-decision">
<h2><span class="section-number">36.2. </span>Implementación del modelo de árboles de decisión<a class="headerlink" href="#implementacion-del-modelo-de-arboles-de-decision" title="Link to this heading">#</a></h2>
<p>Primero, importaremos las librerías necesarias y prepararemos los datos para el modelo. Luego, ajustaremos el modelo de árboles de decisión y evaluaremos su rendimiento utilizando la matriz de confusión y la importancia de las características.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Cargar el dataset</span>
<span class="c1"># Supongamos que el DataFrame se llama &#39;data&#39;</span>

<span class="c1"># Convertir variables categóricas a numéricas</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Dividir el dataset en conjuntos de entrenamiento y prueba</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Crear y entrenar el modelo de árboles de decisión</span>
<span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Realizar predicciones sobre el conjunto de prueba</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluar el modelo</span>
<span class="c1"># Matriz de confusión</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matriz de confusión:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>

<span class="c1"># Informe de clasificación</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># AUC ROC</span>
<span class="n">roc_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC ROC:&quot;</span><span class="p">,</span> <span class="n">roc_auc</span><span class="p">)</span>

<span class="c1"># Importancia de las características</span>
<span class="n">feature_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">tree_model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">feature_importance</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Importancia de las Características&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Cargar el dataset</span>
<span class="c1"># Supongamos que el DataFrame se llama &#39;data&#39;</span>

<span class="c1"># Convertir variables categóricas a numéricas</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Dividir el dataset en conjuntos de entrenamiento y prueba</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;RIESGO&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1952</span><span class="p">)</span>

<span class="c1"># Crear y entrenar el modelo de árboles de decisión</span>
<span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Realizar predicciones sobre el conjunto de prueba</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluar el modelo</span>
<span class="c1"># Matriz de confusión</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matriz de confusión:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>

<span class="c1"># Informe de clasificación</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># AUC ROC</span>
<span class="n">roc_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC ROC:&quot;</span><span class="p">,</span> <span class="n">roc_auc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Matriz de confusión:
[[26  0]
 [ 2 26]]
              precision    recall  f1-score   support

           0       0.93      1.00      0.96        26
           1       1.00      0.93      0.96        28

    accuracy                           0.96        54
   macro avg       0.96      0.96      0.96        54
weighted avg       0.97      0.96      0.96        54

AUC ROC: 0.9642857142857143
</pre></div>
</div>
</div>
</div>
</section>
<section id="explicacion-matematica-de-feature-importances-en-arboles-de-decision">
<h2><span class="section-number">36.3. </span>Explicación Matemática de <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> en Árboles de Decisión<a class="headerlink" href="#explicacion-matematica-de-feature-importances-en-arboles-de-decision" title="Link to this heading">#</a></h2>
<p>En los modelos de árboles de decisión, como los implementados en la librería scikit-learn, <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> es un atributo que proporciona una medida de la importancia de cada característica para la predicción. La importancia se calcula durante la construcción del árbol y es determinante para entender cómo contribuyen las características individuales al modelo. Aquí explicaré matemáticamente cómo se determinan estas importancias.</p>
<section id="calculo-de-la-importancia-de-las-caracteristicas">
<h3><span class="section-number">36.3.1. </span>Cálculo de la Importancia de las Características<a class="headerlink" href="#calculo-de-la-importancia-de-las-caracteristicas" title="Link to this heading">#</a></h3>
<p>La importancia de una característica se calcula con base en cuánto ayuda a mejorar la precisión del modelo, específicamente a través de la reducción del índice de impureza, generalmente medido por el Gini impurity o la entropía en el caso de clasificación, y la varianza total o el error cuadrático medio en el caso de regresión.</p>
<section id="formula-general">
<h4><span class="section-number">36.3.1.1. </span>Fórmula General<a class="headerlink" href="#formula-general" title="Link to this heading">#</a></h4>
<p>Para un árbol individual, la importancia de una característica <span class="math notranslate nohighlight">\( j \)</span> se calcula sumando la disminución en el índice de impureza (o “ganancia”) que resulta de usar la característica <span class="math notranslate nohighlight">\( j \)</span> en todos los nodos <span class="math notranslate nohighlight">\( t \)</span> donde la característica es utilizada para dividir el nodo. Esta suma se normaliza dividiendo por la suma de todas las disminuciones de impureza en el árbol. Matemáticamente, se expresa como:</p>
<div class="math notranslate nohighlight">
\[
\text{Importancia}(j) = \frac{\sum_{t \in T_j} p(t) \times \Delta i(s_t, t)}{\sum_{t \in T} p(t) \times \Delta i(s_t, t)}
\]</div>
<p>Donde:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( T_j \)</span> es el conjunto de nodos que utilizan la característica <span class="math notranslate nohighlight">\( j \)</span> para dividirse.</p></li>
<li><p><span class="math notranslate nohighlight">\( p(t) \)</span> es la proporción de muestras que llegan al nodo <span class="math notranslate nohighlight">\( t \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \Delta i(s_t, t) \)</span> es la reducción del índice de impureza debido a la división en el nodo <span class="math notranslate nohighlight">\( t \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( s_t \)</span> es el punto de división en el nodo <span class="math notranslate nohighlight">\( t \)</span> basado en la característica <span class="math notranslate nohighlight">\( j \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( T \)</span> es el conjunto de todos los nodos en el árbol.</p></li>
</ul>
</section>
<section id="gini-impurity">
<h4><span class="section-number">36.3.1.2. </span>Gini Impurity<a class="headerlink" href="#gini-impurity" title="Link to this heading">#</a></h4>
<p>Para la clasificación, el Gini impurity de un nodo se calcula como:</p>
<div class="math notranslate nohighlight">
\[
i(t) = 1 - \sum_{k=1}^K p_{t,k}^2
\]</div>
<p>donde <span class="math notranslate nohighlight">\( p_{t,k} \)</span> es la proporción de muestras de clase <span class="math notranslate nohighlight">\( k \)</span> en el nodo <span class="math notranslate nohighlight">\( t \)</span>. La reducción de Gini impurity debido a una división que usa la característica <span class="math notranslate nohighlight">\( j \)</span> se mide entonces por la diferencia en impureza antes y después de la división, ponderada por el número de muestras que llegan a cada nodo hijo.</p>
</section>
</section>
<section id="ejemplo-ilustrativo">
<h3><span class="section-number">36.3.2. </span>Ejemplo Ilustrativo<a class="headerlink" href="#ejemplo-ilustrativo" title="Link to this heading">#</a></h3>
<p>Si un nodo <span class="math notranslate nohighlight">\( t \)</span> se divide en dos nodos hijos <span class="math notranslate nohighlight">\( t_1 \)</span> y <span class="math notranslate nohighlight">\( t_2 \)</span> utilizando la característica <span class="math notranslate nohighlight">\( j \)</span>, la ganancia de Gini impurity se calcula como:</p>
<div class="math notranslate nohighlight">
\[
\Delta i(s_t, t) = i(t) - \left(\frac{n_{t1}}{n_t} i(t_1) + \frac{n_{t2}}{n_t} i(t_2)\right)
\]</div>
<p>donde <span class="math notranslate nohighlight">\( n_{t} \)</span>, <span class="math notranslate nohighlight">\( n_{t1} \)</span>, y <span class="math notranslate nohighlight">\( n_{t2} \)</span> son el número de muestras en los nodos <span class="math notranslate nohighlight">\( t \)</span>, <span class="math notranslate nohighlight">\( t_1 \)</span>, y <span class="math notranslate nohighlight">\( t_2 \)</span>, respectivamente.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importancia de las características</span>
<span class="n">feature_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">tree_model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">feature_importance</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Importancia de las Características&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ee040a5f879848deb1b5e015a8ebff40622c72ba5931a646b6c75e4e41eced6e.png" src="_images/ee040a5f879848deb1b5e015a8ebff40622c72ba5931a646b6c75e4e41eced6e.png" />
</div>
</div>
<p>Ahora, una representación visual del árbol</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">export_graphviz</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">pydotplus</span>
<span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Source</span>

<span class="c1"># para que funcione, el DataFrame y el modelo ya están definidos y entrenados como &#39;tree_model&#39;</span>

<span class="c1"># Dividir el dataset en conjuntos de entrenamiento y prueba (suponiendo que ya se ha hecho)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1952</span><span class="p">)</span>
<span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Exportar el modelo del árbol a un archivo dot</span>
<span class="n">dot_data</span> <span class="o">=</span> <span class="n">export_graphviz</span><span class="p">(</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">feature_names</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
                           <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;No Riesgo&#39;</span><span class="p">,</span> <span class="s1">&#39;Si Riesgo&#39;</span><span class="p">],</span>
                           <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                           <span class="n">special_characters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Usar Graphviz para visualizar el árbol</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">pydotplus</span><span class="o">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>
<span class="c1"># Si deseas guardar el gráfico en un archivo, puedes usar: graph.write_png(&#39;tree.png&#39;)</span>
<span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">create_png</span><span class="p">())</span>

<span class="c1"># Alternativamente, usando graphviz directamente para una visualización más simple</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">Source</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>
<span class="n">source</span><span class="o">.</span><span class="n">view</span><span class="p">()</span>  <span class="c1"># Esto abrirá el gráfico en tu visualizador de gráficos por defecto</span>
<span class="c1"># Mostrar la imagen en el notebook</span>
<span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">create_png</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/162a741f52bbd49751c325fb7bf94c6899dc55c5bce5793ef540deb1dda221fc.png" src="_images/162a741f52bbd49751c325fb7bf94c6899dc55c5bce5793ef540deb1dda221fc.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Usando graphviz para visualizar y guardar el árbol</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">Source</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>
<span class="n">source</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;decision_tree&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">source</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/61745619e1d91108dc7ac6a6d91c50019c58f94219e31d0a878a56bc991c44d0.svg" src="_images/61745619e1d91108dc7ac6a6d91c50019c58f94219e31d0a878a56bc991c44d0.svg" /></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="DipFin%20Sesion%201.5.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">18. </span>Introducción a la Limpieza de Datos</p>
      </div>
    </a>
    <a class="right-next"
       href="DipFin%20Sesion%203.2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">37. </span>1. Introducción a las Redes Neuronales</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">22. Introducción a la Regresión Lineal</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentos-teoricos">22.1. 1. <strong>Fundamentos Teóricos</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#que-es-la-regresion-lineal">22.1.1. ¿Qué es la Regresión Lineal?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicaciones-en-finanzas">22.1.2. Aplicaciones en Finanzas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relacion-entre-variables">22.1.3. Relación entre Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-los-coeficientes-de-la-ecuacion">22.1.4. Cálculo de los coeficientes de la ecuación</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-lineal-con-predictores-discretos">23. Regresión Lineal con Predictores Discretos</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concepto-de-efecto-y-calculo-de-pendientes">23.1. Concepto de Efecto y Cálculo de Pendientes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ecuacion-del-modelo">23.1.1. <strong>Ecuación del Modelo</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretacion-de-coeficientes">23.1.2. <strong>Interpretación de Coeficientes</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">23.2. Aplicaciones en Finanzas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparacion-entre-regresion-lineal-con-predictores-continuos-y-discretos">23.3. Comparación entre Regresión Lineal con Predictores Continuos y Discretos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizacion-comparativa-de-modelos">23.4. Visualización Comparativa de Modelos</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#caso-de-estudio-analisis-de-precios-de-mercedes-benz-en-ee-uu">24. Caso de Estudio: Análisis de Precios de Mercedes-Benz en EE.UU.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#descripcion-del-dataset">24.1. Descripción del Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparacion-y-limpieza-de-datos">24.2. Preparación y Limpieza de Datos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#codigo-para-la-limpieza-de-datos">24.2.1. Código para la Limpieza de Datos</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-lineal-para-predecir-precios">24.3. Regresión Lineal para Predecir Precios</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#codigo-para-la-regresion-lineal">24.3.1. Código para la Regresión Lineal</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-a-la-regresion-logistica">25. Introducción a la Regresión Logística</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">25.1. Fundamentos Teóricos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definicion-de-regresion-logistica">25.1.1. Definición de Regresión Logística</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relacion-con-la-regresion-lineal">25.2. Relación con la Regresión Lineal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diferencias-entre-regresion-lineal-y-logistica">25.3. Diferencias entre Regresión Lineal y Logística</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importancia-de-la-funcion-logit">25.3.1. Importancia de la Función Logit</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-parametros">26. Cálculo de Parámetros</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocesamiento-de-datos-para-regresion-logistica">27. Preprocesamiento de Datos para Regresión Logística</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variables-dummy">27.1. 1. Variables Dummy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizacion">27.2. 2. Normalización</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformaciones-logaritmicas">27.3. 3. Transformaciones Logarítmicas</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-logit">28. Modelo Logit</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ecuacion-del-modelo-logit">28.1. Ecuación del Modelo Logit</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-probit">29. Modelo Probit</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ecuacion-del-modelo-probit">29.1. Ecuación del Modelo Probit</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#diferencias-clave">30. Diferencias Clave</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-de-clasificacion-binaria-usando-regresion-caso-de-pronostico-de-riesgo">31. Ejemplo de clasificación binaria usando regresión: caso de pronóstico de riesgo</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#descripcion-de-las-columnas-del-dataset">31.1. Descripción de las columnas del dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion-del-modelo-de-regresion-logistica">31.2. Implementación del modelo de regresión logística</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-del-rendimiento-del-modelo-de-regresion-logistica">32. Análisis del rendimiento del modelo de regresión logística</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matriz-de-confusion">32.1. Matriz de confusión</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curva-roc-y-auc">32.2. Curva ROC y AUC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretacion-de-la-curva-roc">32.3. Interpretación de la curva ROC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coeficientes-en-la-regresion-logistica">32.4. Coeficientes en la Regresión Logística</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretacion-de-los-coeficientes">32.5. Interpretación de los Coeficientes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estandares-y-regularizacion">32.6. Estándares y Regularización</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocesamiento">33. Preprocesamiento</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-a-los-arboles-de-decision">34. Introducción a los Árboles de Decisión</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algoritmo-id3-iterative-dichotomiser-3">34.1. Algoritmo ID3 (Iterative Dichotomiser 3)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitaciones-del-id3">34.1.1. Limitaciones del ID3:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algoritmo-c4-5">34.2. Algoritmo C4.5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">34.3. 1. <strong>Fundamentos Teóricos</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptos-basicos">34.3.1. Conceptos Básicos</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#criterios-de-division">34.3.2. Criterios de División</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">35. Criterios de División</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ventajas-y-desventajas-frente-a-modelos-lineales">35.1. Ventajas y Desventajas Frente a Modelos Lineales</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-de-clasificacion-binaria-usando-arboles-de-decision-caso-de-pronostico-de-riesgo">36. Ejemplo de clasificación binaria usando árboles de decisión: caso de pronóstico de riesgo</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">36.1. Descripción de las columnas del dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion-del-modelo-de-arboles-de-decision">36.2. Implementación del modelo de árboles de decisión</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explicacion-matematica-de-feature-importances-en-arboles-de-decision">36.3. Explicación Matemática de <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> en Árboles de Decisión</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-la-importancia-de-las-caracteristicas">36.3.1. Cálculo de la Importancia de las Características</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#formula-general">36.3.1.1. Fórmula General</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gini-impurity">36.3.1.2. Gini Impurity</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-ilustrativo">36.3.2. Ejemplo Ilustrativo</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>